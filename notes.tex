\documentclass{report}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}
\usepackage{xcolor,colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[chapter]{algorithm}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
showstringspaces=false            %
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\setlength{\parindent}{0cm}

\title{Artificial Intelligence Notes}
\author{Patrick Oliver GLAUNER \\
	\texttt{patrick.oliver.glauner@gmail.com}}

\date{\today}

\newtheorem{definition}{Definition}[section]

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
{\em Artificial Intelligence (AI)} is a versatile field, for which there are many different definitions. My favorite one is:
\begin{flushright}
{\em "AI is the science of knowing what to do when you don't know what to do." (Peter Norvig)}\footnote{\url{http://www.youtube.com/watch?v=rtmQ3xlt-4A4m45}}
\end{flushright}
~\\
This document aggregates definitions, findings, thoughts and experiments that are related to AI with a strong focus on {\em Machine Learning}. It also contains some information about related fields such as {\em Robotics} and {\em Computer Vision}.
It was created during the preparations for my master's studies at Imperial College London\footnote{\url{http://www3.imperial.ac.uk/computing/teaching/mcsml}}.
~\\~\\
Most of the content is based on:
\begin{itemize}
\item Andrew Ng. {\em Machine Learning}. Coursera.\footnote{\url{http://www.coursera.org/course/ml}}
\item Andrew Ng. {\em CS229 Machine Learning}. Stanford University.\footnote{\url{http://cs229.stanford.edu/}}
\item Christopher Bishop. {\em Pattern Recognition and Machine Learning}.\footnote{\url{http://research.microsoft.com/en-us/um/people/cmbishop/prml/}}
\item {\em Deep Learning Tutorials}.\footnote{\url{http://www.deeplearning.net/tutorial/}}
\item Geoffrey Hinton. {\em Neural Networks for Machine Learning}. Coursera.\footnote{\url{http://class.coursera.org/neuralnets-2012-001l}}
\item Maja Pantic. {\em 395 Machine Learning}. Imperial College London.\footnote{\url{http://ibug.doc.ic.ac.uk/courses/machine-learning-course-395/}}
\item Peter Norvig. {\em CS212 Design of Computer Programs}. Udacity.\footnote{\url{http://www.udacity.com/course/cs212}}
\item Sebastian Thrun. {\em CS373 Artificial Intelligence for Robotics}. Udacity.\footnote{\url{http://www.udacity.com/course/cs373}}
\item Sebastian Thrun and Peter Norvig. {\em CS271 Intro to Artificial Intelligence}. Udacity.\footnote{\url{http://www.udacity.com/course/cs271}}
\item Stuart Russell and Peter Norvig. {\em Artificial Intelligence: A Modern Approach}. Third edition.\footnote{\url{http://aima.cs.berkeley.edu/}}
\item Tom Mitchell. {\em Machine Learning}. \footnote{\url{http://www.cs.cmu.edu/~tom/mlbook.html}}
\item {\em Unsupervised Feature Learning and Deep Learning}. Stanford University. \footnote{\url{http://deeplearning.stanford.edu/wiki}}
\item {\em Wikipedia}. Various articles.\footnote{\url{http://www.wikipedia.org/}}
\end{itemize}

With reference to the purpose of this document, it does not contain original research. Quite the opposite, various statements were copied directly from other sources.

~\\~\\~\\~\\
\begin{flushright}
Patrick GLAUNER
\end{flushright}

\end{abstract}


\chapter{General}
\section{Terminology}
\subsection{Intelligent agents}
An {\bf agent} operates autonomously. A {\bf rational agent} acts to achieve the best outcome or expected outcome if there is uncertainty. An {\bf agent program} implements the {\bf agent function} which maps perceptions to actions.

\subsection{Task environments}
{\bf Single vs. multi agent}: the agent is the only one. {\bf Fully observable} vs. {\bf partially observable}: the agent's sensors perceive the complete state of the relevant environment at each point in time. {\bf Deterministic} vs. {\bf stochastic}: the agent's state and action uniquely determine the next state. {\bf Discrete} vs. {\bf continuous}: finite amount of states, actions and outcomes. {\bf Benign} vs. {\bf adversarial}: there is no opponent. {\bf Known vs. unknown}: the agent's knowledge about the "law of physics" of the environment, the outcomes (or probabilities) for all actions are given.

\section{Turing test}
Alan turing suggested in 1950 that we should rather ask whether machines can pass a behavioral intelligence test than asking whether machines can think.
This test is called the {\bf Turing test}. This test is for a program to have a conversation (via online typed messages) with an interrogator for five minutes. The interrogator then has to guess if the conversation is with a program or a person. The program passes the test if it fools the interrogator 30\% of the time.
Turing stipulated that a computer with a storage of $10^9$ units by the year 2000 could be programmed to pass the test.
He was wrong, but many people have been fooled when they did not know they might be chatting with a computer.
A computer would need to possess the following capabilities to pass the test:
\begin{itemize}
\item {\bf Natural language processing} to enable it to communicate successfully in English
\item {\bf Knowledge representation} to store what it knows or hears
\item {\bf Automated reasoning} to use the stored information to answer questions and to draw new conclusions
\item {\bf Machine learning} to adapt to new circumstances and to detect and extrapolate patterns
\end{itemize}
Passing the test provides plenty of unsolved problems to work on.\\
In addition, the so-called {total Turing Test} includes a video signal so that the interrogator can test the subject's perceptual abilities, as well as the opportunity for the interrogator to manipulate physical objects.
To pass the total Turing Test, the computer will need
\begin{itemize}
\item {\bf Computer vision} to perceive objects
\item {\bf Robotics} to manipulate objects and move about
\end{itemize}
These six disciplines cover most of AI. Most researchers have not been focusing on passing this test but rather on studying the underlying principles of AI.



\chapter{Mathematics}
This chapter summarizes and explains mathematical foundations which are relevant to machine learning.

\section{Probabilities}
\subsection{Introduction}

\begin{definition}[Complement]
$P(\neg A) = 1 - P(A)$
\end{definition}

\begin{definition}[Inclusion-exclusion principle]
$P(A\vee B) =P(A) + P(B) - P(A,B)$
\end{definition}


\subsection{Independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {A};
\node[mynode, right=1.5cm of A] (B) {B};
\end{tikzpicture}
\caption{Independence}
\label{ref:independence}
\end{figure}

As seen in Figure~\ref{ref:independence}, $A$ and $B$ are independent:
\begin{definition}
P(A,B) = P(A)P(B)
\end{definition}

\begin{definition}
$P(A\vert B) = P(A)$ or $P(B\vert A) = P(B)$ 
\end{definition}


\subsection{Conditional independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,below left=1cm of C] (A) {A};
\node[mynode,below right=1cm of C] (B) {B};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence1}
\end{figure}


As seen in Figure~\ref{ref:condindependence1}, $A$ and $B$ are conditionally independent, given $C$:
\begin{definition}
$P(A,B\vert C) = P(A\vert C)P(B\vert C)$
\end{definition}

\begin{definition}
$P(A\vert B,C) = P(A\vert C)$ and $P(B\vert A,C) = P(B\vert C)$
\end{definition}

$A\independent B \vert C \neq A\independent B$
\\

As seen in Figure~\ref{ref:condindependence2}, $A\independent B$ when $C$ is unknown. When $C$ is known, $A$ and $B$ are dependent. Therefore, independence does not imply conditional independence.


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence2}
\end{figure}


\subsection{Conditional probabilities}
\begin{definition}[Complement]
$P(\neg A\vert B) = 1 - P(A\vert B)$
\end{definition}

For $a$ and $b$ to be true, $b$ needs to be true and $a$ needs to be true given $b$:
\begin{definition}[Product rule]
$P(A,B) =P(A\vert B)P(B)$
\end{definition}

\subsection{Total probability}
\begin{definition}
$P(A) = \sum_{b\in B}{P(A,b)} = \sum_{b\in B}{P(A\vert b)P(b)}$
\end{definition}

\begin{definition}[For conditional variables]
$P(A\vert C) = \sum_{b\in B}{P(A\vert C,b)P(b\vert C)}$
\end{definition}

\subsection{Bayes' rule}
\label{chapter:bayesrule}
Applying product rule and total probability:
\begin{definition}
$P(A\vert B) = \frac{P(B\vert A)P(A)}{\sum_{a\in A}{P(B\vert a)P(a)}} = \frac{P(B\vert A)P(A)}{P(B)}$
\end{definition}

The terminology is: $Posterior = \frac{Likelihood\times Prior}{Normalizer}$\\

The normalizer might be difficult to calculate. It can be substituted with pseudo probabilities:
\begin{enumerate}
\item $P'(A\vert B) = P(B\vert A)P(A)$ and $P'(\neg A\vert B) = P(B\vert \neg A)P(\neg A)$
\item $P(A\vert B) = \alpha P'(A\vert B)$ and $P(\neg A\vert B) = \alpha P'(\neg A\vert B)$ with $\alpha = \frac{1}{P'(A\vert B) + P'(\neg A\vert B)}$
\end{enumerate}

\begin{definition}[General Bayes' rule]
$P(A\vert B,e) = \frac{P(B\vert A,e)P(A\vert e)}{P(B\vert e)}$
\end{definition}


\section{Linear algebra}
\subsection{Eigenvalues and eigenvectors}
Let $A$ be an $n\times n$ matrix. The number $\lambda$ is an {\bf eigenvalue} of $A$ if there exists a non-zero vector $x$ such that:
\begin{align*}
Ax = \lambda x
\end{align*}
In this case, vector $x$ is called an {\bf eigenvector} of $A$ corresponding to $\lambda$. In analytic geometry, for example, a three-element vector may be seen as an arrow in three-dimensional space starting at the origin. In that case, an eigenvector $v$ is an arrow whose direction is either preserved or exactly reversed after multiplication by $A$. The corresponding eigenvalue determines how the length of the arrow is changed by the operation, and whether its direction is reversed or not, determined by whether the eigenvalue is negative or positive. The condition can be rewritten:
\begin{align*}
(A -\lambda I)x = 0
\end{align*}
Where $I$ is the $n\times n$ identity matrix. In order for a non-zero vector $x$ to satisfy this equation, $A -\lambda I$ must not be invertible. That is, the determinant of $A -\lambda I$ must be equal $0$. $p(\lambda)=\mbox{det}(A -\lambda I)$. Thus, $\lambda = [\lambda_1, ..., \lambda_n]^T$ are the eigenvalues of $A$. To find eigenvectors $x = [x_1,...,x_n]^T$ corresponding to an eigenvalue $\lambda$, the system of linear equations needs to be solved:
\begin{align*}
(A -\lambda I)x = 0
\end{align*}

Example:
$A = \begin{pmatrix}
2 & -4 \\
-1 & -1
\end{pmatrix}$. Then $p(\lambda)=\mbox{det}\begin{pmatrix}
2 - \lambda & -4 \\
-1 & -1 - \lambda
\end{pmatrix}$ \\
$= (2-\lambda)(-1-\lambda)-(-4)(-1)$ \\
$= \lambda^2 - \lambda - 6$ \\
$= (\lambda - 3)(\lambda + 2)$. \\
Thus, $\lambda_1=3$ and $\lambda_2=-2$. \\
To find the eigenvectors corresponding to $\lambda_1=3$, $x=[x_1, x_2]^T$, then $(A -3I)x = 0$ gives:\\
\begin{align*}
\begin{pmatrix}
2-3 & -4 \\
-1 & -1-3
\end{pmatrix} \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
0 \\
0
\end{pmatrix}
\end{align*}
From which the duplicate equations can be obtained:
\begin{align*}
-x_1-4x_2 = 0 \\
-x_1-4x_2 = 0
\end{align*}
Therefore, all eigenvectors corresponding to $\lambda_1=3$ are multiples of $\begin{pmatrix}
-4 \\
1
\end{pmatrix}$ which is a {\bf basis} of the eigenspace corresponding to $\lambda_1=3$.\\
Repeating this process with $\lambda_2=-2$:
\begin{align*}
4x_1-4x_2 = 0 \\
-x_1+x_2 = 0
\end{align*}
Thus, an eigenvector corresponding to $\lambda_2=-2$ is: $\begin{pmatrix}
1 \\
1
\end{pmatrix}$ which is a basis of the eigenspace corresponding to $\lambda_2=-2$.

\section{Calculus}

\subsection{Hessian matrix}
The {\bf Hessian matrix} is a square matrix of second-order partial derivatives of a function.
Given the real-valued function $f(x_1,...,x_n)$, the Hessian matrix is:
\begin{align*}
H(f) = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2}  & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2}  & \dots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2}  & \dots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
\end{align*}

\subsection{Jacobian matrix}
The {\bf Jacobian matrix} is the matrix of all first-order partial derivatives of a vector-valued function.
$F : \mathbb{R}^n \mapsto \mathbb{R}^m$ is a function which takes as input real $n$-tuples and produces as output real $m$-tuples.
Such a function is given by $m$ real-valued component functions $F_1(x_1,...,x_n),...,F_m(x_1,...,x_n)$.
The Jacobian matrix is a $m\times n$ matrix $J$ of $F$.
\begin{align*}
J(f) = \begin{pmatrix}
\frac{\partial F_1}{\partial x_1} & \dots & \frac{\partial F_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial F_m}{\partial x_1} & \dots & \frac{\partial F_m}{\partial x_n}
\end{pmatrix}
\end{align*}



\section{Statistics}

\subsection{Foundations}
The {\bf mean} or {\bf expected value} is defined:
\begin{align*}
\mu = E[X] = \sum_i^m x_iP(x_i)
\end{align*}

For a data set, the mean is:
\begin{align*}
\mu = E[X] = \frac{1}{m}\sum_i^m x_i
\end{align*}

The {\bf variance} $\sigma^2$ measures how far a set of numbers is spread out:
\begin{align*}
\sigma^2 = E[(X-\mu)^2] = \frac{1}{m}\sum_i^m (x_i-\mu)^2
\end{align*}

The {\bf unbiased sample variance} is defined:
\begin{align*}
\sigma^2 = \frac{1}{m-1}\sum_i^m (x_i-\mu)^2
\end{align*}

The {\bf standard deviation} $\sigma$ shows how much variation or dispersion from the average exists:
\begin{align*}
\sigma = \sqrt{\sigma^2(X)}
\end{align*}

\subsection{Covariance}
{\bf Covariance} is a measure of how much two random variables change together:
\begin{align*}
Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = \frac{1}{m}\sum_i^m (x_i-\mu_x)(y_i-\mu_y)
\end{align*}
Variance is a special case of the covariance when the two variables are identical:
\begin{align*}
Cov(X,X) = \sigma^2(X)
\end{align*}

The {\bf covariance matrix} $\Sigma$ is a matrix whose element in the $i$, $j$ position is the covariance between the $i$th and the $j$th elements of a random vector:
\begin{align*}
\Sigma_{ij} = Cov(X_i, X_j)
\end{align*}
\begin{align*}
\Sigma = \frac{1}{m}\sum_i^m (x_i-\mu)^T(x_i-\mu)
\end{align*}

Example:
\begin{align*}
X = \begin{pmatrix}
3 & 8\\
4 & 7\\
5 & 5\\
6 & 3\\
7 & 2
\end{pmatrix}
\end{align*}

\begin{align*}
\mu_1 = 5, \mu_2 = 5
\end{align*}

\begin{align*}
\Sigma = \begin{pmatrix}
2 & -3.2\\
-3.2 & 5.2\\
\end{pmatrix}
\end{align*}


\subsection{Gaussian distribution}
{\bf Gaussian distribution} or {\bf normal distribution} is defined:
\begin{align*}
f(x\vert \mu,\sigma^2)= \frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(x_i-\mu)^2}{2\sigma^2})
\end{align*}
The area under the curve is equal to one. The {\bf 68-95-99.7 rule} states that 68\% of values drawn from a normal distribution are within one $\sigma$ away from the mean, 95\% and 99.7\% within two and three $\sigma$ respectively.
\\
The product of two Gaussians $\mu_1, \sigma_1^2, \mu_2, \sigma_2^2$ is proportional to a Gaussian:
\begin{align*}
\mu\prime = \frac{\sigma_2^2\mu_1 + \sigma_1^2\mu_2}{\sigma_1^2 + \sigma_2^2} \\
\sigma^{2}\prime = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{align*}


{\bf Multivariate Gaussian} distribution is defined:
\begin{align*}
f(x\vert \mu,\Sigma)=\frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}\mbox{exp}(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
\end{align*}

$\Sigma$ is the covariance matrix and $\mu$ is a vector of means.

\subsection{Estimation of mean and variance}
This section shows how to derive estimations for mean and variance using maximum likelihood estimation defined in Chapter~\ref{ref:estimationsection}.\\

The data $X$ are distributed {\bf i.i.d.} (independently and identically distributed). Therefore:
\begin{align*}
p(x_1...x_m\vert \mu,\sigma^2)=\prod_i^m f(x_i\vert \mu,\sigma^2)=\prod_i^m (\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(x_i-\mu)^2}{2\sigma^2}))
\end{align*}
\begin{align*}
=(\frac{1}{2\pi\sigma^2})^{\frac{m}{2}}\mbox{exp}(-\frac{\sum_i(x_i-\mu)^2}{2\sigma^2})
\end{align*}
Applying log:
\begin{align*}
g = \frac{m}{2}\mbox{log }(\frac{1}{2\pi\sigma^2})-\frac{1}{2\sigma^2}\sum_i^m(x_i-\mu)^2
\end{align*}

The mean estimation can be derived:
\begin{align*}
\frac{\partial g}{\partial \mu} = \frac{1}{\sigma^2}\sum_i^m (x_i - y) = 0
\end{align*}
\begin{align*}
m\mu = \sum_i^m x_i \implies \mu = \frac{1}{m}\sum_i^m x_i
\end{align*}

The variance estimation can be derived:
\begin{align*}
\frac{\partial g}{\partial \sigma} = -\frac{m}{\sigma}+\frac{1}{\sigma^3}\sum_i^m (x_i - y)^2 = 0
\end{align*}
\begin{align*}
\sigma^2 = \frac{1}{m}\sum_i^m (x_i - y)^2
\end{align*}



\chapter{Problem Solving}
\section{Introduction}
A {\bf problem solving agent} requires a deterministic and fully observable environment. Please see Chapter~\ref{ref:chapterplanning} for planning in more complex environments. Table~\ref{ref:search} compares general search algorithms. 
{\bf Uninformed search} is a rigid procedure with no knowledge of the cost of a given node to the goal.
{\bf Informed search} has knowledge of the cost of a given node to the goal in the form of an evaluation function, which assigns a real number to each node.

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c|c}
 & Breadth-first & Uniform-cost & Depth-first\\
\hline
\hline
Expansion order
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {3};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {4};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {5};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {4};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {2};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {7};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {6};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {5};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {3};
\path (A) edge[-latex] node[left] {5} (B)
(A) edge[-latex] node[right] {2} (C)
(B) edge[-latex] node[left] {3} (D)
(B) edge[-latex] node[right] {2} (E)
(C) edge[-latex] node[left] {4} (F)
(C) edge[-latex] node[right] {2} (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {5};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {3};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {4};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
\\
\hline
Expansion strategy & shallowest & cheapest & deepest\\
\hline
Optimal & yes & yes & no\\
\hline
Complete & yes & yes & no\\
\hline
Frontier size ($n$ levels) & $2n$ & $2n$ & $n$\\
\end{tabular}
\end{center}
\caption{Comparison of search algorithms}
\label{ref:search}
\end{table}

\section{Shortest path}
The {\bf shortest path algorithm} is an uninformed search algorithm that finds the shortest path from the {\bf start state} to a {\bf goal state} in terms of the number of path elements which all have a uniform cost.
Each state might have a number of {\bf successors}. Algorithm~\ref{python:shortestpath} contains an implementation in Python.
\\
The following three parameters of the algorithm are:
\begin{itemize}
\item {\em start}: the start state
\item {\em successors}: function which returns the successors of a state and the corresponding actions
\item {\em is\_goal}: function which returns true of a state is a goal
\end{itemize}
The {\bf frontier} contains all non-visited successors of previously visited states. It is sorted by path length in ascending order. Visited states will not be visited again.
The algorithm picks the first element of the frontier and {\bf expands}, meaning to go through its successors.
A successor might be a goal state. If so, the algorithm returns the successful path.
If not, the new state is added to the end of the frontier.
Each state in the frontier contains the entire path and corresponding actions that lead to it.


\begin{algorithm}
\caption{Shortest path}
\label{python:shortestpath}
\begin{python}
def shortest_path_search(start, successors, is_goal):
    """Find the shortest path from start state to a state
    such that is_goal(state) is true."""
    if is_goal(start):
        return [start]
    explored = set([start]) # set of states we have visited
    frontier = [ [start] ] # ordered list of paths we have blazed
    while frontier:
        path = frontier.pop(0)
        s = path[-1]
        for (state, action) in successors(s).items():
            if state not in explored:
                explored.add(state)
                path2 = path + [action, state]
                if is_goal(state):
                    return path2
                else:
                    frontier.append(path2)
    return Fail

Fail = []
\end{python}
\end{algorithm}


\section{Lowest cost}
The {\bf lowest cost algorithm} is an informed search algorithm that finds the shortest path from the {\bf start state} to a {\bf goal state} in terms of the aggregated costs per path as actions can have different costs.
\\
Algorithm~\ref{python:lowestcost} contains an implementation in Python. It looks relatively similar to the shortest path algorithm in Algorithm~\ref{python:shortestpath}.
Nonetheless, there are some remarkable differences, including:
First, it has another parameter, {\em action\_cost}, a function which returns the cost of an action.
Second, path contain action costs and total costs.
Third, the frontier is sorted by total path costs.
Fourth, the goal test is postponed and moved to the popping of successor states from the frontier. Only then, a path can be correctly classified as the lowest cost path. If it was done before directly after the successor generation, as in the shortest path algorithm, it might return a non-lowest cost path.
Fifth, the previous difference implies that a goal test at the very beginning of the algorithm is not necessary any more and therefore got dropped.


\begin{algorithm}
\caption{Lowest cost}
\label{python:lowestcost}
\begin{python}
def lowest_cost_search(start, successors, is_goal, action_cost):
    """Return the lowest cost path."""
    explored = set() # set of states we have visited
    frontier = [ [start] ] # ordered list of paths we have blazed
    while frontier:
        path = frontier.pop(0)
        state1 = final_state(path)
        if is_goal(state1):
            return path
        explored.add(state1)
        pcost = path_cost(path)
        for (state, action) in successors(state1).items():
            if state not in explored:
                total_cost = pcost + action_cost(action)
                path2 = path + [(action, total_cost), state]
                add_to_frontier(frontier, path2)
    return Fail

def path_cost(path):
    "The total cost of a path (which is stored in a tuple
    with the final action)."
    if len(path) < 3:
        return 0
    else:
        action, total_cost = path[-2]
        return total_cost

def final_state(path): return path[-1]

def add_to_frontier(frontier, path):
    "Add path to frontier, replacing costlier path if there is one."
    # (This could be done more efficiently.)
    # Find if there is an old path to the final state of this path.
    old = None
    for i,p in enumerate(frontier):
        if final_state(p) == final_state(path):
            old = i
            break
    if old is not None and path_cost(frontier[old]) < path_cost(path):
        return # Old path was better; do nothing
    elif old is not None:
        del frontier[old] # Old path was worse; delete it
    ## Now add the new path and re-sort
    frontier.append(path)
    frontier.sort(key=path_cost)
\end{python}
\end{algorithm}


\section{Applying generalized search to problem solving}
The previously discussed implementations of shortest path and lowest cost are abstract as the determination of successors, actions, costs and goals are done by concrete implementations of these functions.
This separation allows to solve a number of problems easily by only writing concrete implementations without implementing a single line to traverse the {\bf state space}.
\\
In this section, the {\bf pouring problem}, is solved this way. In the standard pouring problem, there are two classes of different capacity.
Legal actions are filling a class, emptying a class or entirely pouring a class into the other one (glutting is not legal).
Given an initial filling, a goal filling must be achieved by applying these actions.
The shortest path of actions must be returned or no path in case the problem is not solvable for a certain configuration of start, goal and capacities.
\\
Algorithm~\ref{python:pouringproblem} contains an implementation of the function and a sample configuration:
\begin{itemize}
\item Capacities: $418$ and $986$
\item Start: $0$ and $0$
\item Goal: $6$ and $0$
\end{itemize}

The shortest solution requires $618$ actions.


\begin{algorithm}
\caption{Pouring problem}
\label{python:pouringproblem}
\begin{python}
START = (0, 0)
GOAL = (6, 0)

def successors(X, Y):
    def sc(state):
        x, y = state
        assert x <= X and y <= Y
        return {((0, y+x) if y+x <= Y else (x-(Y-y), (Y))): 'x->y',
                ((x+y, 0) if x+y <= X else (X, (y-(X-x)))): 'x<-y',
                (X, y): 'fill x',
                (x, Y): 'fill y',
                (0, y): 'empty x',
                (x, 0): 'empty y'}
    return sc

path = shortest_path_search(START,
       successors(418, 986),
       lambda state: state == GOAL)
\end{python}
\end{algorithm}



\section{A* search}
$A*$ is an informed search algorithm that expands the path with that has the minimum value $f$.

\begin{definition}[A* cost function]
$f = g + h$ with $g(path) =$ current path cost and $h(path) = h(s) =$ estimated distance to goal
\end{definition}

$f$ is a {\bf heuristic function}. $A*$ finds lowest cost path if: $h(s) <$ true cost. Subsequently, $h$ never overestimates and is {\bf optimistic} or {\bf admissible}. See Figure~\ref{ref:heuristic} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
S &\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
9&8&7&6&5&4 \\
\hline
8&7&6&5&4&3 \\
\hline
7&6&5&4&3&2 \\
\hline
6&5&4&3&2&1 \\
\hline
5&4&3&2&1&0 \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and heuristic}
\label{ref:heuristic}
\end{figure}



\chapter{Planning}
\label{ref:chapterplanning}
A {\bf policy} specifies what an agent should do for any state that the agent might reach. An {\bf optimal policy} is a policy that yields the highest expected {\bf utility} (meaning "the quality of being useful"). See Figure~\ref{ref:policy} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&\cellcolor{black}&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\rightarrow$&$\rightarrow$&$\uparrow$&$\uparrow$&&* \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and policy}
\label{ref:policy}
\end{figure}

A {\bf planning agent} interleaves planning and execution.
This is necessary for environments which are {\bf stochastic}, {\bf multi agent}, {\bf partially observable} or {\bf unknown}. \\
Plans might be {\bf hierarchical} which consist of actions on different levels. Some of these levels can be planned ahead, others while executing the plan.
Example: A* can find a route from A to Z but the route might change due to accidents.
Low-level actions such as turning the steering wheel or pressing the pedals cannot be planned well-ahead in time in the real world. \\
Plan sequences might be infinite in stochastic environments.
For example a robot is not guaranteed to move properly and can only perform certain actions once it successfully moved.
A loop-based representation allows to describe such a plan efficiently. \\
{\bf Scheduling} of tasks is another planning problem that has been extensively studied in the field of operating systems. \\
Chapter~\ref{chapter:reinforcementlearning} covers {\bf reinforcement learning} which can be utilized to do efficient planning under uncertainty.


\chapter{Logic}
\section{Introduction}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c}
& World & Beliefs \\
\hline
\hline
Probability theory & facts & $[0,1]$ \\
\hline
Propositional logic & facts & true, false, unknown \\
\hline
First-order logic & relations, objects, functions & true, false, unknown \\
\end{tabular}
\end{center}
\caption{Comparison of formal languages}
\label{ref:complang}
\end{table}


\section{Propositional logic}
Equivalent to Boolean Algebra. There are efficient inference algorithms to determine validity and satisfiability. Limitations:
\begin{itemize}
  \item It can only handle true and false values
  \item No capability to handle uncertainty
  \item No objects
  \item No shortcuts to express similar properties, only like: $A_1 \wedge A_2 \wedge ... \wedge A_n$
\end{itemize}


\section{First-order logic}
First-order logic is sufficiently expressive to represent a reasonable amount of our commonsense knowledge. It is also used in many further representation languages and has been studied intensively for many decades. The first-order logic syntax is defined in Figure~\ref{ref:firstordersyntax}.

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$Sentence$ & $\rightarrow$ & $AtomicSentence$ $\vert$ $ComplexSentence$ \\
$AtomicSentence$ & $\rightarrow$ & $Predicate$ $\vert$ $Predicate(Term,...)$ $\vert$ $Term=Term$ \\
$ComplexSentence$ & $\rightarrow$ & $(Sentence)$ $\vert$ $[Sentence]$ \\
 & $\vert$ & $\neg Sentence$ \\
 & $\vert$ & $Sentence \wedge Sentence$ \\
 & $\vert$ & $Sentence \vee Sentence$ \\
 & $\vert$ & $Sentence \implies Sentence$ \\
 & $\vert$ & $Sentence \iff Sentence$ \\
 & $\vert$ & $Quantifier$ $Variable, ...$ $Sentence$ \\
\\
$Term$ & $\rightarrow$ & $Function(Term,...)$ \\
 & $\vert$ & $Constant$ \\
 & $\vert$ & $Variable$ \\
 \\
$Quantifier$ & $\rightarrow$ & $\forall$ $\vert$ $\exists$ \\
$Constant$ & $\rightarrow$ & $A$ $\vert$ $X_1$ $\vert$ $John$ $\vert$ $...$ \\
$Variable$ & $\rightarrow$ & $a$ $\vert$ $x$ $\vert$ $s$ $\vert$ $..$ \\
$Predicate$ & $\rightarrow$ & $True$ $\vert$ $False$ $\vert$ $After$ $\vert$ $Loves$ $\vert$ $Raining$ $\vert$ $...$\\
$Function$ & $\rightarrow$ & $FullName$ $\vert$ $PhoneNumber$ $\vert$ $...$ \\
\\
$OPERATOR$ $PRECEDENCE$ & $:$ & $\neg,=,\wedge,\vee,\implies,\iff$ \\
\end{tabular}
\caption{First-order logic syntax}
\label{ref:firstordersyntax}
\end{figure}



\section{Higher-order logic}
Views relations and functions as objects themselves. Therefore, it allows more powerful expressions, e.g. relations on relations: $\forall_R$ $Transitive(R)\iff \forall a,b,c$ $R(a,b)\wedge R(b,c) \implies R(a,c)$

\section{Solving logic puzzles}
The {\bf zebra puzzle} is a well-known {\bf logic puzzle} invented by Albert Einstein:

\begin{enumerate}
\item There are five houses.
\item The Englishman lives in the red house.
\item The Spaniard owns the dog.
\item Coffee is drunk in the green house.
\item The Ukrainian drinks tea.
\item The green house is immediately to the right of the ivory house.
\item The Old Gold smoker owns snails.
\item Kools are smoked in the yellow house.
\item Milk is drunk in the middle house.
\item The Norwegian lives in the first house.
\item The man who smokes Chesterfields lives in the house next to the man with the fox.
\item Kools are smoked in the house next to the house where the horse is kept.
\item The Lucky Strike smoker drinks orange juice.
\item The Japanese smokes Parliaments.
\item The Norwegian lives next to the blue house.
\end{enumerate}

There are two questions:
\begin{enumerate}
\item Who drinks water?
\item Who owns the zebra?
\end{enumerate}

Answers:
\begin{enumerate}
\item The Norwegian drinks water
\item The Japanese owns the zebra
\end{enumerate}

Algorithm~\ref{python:zebrapuzzleslow} is an initial implementation to solve the zebra puzzle. As there are $5!^5$ permutations, the algorithm is too slow. It uses an iterator to avoid far indentations.
\\
There are multiple strategies to solve this problem faster. First, {\em Prolog} could be used which creates new problem as the language is difficult to use.
\\
Second, the Python code can be boosted by reshuffling the loops and conditions to exclude unsuccessful paths as soon as possible as implemented in Algorithm~\ref{python:zebrapuzzlefast}.
It takes less than 1 millisecond to solve the problem on a modern computer.
This result might have {\bf measurement bias}\footnote{Todd Mytkowicz et al. {\em Producing Wrong Data Without Doing Anything Obviously Wrong!}. \url{www-plan.cs.colorado.edu/diwan/asplos09.pdf}}, nonetheless it indicates a significant speedup because of logical improvements.
\\
This example demonstrates that computationally difficult problems can be speeded up by applying logic and proper heuristics.

\begin{algorithm}
\caption{Inefficient solution of the zebra puzzle}
\label{python:zebrapuzzleslow}
\begin{python}
import itertools

def imright(h1, h2): return h1 - h2  == 1

def nextto(h1, h2): return abs(h1 - h2) == 1

def zebra_puzzle_slow():
    houses = [first, _, middle, _, _] = [1, 2, 3, 4, 5]
    orderings = list(itertools.permutations(houses))
    return next((WATER, ZEBRA)
                for (Englishman, Spaniard, Ukranian,
                        Japanese, Norwegian) in orderings
                for (red, green, ivory, yellow, blue) in orderings
                for (dog, snails, fox, horse, ZEBRA) in orderings
                for (coffee, tea, milk, oj, WATER) in orderings
                for (OldGold, Kools, Chesterfields, LuckyStrike,
                        Parliaments) in orderings
                if Englishman is red
                if Spaniard is dog
                if coffee is green
                if Ukranian is tea
                if imright(green, ivory)
                if OldGold is snails
                if Kools is yellow
                if milk is middle
                if Norwegian is first
                if nextto(Chesterfields, fox)
                if nextto(Kools, horse)
                if LuckyStrike is oj
                if Japanese is Parliaments
                if nextto(Norwegian, blue)
                )
\end{python}
\end{algorithm}

\begin{algorithm}
\caption{Efficient solution of the zebra puzzle}
\label{python:zebrapuzzlefast}
\begin{python}
def zebra_puzzle():
    houses = [first, _, middle, _, _] = [1, 2, 3, 4, 5]
    orderings = list(itertools.permutations(houses))
    return next((WATER, ZEBRA)
                for (red, green, ivory, yellow, blue) in orderings
                if imright(green, ivory)
                for (Englishman, Spaniard, Ukranian,
                         Japanese, Norwegian) in orderings
                if Englishman is red
                if Norwegian is first
                if nextto(Norwegian, blue)
                for (coffee, tea, milk, oj, WATER) in orderings
                if coffee is green
                if Ukranian is tea
                if milk is middle
                for (OldGold, Kools, Chesterfields, LuckyStrike,
                        Parliaments) in orderings
                if Kools is yellow
                if LuckyStrike is oj
                if Japanese is Parliaments
                for (dog, snails, fox, horse, ZEBRA) in orderings
                if Spaniard is dog
                if OldGold is snails
                if nextto(Chesterfields, fox)
                if nextto(Kools, horse)
                )
\end{python}
\end{algorithm}


\chapter{Probabilistic reasoning}
Independence and conditional independence relationships allow to simply represented probabilistic representation.
This chapter introduces {\bf Bayes networks} which are a systematic way to explicitly represent such relationships.
There are also efficient ways how to do inference on them to learn probabilities of variables.

\section{Bayes networks}
Bayes networks define probability distributions over random variables and allow compact specification of full joint distributions. The joint probability of the network shown in Figure~\ref{ref:samplenetwork} is: $P(A,B,C,D,E)=P(A)(B)P(C\vert A,B)P(D\vert C)P(E\vert C)$. The general equation is:
\begin{definition}
$P(x_1,...,x_n) = \prod_{i=1}^{n}{P(x_i\vert parents(x_i))}$
\end{definition}


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\node[mynode,below left=1cm of C] (D) {D};
\node[mynode,below right=1cm of C] (E) {E};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D)
(C) edge[-latex] (E);
\end{tikzpicture}
\caption{Sample Bayes network}
\label{ref:samplenetwork}
\end{figure}

\section{d-separation}
Stands for direction-dependent separation. d-separated variables are independent. $X$ and $Y$ are d-separated if there is no active path between them.
Paths consists of {\bf triplets} as defined in Table~\ref{ref:triplets} in which a filled circle represents a known probability. An inactive triplet makes an entire path inactive. \\


\begin{table}[h!]
\begin{center}
\begin{tabular}{c|c}
Active & Inactive \\
\hline
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
causal chain
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A, fill=black] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
common cause
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode, fill=black] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode,fill=black] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
common effect
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\node[mynode,below=0.5cm of C,color=white] (D) {};
\node[mynode,below=0cm of D, fill=black] (E) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D);
\end{tikzpicture}
common effect with observed descendant
&
\\
\end{tabular}
\end{center}
\caption{Active and inactive triplets with known variables filled}
\label{ref:triplets}
\end{table}

For Figure~\ref{ref:samplenetwork}, there are the following (conditional) independences:
\begin{itemize}
\item $A \independent B$
\item $A \independent E \vert C$ and $B \independent D \vert C$
\item $D \independent E \vert C$
\end{itemize}



Figure~\ref{ref:sampledseparation1} is a sample graph\footnote{Prof. Abbeel. {\em D-separation}. \url{http://www.youtube.com/watch?v=i0CGsHhjISU}}. Is $A$ guaranteed to be independent of $D$?
\begin{itemize}
\item Path $D\rightarrow C\rightarrow A$ is an \underline{active} triplet (causal chain) $\implies$ active
\end{itemize}
Since there is an active path paths between $A$ and $D$, $A$ and $D$ are not independent.

Figure~\ref{ref:sampledseparation2} is another sample graph. Is $D$ guaranteed to be independent of $C$, given $E$?
\begin{itemize}
\item Path $C\rightarrow A\rightarrow D$ is an \underline{inactive} triplet (common effect) $\implies$ inactive
\item Path $C\rightarrow B\rightarrow E\rightarrow D$ contains the \underline{active} triplet $C\rightarrow B\rightarrow E$ (common cause) and the \underline{inactive} triplet $B\rightarrow E\rightarrow D$ (causal chain) $\implies$ inactive
\item Path $C\rightarrow B\rightarrow E\rightarrow A \rightarrow D$ contains the \underline{active} triplet $C\rightarrow B\rightarrow E$ (common cause), the \underline{inactive} triplet $B\rightarrow E\rightarrow A$ (causal chain) and the \underline{inactive} triplet $E\rightarrow A\rightarrow D$ (common effect) $\implies$ inactive
\item Path $C\rightarrow A\rightarrow E\rightarrow D$ contains the \underline{inactive} triplet $C\rightarrow A\rightarrow E$ (common effect) and the \underline{inactive} triplet $A\rightarrow E\rightarrow D$ (causal cause) $\implies$ inactive
\end{itemize}
Since all path paths between $D$ and $C$ are inactive, $D$ and $C$ are independent, given $E$.



\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (B) {B};
\node[mynode,below left=0.5cm and 0.4 of B] (C) {C};
\node[mynode,below right=0.5cm and 0.4 of B] (D) {D};
\node[mynode,below left=-1cm and 0.4 of C] (A) {A};
\node[mynode,below right=0.5cm and 0.05 of C] (E) {E};
\path (B) edge[-latex] (C)
(B) edge[-latex] (A)
(B) edge[-latex] (D)
(C) edge[-latex] (A)
(D) edge[-latex] (C)
(D) edge[-latex] (E);
\end{tikzpicture}
\caption{Sample graph for d-separation}
\label{ref:sampledseparation1}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (B) {B};
\node[mynode,below left=0.5cm and 0.4 of B] (C) {C};
\node[mynode,below right=0.5cm and 0.4 of B,fill=gray] (E) {E};
\node[mynode,below left=0.5cm and 0.05 of C] (A) {A};
\node[mynode,below right=0.5cm and 0.05 of C] (D) {D};
\path (B) edge[-latex] (C)
(B) edge[-latex] (E)
(C) edge[-latex] (A)
(D) edge[-latex] (A)
(E) edge[-latex] (A)
(E) edge[-latex] (D);
\end{tikzpicture}
\caption{Sample graph for d-separation given $E$}
\label{ref:sampledseparation2}
\end{figure}


\section{Explaining away}
{\bf Explaining away} is special instance of inference on Bayes networks. Given Figure~\ref{figure:explaining away}, a person if happy (H) if it is sunny (S) or if the person got a salary raise (R) or both.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,ellipse,align=center}
]
\node[mynode] (C) {Happy};
\node[mynode,above left=1cm of C] (A) {Sunny};
\node[mynode,above right=1cm of C] (B) {Raise};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\caption{Explaining away}
\label{figure:explaining away}
\end{figure}

If the person is happy, sunny weather can {\bf explain away} the cause of happiness because it makes a salary raise less likely.
The happiness can be explained by the sunny weather.
In contrast, if the person is happy, but the weather is rainy, a salary raise is more likely to cause the happiness.
\\
The given probabilities are:
\begin{itemize}
\item $P(S) = 0.7$
\item $P(R) = 0.01$
\item $P(H\vert S, R) = 1$
\item $P(H\vert \neg S, R) = 0.9$
\item $P(H\vert S, \neg R) = 0.7$
\item $P(H\vert \neg S, \neg R) = 0.1$
\end{itemize}

This implies various probabilities:
\begin{itemize}
\item $R\independent S \implies P(R\vert S) = 0.01$
\item $P(H\vert S) = P(H\vert S, R)P(R) + P(H\vert S, \neg R)P(\neg R) = 0.703$
\item $P(H\vert \neg S) = P(H\vert \neg S, R)P(R) + P(H\vert \neg S, \neg R)P(\neg R) = 0.108$
\item $P(H) = P(H\vert S)P(S) + P(H\vert \neg S)P(\neg S) = 0.5245$
\end{itemize}

The probability of a salary raise given happiness and sunny weather is (applying general Bayes rule as defined in Chapter~\ref{chapter:bayesrule}) is very low:
\begin{align*}
P(R\vert H,S) = \frac{P(H\vert R, S)P(R\vert S)}{P(H\vert S)} = 0.0142
\end{align*}

Therefore, a salary raise is unlikely to cause the happiness when the weather is sunny. The weather then {\bf explains away} the cause of happiness.
\\
Comparing this to the probability of a salary raise given only happiness and not knowing anything about the weather helps to understand the explain away effect:
\begin{align*}
P(R\vert H) = \frac{P(H\vert R)P(R)}{P(H)} = 0.0185
\end{align*}

Not knowing if the weather is sunny makes the chance of a salary raise having caused the happiness is higher.
\\
The most extreme case of making a salary raise likely for happiness is to assume bad weather which definitely does not cause the happiness:
\begin{align*}
P(R\vert H,\neg S) = \frac{P(H\vert R, \neg S)P(R\vert \neg S)}{P(H\vert \neg S)} = 0.0833
\end{align*}

In general, if a certain effect could be caused by multiple causes, seeing one of those causes can {\bf explain away} any other potential cause of this effect.
Explaining away occurs when there is a {\bf noisy OR} configuration in the network.


\section{Noisy OR}
In order to make {\bf conditional probability tables} (CPTs) more compact, some causal relationships can be models as an OR of the negation of causes. Noisy-OR is a generalization of the logical OR.
In general, noisy logical relationships in which a variable depends on $k$ parents can be described using $O(k)$ parameters instead of $O(2^k)$ for the full CPT.
This makes assessment and learning much easier.
It makes two assumptions: First, it assumes that all possible causes are listed and second that each cause is independent of the others.
\\
The probability that none of the causes caused effect $E$ is the product of the probabilities that each one {\bf did not} cause $E$:
\begin{align*}
P(E\vert parents(E)) = P(E\vert C_1,...,C_n) = 1 - \prod_i (1 - P(E\vert C_i))
\end{align*}

\section{Markov chains}
\label{ref:markovchains}
A {\bf Markov chain} is a temporal Bayes network which consists of states and transition probabilities. Figure \ref{ref:samplemarkovchain} represents a sample Markov chain.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (R) {R};
\node[mynode, right=1.5cm of R] (S) {S};
\path (R) edge [->,bend left] node[above] {0.4} (S)
(R) edge [loop above] node {0.6} (R)
(S) edge [->,bend left] node[below] {0.2} (R)
(S) edge [loop above] node {0.8} (S);
\end{tikzpicture}
\caption{Sample Markov chain}
\label{ref:samplemarkovchain}
\end{figure}

Given $P(R_0)=1$, the following probabilities can be computed using the law of total probability:
\begin{itemize}
\item $P(R_1)=0.6$
\item $P(R_2)=0.44$
\item $P(R_3)=0.377$
\end{itemize}

For $P(R_\infty)=\lim\limits_{t \rightarrow \infty P(R_t)}$, the Markov chain settles to the {\bf stationary distribution}. This can be calculated by reexpressing the convergence:
\begin{align*}
P(R_t) = P(R_{t-1}) = P(R_t\vert R_{t-1})P(R_{t-1}) + P(R_t\vert S_{t-1})P(S_{t-1})
\end{align*}
By renaming $P(R_{t-1})$ to $x$, this equation can be solved:
\begin{align*}
x &= P(R_t\vert R_{t-1})x + P(R_t\vert S_{t-1})(1-x) &\\
x &= 0.6x + 0.2(1-x) \\
x &= \frac{1}{3}
\end{align*}
Therefore, $P(R_\infty)=\frac{1}{3}$ and $P(S_\infty)=\frac{2}{3}$. \\
The probabilities can be also learned from a data set. For $RSSSRSR$, the probabilities are:
\begin{itemize}
\item $P(R_0)=1$
\item $P(S\vert R)=1$
\item $P(S\vert S)=0.5$
\item $P(R\vert S)=0.5$
\item $P(R\vert R)=0$
\end{itemize}
{\bf Laplacian smoothing} can be used to reduce overfitting (e.g. all data sets start in $R$). For $K=1$, the probabilities are
\begin{itemize}
\item $P(R_0)=\frac{1+1}{1+2}=\frac{2}{3}$
\item $P(S\vert R)=\frac{2+1}{2+2}=\frac{3}{4}$
\item $P(S\vert S)=\frac{2+1}{4+2}=\frac{1}{2}$
\item $P(R\vert S)=\frac{2+1}{4+2}=\frac{1}{2}$
\item $P(R\vert R)=\frac{0+1}{2+2}=\frac{1}{4}$
\end{itemize}


\chapter{Learning}

{\bf Machine learning} is the field of study that gives computers the ability to learn without being explicitly programmed.
This means that machine learning algorithms learn models from data.
This is different to Bayes networks which reasons with known models.
\\
Another definition of machine learning is:
\begin{flushright}
{\em "A computer program is said to learn from {\bf experience E} with respect to some class of {\bf tasks T} and {\bf performance measure P}, if its performance at tasks in T, as measured by P, improves with experience E." (Mitchell)}
\end{flushright}

Example: feeding a learning algorithm a lot of historical weather data and have it learn to predict weather.
\begin{itemize}
\item T: predicting weather
\item E: the process of the algorithm examining a large amount of historical weather data
\item P: the probability of it correctly predicting a future date's weather
\end{itemize}


{\bf Statistics} focuses on analyzing existing data and drawing valid conclusions, whereas machine learning focuses on focuses on making predictions.
\\
\\
The following items give an overview of machine learning and possible applications.
\\
{\bf What}:
\begin{itemize}
  \item Parameters
  \item Structure
  \item Hidden concepts
\end{itemize}

{\bf What from}:
\begin{itemize}
  \item Supervised
  \item Unsupervised
  \item Reinforcement
\end{itemize}

{\bf What for}:
\begin{itemize}
  \item Prediction
  \item Diagnostics
  \item Summarization
  \item ...
\end{itemize}

{\bf How}:
\begin{itemize}
  \item Passive: learning agent is just an observer and has no impact on the data itself
  \item Active
  \item Online: learning while data is being generated
  \item Offline: learning by processing data in batch
\end{itemize}

{\bf Outputs}:
\begin{itemize}
  \item Classification: discrete values
  \item Regression: continuous
\end{itemize}

{\bf Details}:
\begin{itemize}
  \item Generative: model data as generally as possible
  \item Descriminative: seek to distinguish data
\end{itemize}


\chapter{Supervised learning}
Supervised learning learns from {\bf labeled training examples}. If there are not sufficiently many features, the learning algorithm may {\bf underfit}.
In contrast, it may {\bf overfit} for too many features meaning it only matches the training data well but fails to generalize to new examples.
The vast majority of research in machine learning has been done in supervised learning.
Neural networks are covered in Chapter~\ref{chapter:neuralnetworks}.

\section{Concept learning}
{\bf Concept learning} infers a Boolean-valued function from training data and is a prototype binary classification if data belongs to the target concept or not.
\\
It is also called {\bf inductive learning}, meaning that the system tries to induce a "general rule" from a training set.
\\
The aim is to find a hypothesis $h\in H$ such that $\forall x\in X: (h(x) - c(x)) < \epsilon \approx 0$ where each $x^{(i)}_j$ may be $?$ (any value is acceptable), $0$ (no value is acceptable) or a specific value.
\\
Many concept learning algorithms utilize general-to-specific ordering of hypotheses:
\begin{itemize}
\item $h_1$ precedes (is more general than) $h_2 \iff \forall x\in X: [(h_2(x)=1)\implies (h_1(x)=1)]$ \\
e.g. $h_1 = (?,?,yes,?,?,?)$ and $h_2 = (?,?,yes,?,?,yes) \implies h_1 >_g h_2$
\item $h_2$ succeeds (is more specific than) $h_1 \iff \forall x\in X: [(h_2(x)=1)\implies (h_1(x)=1)]$ \\
e.g. $h_1 = (?,?,yes,?,?,?)$ and $h_2 = (?,?,yes,?,?,yes)  \implies h_2 <_g h_1$
\item $h_1$ and $h_2$ are of equal generality $\iff \exists x\in X: [(h_1(x)=1)\iff (h_2(x)=1)]$ \\
e.g. $h_1 = (?,?,yes,?,?,?)$ and $h_2 = (?,?,?,?,?,yes) \implies h_1 =_g h_2$
\end{itemize}

Algorithm~\ref{alg:finds} returns the most specific hypothesis $h$ that best fits the positive training set. The hypothesis returned by Find-S will also fit negative examples as long as training examples are correct.
\\
Find-S has a number of drawbacks:
\begin{itemize}
\item is sensitive to noise that is present in most training examples
\item There is not guarantee that $h$ is the only $h$ that fits the data
\item Several maximally specific hypotheses may exist that fit the data, but Find-S outputs only one
\item Why should one prefer most specific hypotheses over most general hypotheses?
\end{itemize}

\begin{algorithm}
\caption{Find-S}
\label{alg:finds}
\begin{algorithmic}
\State $h \gets (0,...,0)$ \Comment{Initialize $h\in H$ to the most specific hypothesis}
\For{each positive training example $x \in X$}
\For{eachatrribute $a_i \in h$}
\If{$a_i$ is satisfied by $x$}
\State Do nothing
\Else
\State Replace $a_i$ in $h$ so that $h' >_g, h\gets h'$
\EndIf
\EndFor
\EndFor
\\
\Return{$h$}
\end{algorithmic}
\end{algorithm}

The following example demonstrates the algorithm by finding the smallest rectangle that contains all positive coordinates in Table~\ref{table:coordinates}. Table~\ref{table:findscomputation} contains the steps of the computation of the hypothesis.

\begin{figure}[h!]
\centering
\begin{tabular}{c||c|c}
Class & $x$ & $y$ \\
\hline
\hline
+ & 4 & 4 \\
- & 1 & 1 \\
+ & 5 & 3 \\
- & 10 & 10 \\
+ & 6 & 5
\end{tabular}
\caption{Sample coordinates}
\label{table:coordinates}
\end{figure}


\begin{figure}[h!]
\centering
\begin{tabular}{c||c|c|c|c||l}
Step & $x_1$ & $y_1$ & $x_2$ & $y_2$ & Comments \\
\hline
\hline
0 & 0 & 0 & 0 & 0 & Initial value \\
1 & 4 & 4 & 4 & 4 & (4,4) is not in $h$, change $h$ so it just contains (4,4) \\
2 & 4 & 3 & 5 & 4 & (5,3) is not in $h$, enlarge $h$ to fit both \\
3 & 4 & 3 & 6 & 5 &  Enlarge again
\end{tabular}
\caption{Find-S hypothesis computation}
\label{table:findscomputation}
\end{figure}


The last three drawbacks of Find-S are addressed by Algorithm~\ref{alg:candiateelimination}. The main idea is to return a set of hypotheses $VS\subseteq H$ that are consistent with data $X$.

\begin{algorithm}
\caption{Candidate elimination}
\label{alg:candidateelimation}
\begin{algorithmic}
\State $G\in VS \gets (?,...,?)$ \Comment{Initialize to the most general hypothesis}
\State $S\in VS \gets (0,...,0)$ \Comment{Initialize to the most specific hypothesis}
\For{each positive training example $x \in X$}
\If{$x$ is a positive example}
\State Remove from $G$ all $h$ that are not consistent with $x$
\For{each hypothesis $s \in S$ that is not consistent with $x$}
\State Replace $s$ with all $h$ that are consistent with $x, h>_g s, h<_g g\in G$
\State Remove from $S$ all $s$ being more general than other $s$ in $S$
\EndFor
\EndIf

\If{$x$ is a negative example}
\State Remove from $S$ all $h$ that are not consistent with $x$
\For{each hypothesis $g \in G$ that is not consistent with $x$}
\State Replace $g$ with all $h$ that are consistent with $x, g>_g h, h>_g s\in S$
\State Remove from $G$ all $g$ being less general than other $g$ in $G$
\EndFor
\EndIf
\EndFor
\\
\Return{hypotheses $G$ and $S$}
\end{algorithmic}
\end{algorithm}



\section{Decision trees}
{\bf Decision tree} learning algorithms are simple and successful forms of machine learning. A decision tree maps an input vector to a single output value, the "decision". Input and output values can be discrete or continuous. This section only covers decision trees for classification which map a discrete input vector to a discrete output value, the class. \\
A decision tree performs a sequence of tests on the input data and eventually generates a decision. This decision can also be expressed in propositional logic by aggregating all paths leading to {\em true}:
\begin{align*}
Goal \iff (Path_1 \vee Path_2 \vee ...)
\end{align*}
Decision trees are suitable for describing a variety of problems. Some problems cannot be described properly using decision trees such as the majority function which returns true iff more than half of the inputs are true. This function requires an exponentially large decision tree. \\
A decision tree can be learned from a training set. Finding the {\bf optimal decision tree} is NP-complete as there are $2^{2^n}$ many possible trees. With some heuristics, one can find a good approximate solution which is a small (but not the smallest) tree. \\
The sample data defined in Figure~\ref{table:decisiontreeexamples} is used in this section with the following attributes:
\begin{itemize}
\item {\em Alternate}: whether there is a suitable alternative restaurant nearby
\item {\em Bar}: whether the restaurant has a comfortable bar area to wait in
\item {\em Fri/Sat}: true on Fridays and Saturdays
\item {\em Hungry}: whether we are hungry
\item {\em Patrons}: how many people are in the restaurant (values are None, Some and Full)
\item {\em Price}: the restaurant's price range
\item {\em Raining}: whether it is raining outside
\item {\em Reservation}: whether we made a reservation
\item {\em Type}: the kind of restaurant (French, Italian Thai, or burger)
\item {\em WaitEstimate}: the wait estimated by the host (0-10 minutes, 10-30, 30-60 or $>$60)
\end{itemize}

{\em WillWait} is the goal of the training data set.

\begin{figure}[h!]
\centering
\begin{tabular}{c||c|c|c|c|c|c|c|c|c|c||c}
 & Alt & Bar & Fri & Hun & Pat & Price & Rain & Res & Type & Est & WillWait \\
\hline
\hline
$x^{(1)}$ & T & F & F & T & Some & \$\$\$ & F & T & French & 0-10 & T \\
$x^{(2)}$ & T & F & F & T & Full & \$ & F & F & Thai & 30-60 & F \\
$x^{(3)}$ & F & T & F & F & Some & \$ & F & F & Burger & 0-10 & T \\
$x^{(4)}$ & T & F &T & T & Full & \$ & T & F & Thai & 30-60 & T \\
$x^{(5)}$ & T & F & T & F & Full & \$\$\$ & F & T & French & $>$60 & F \\
$x^{(6)}$ & F & T & F & T & Some & \$\$ & T & T & Italian & 0-10 & T \\
$x^{(7)}$ & F & T & F & F & None & \$ & T & F & Burger & 0-10 & F \\
$x^{(8)}$ & F & F & F & T & Some & \$\$ & T & T & Thai & 0-10 & T \\
$x^{(9)}$ & F & T & T & F & Full & \$ & T & F & Burger & $>$60 & F \\
$x^{(10)}$ & T & T & T & T & Full & \$\$\$ & F & T & Italian & 10-30 & F \\
$x^{(11)}$ & F & F & F & F & None & \$ & F & F & Thai & 0-10 & F \\
$x^{(12)}$ & T & T & T & T & Full & \$ & F & F & Burger & 30-60 & T \\
\end{tabular}
\caption{Decision tree training set}
\label{table:decisiontreeexamples}
\end{figure}

Algorithm~\ref{alg:decisiontreelearning} learns a good approximation solution. It adopts a greedy divide-and-conquer strategy  which always tests the most important attribute first. This test divides the problem up into smaller subproblems that can then be solved recursively.

\begin{algorithm}
\caption{Decision tree learning}
\label{alg:decisiontreelearning}
\begin{algorithmic}
\Function{DT-LEARNING}{$examples$, $attributes$, $parent\_examples$}
\If{empty(examples)} \Return{PLURALITY-VAL($parent\_examples$)}
\ElsIf{all examples have same classification} \Return{the classification}
\ElsIf{empty(attributes)} \Return{PLURALITY-VAL($examples$)}
\Else
\State $A$ $\gets \max\limits_{a\in attributes}$ IMPORTANCE($a$, $examples$)
\State $tree$ $\gets$ a new decision tree with root test $A$
\For{$v_k \in A$}
\State $exs$ $\gets \{e \vert e\in examples \wedge e.A = v_k \}$
\State $subtree$ $\gets$ DT-LEARNING($exs$, $attributes - A$, $examples$)
\State add a branch to $tree$ with label $(A=v_k)$ and subtree $subtree$
\EndFor
\State \Return{$tree$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

{\em PLURALITY-VAL} selects the most common output value among a set of examples and breaks ties randomly.
\\

"Most important attribute" means the one that makes the most difference to the classification of an example. That way, the algorithm is supposed to get the correct classification with a small number of tests meaning that all generated paths in the thee will be short and the tree as a whole will be shallow. The {\em IMPORTANCE} function of Algorithm~\ref{alg:decisiontreelearning} can be defined in terms of {\bf information gain} which uses {\bf entropy} theory, the fundamental quantity in information theory.
\\
Entropy is a measure of the uncertainty of a random variable, acquisition of information corresponds to a reduction in entropy. Entropy is defined as
\begin{align*}
H(V) = -\sum_k P(v_k)log_2 P(v_k)
\end{align*}
with unit {\em bit}.

$B(q)$ is the entropy of a Boolean random variable that is true with probability $q$:
\begin{align*}
B(q) = -(q\mbox{ }log_2 q + (1-q)log_2(1-q))
\end{align*}
If a training set contains $p$ positive and $n$ negative examples, then the entropy of the goal attribute is
\begin{align*}
H(Goal) = B(\frac{p}{p+n})
\end{align*}

An attribute $A$ with $d$ distinct values divides the training set $E$ into subsets $E_1,...,E_d$. Each subset $E_k$ has $p_k$ positive examples $n_k$ negative examples.. Going along that branch, one needs an additional $B(p_k/(p_k+n_k))$ bits of information to answer the question. A randomly chosen example from the training set has the $k$th value for the attribute with probability $(p_k+n_k)/(p+n)$, so the expected entropy remaining after testing the attribute $A$ is
\begin{align*}
Remainder(A) = \sum_{k=1}^{d}\frac{p_k+n_k}{p+n}B(\frac{p_k}{p_k+n_k})
\end{align*}

The {\bf information gain} from attribute test on $A$ is the expected reduction in entropy:
\begin{align*}
Gain(A) = B(\frac{p}{p+n}) - Remainder(A)
\end{align*}

In fact, $Gain(A)$ is computed by the {\em IMPORTANCE} function.
\\
A learned tree for the training set in Figure~\ref{table:decisiontreeexamples} is showed in Figure~\ref{fig:learneddecisiontreeexample}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    fact/.style={rectangle, draw=none, rounded corners=1mm, fill=white,
        text centered, anchor=north, text=black},
    state/.style={rectangle, draw=none, fill=black,
        text centered, anchor=north, text=white},
    leaf/.style={rectangle, draw=none, fill=gray,
        text centered, anchor=north, text=white},
    level distance=0.5cm, growth parent anchor=south
]
\node (Patrons) [state] {Patrons?} [sibling distance=6cm]
child{
	node (PatronsNone) [fact] {None}
	child{
		node (PatronsFalse) [leaf] {False}
	}
}
child{ [sibling distance=4cm]
	node (PatronsFull) [fact] {Full}
	child{
		node (WaitEstimate) [state] {WaitEstimate?}
		child{
			node (Wait1030) [fact] {10-30}
			child{
				node (WaitFalse1) [leaf] {False}
			}
		}
		child{ [sibling distance=2cm]
			node (Wait3060) [fact] {30-60}
			child{
				node (FriSat) [state] {Fri/Sat?}
				child{
					node (FriSatFalse) [fact] {False}
					child{
						node (FriSatFalseGoal) [leaf] {False}
					}
				}
				child{
					node (FriSatTrue) [fact] {True}
					child{
						node (FriSatTrueGoal) [leaf] {True}
					}
				}
			}
		}
		child{
			node (Wait60) [fact] {$>60$}
			child{
				node (WaitFalse2) [leaf] {False}
			}
		}
	}
}
child{ [sibling distance=4cm]
	node (PatronsSome) [fact] {Some}
	child{
		node (PatronsTrue) [leaf] {True}
	}
}
;

\end{tikzpicture}
\caption{Learned decision tree}
\label{fig:learneddecisiontreeexample}
\end{figure}

Decision trees may {\bf overfit} which can be handled by {\bf pruning} long branches. \\
One important property of decision trees is that it is possible for a human to understand the reason for the output of the learning algorithm. This is a {\em legal requirement} for financial decisions that are subject to anti-discrimination laws. This is a property not shared by some other representations, such as neural networks.



\section{Linear regression}
{\bf Linear regression} computes a linear regression function for $m$ {\bf training examples} having $n$ {\bf features}. The objective is to minimize the cost function:
\begin{definition}[Cost function]
$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
\end{definition}

Where the hypothesis $h_\theta(x)$ is given by the linear model:
\begin{definition}[Hypothesis]
$h_\theta(x) = \theta^{T}x = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n$ with $x_0 = 1$
\end{definition}

The parameters of the model to be learned are the $\theta_j$ values. One way to do this is to use the batch {\bf gradient descent} algorithm.
Chapter~\ref{chapter:gradientdescent} covers gradient descent and advanced optimization methods.

\begin{definition}[Gradient descent] ~\\
loop until converge \{ \\
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (simultaneously update $\theta_j$ for all $j$) \\
\}
\end{definition}

{\bf Vectorization} allows to do the updates simultaneously through various matrix operations. Matrix multiplications can be executed very efficiently (Strassen algorithm). Matrix $X$ contains $m$ transposed training data columns and $n$ features where feature $x_0$ contains entirely 1s:
$X$ = $\begin{pmatrix}
\mbox{------} (x^{(1)})^T \mbox{------} \\
\mbox{------} (x^{(2)})^T \mbox{------} \\
... \\
\mbox{------} (x^{(m)})^T \mbox{------}
\end{pmatrix}$


The cost function and gradient descent then look:

\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{2m}(X\theta-y)^T(X\theta-y)$\\
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(h_{\theta}(x)-y) := \theta - \alpha \frac{1}{m}X^{T}(X\theta-y)$
\end{definition}

The {\bf normal equation} is a closed-form solution for linear regression which requires no loop, no learning rate and no feature normalization. It may be too slow for very large matrices as inversion is $O(n^3)$. $X$ may be singular and cannot be inverted. It is usually recommended to calculate the pseudo inverse.
\begin{definition}[Normal equation]
$\theta = (X^{T}X)^{-1}X^{T}y$
\end{definition}

\section{Logistic regression}

Logistic regression allows linear classification. The parameters $\theta_j$ comprise the linear separator called {\bf decision boundary}. The hypothesis is defined as:
\begin{definition}[Hypothesis]
$h_\theta(x) = g(\theta^{T}x)$
\end{definition}

The hypothesis can be more complex to use logistic regression for non-linear classification. Once trained, logistic regression predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). $g$ is the {\bf sigmoid function} which is defined as:
\begin{definition}[Sigmoid function]
$g(z) = \frac{1}{1 + e^{-z}}$
\end{definition}

The cost function in logistic regression is convex:
\begin{definition}[Cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$
\end{definition}

The {\bf cross-entropy error function} is the best maximum likelihood estimate for logistic regression.
Using it instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization.


\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{m}[-y^T log(g(X\theta))-(1-y)^T log(1-g(X\theta))]$
\end{definition}

This gradient descent looks identical to the linear regression gradient but the formula is actually different because of the different definition of $h_{\theta}(x)$.
\begin{definition}[Gradient descent]
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(g(X\theta)-y)$
\end{definition}


\section{Regularization}
{\bf Regularization} allows to prevent overfitting by "penalizing" large $\theta_j$ values. Feature $x_0=1$ is usually not affected by regularization. If $\lambda$ is set too large, the learning algorithm may underfit. For linear regression, the definitions change to:

\begin{definition}[Regularized linear regression cost function] ~\\
$J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+ {\bf \lambda \sum_{j=1}^n\theta_j^2}]$
\end{definition}

\begin{definition}[Regularized linear regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} {\bf +\frac{\lambda}{m}\theta_j}]$
\end{definition}

Regularization also allows the normal equation to be invertible by adding a $(n+1)\times (n+1)$ matrix:
\begin{definition}[Regularized normal equation] ~\\
$\theta = (X^{T}X+\lambda
\begin{pmatrix}
0 & & & \\
&1& &\\
& & ... & \\
& & & 1
\end{pmatrix}
)^{-1}X^{T}y$
\end{definition}

For logistic regression, the definitions change to:

\begin{definition}[Regularized logistic regression cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))] {\bf + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2}$
\end{definition}

\begin{definition}[Regularized logistic regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ) {\bf + \frac{\lambda}{m}\theta_j}]$
\end{definition}


\section{Naive Bayes}
{\bf Bayes classifiers} rely on Bayes rule which maps a vector $X$ to a discrete random variable $C$ - a class. Training such a classifier is impractical because of the necessary exponential amount of training examples. A {\bf naive Bayes classifier} relaxes conditional dependency by assuming conditional independence between the parameters in $X$ given $C$. Naive Bayes is a powerful classifier that is often used in text classification (e.g. spam filtering) and is a dependable baseline when comparing different classifiers. Usually, it works on a so-called {\bf bag of words} which is a frequency of words in a text.
\begin{align*}
P(C\vert X) = P(C\vert x_1...x_n) = \frac{P(C)P(x_1...x_n\vert C)}{P(x_1...x_n)}
\end{align*}
\begin{align*}
= \frac{P(C)P(x_1\vert C)...P(x_n\vert C)}{P(x_1)...P(x_n)}
\end{align*}

To get the most likely class, the denominator can get dropped as it is independent of $C$.

\begin{align*}
C = \max\limits_{C} P(C)\prod_iP(x_i\vert C)
\end{align*}

\subsection{Using naive Bayes}
The probability:
\begin{align*}
P(x_i) = \frac{count(x_i)}{N}
\end{align*}
might be zero and would then reduce the entire product to zero. {\bf Laplacian smoothing} can deal with this overfitting problem:
\begin{align*}
P(x_i) = \frac{count(x_i)+k}{N+k\vert X\vert}
\end{align*}
It adds $k$ to the numerator and normalizes by adding $k$ to every single class of $X$. This change to the normalizer guarantees that $\sum_i P(x_i) = 1$.
Using a {\bf cross validation set} helps to find the best $k$, see Chapter~\ref{ref:modelselection}.
\\
\\
To prevent numerical underflow while multiplying lots of probabilities, naive Bayes can add the logarithms of these probabilities:
\begin{align*}
C = \max\limits_{C} log(P(C))+\sum_i log(P(x_i\vert C))
\end{align*}

\subsection{Spam classification example}
In the section, naive Bayes is demonstrated on a spam filter example. Bad messages are called "Spam", good messages are "ham". The sample messages are shown in Figure~\ref{ref:spamsample}.

\begin{figure}[h!]
\centering
\begin{tabular}{c||c|c}
& SPAM & HAM \\
\hline
\hline
1 & OFFER IS SECRET & PLAY SPORTS TODAY \\
2 & CLICK SECRET LINK & WENT PLAY SPORTS \\
3 & SECRET SPORTS LINK & SECRET SPORTS EVENT \\
4 & & SPORTS IS TODAY \\
5 & & SPORTS COSTS MONEY \\
\end{tabular}
\caption{Sample messages}
\label{ref:spamsample}
\end{figure}

The following non-smoothed ($K=0$) probabilities can then be computed which are based on the empirical count (can be derived by maximum likelihood estimation defined in Chapter~\ref{ref:estimationsection}):
\begin{itemize}
\item $P(SPAM) = 3/8$
\item $P("SECRET"\vert SPAM) = 1/3$
\item $P("SECRET"\vert HAM) = 1/15$
\item $P(SPAM\vert "SPORTS") = \frac{P("SPORTS"\vert SPAM)P(SPAM)}{P("SPORTS")}=\frac{1/9 * 3/8}{6/24}=3/18$
\item $P(SPAM\vert "SECRET\,IS\,SECRET")= \frac{P("SECRET\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("SECRET\:IS\:SECRET")}$\\
$=\frac{1/3*1/9*1/3*3/8}{1/3*1/9*1/3*3/8+1/15*1/15*1/15*5/8}=\frac{1/216}{1/216+1/5400}=\frac{25}{26}$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{0*1/9*1/3*3/8}{0*1/9*1/3*3/8+2/15*1/15*1/15*5/8}=0$
\end{itemize}

The last example demonstrates a clear overfit. The following smoothed ($K=1$) probabilities can be computed. The last examples how the model deals with with the nonexistence of "TODAY" in the spam messages.
\begin{itemize}
\item $P(SPAM) = \frac{3+{\bf 1}}{8+{\bf 2}} = 2/5$
\item $P(HAM) = \frac{5+{\bf 1}}{8+{\bf 2}} = 3/5$
\item $P("TODAY"\vert SPAM) = \frac{0+{\bf 1}}{9+{\bf 12}} = 1/21$
\item $P("TODAY"\vert HAM) = \frac{2+{\bf 1}}{15+{\bf 12}} = 1/9$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{1/21*2/21*4/21*2/5}{1/21*2/21*4/21*2/5+3/27*2/27*2/27*3/5}=0.4858$
\end{itemize}

This simple spam filter is only illustrative and will perform poorly on real-world data. Nonetheless, other factors such as case, links and others can be added to the products. Support vector machines can be used to create very powerful spam filters as demonstrated in Andrew Ng's lecture.


\subsection{Comparison to logistic regression}
Naive Bayes requires only a small amount of data, converges quickly and is very fast. In comparison, logistic regression requires more data and computations are more expensive. Nonetheless, it can outperform naive Bayes on a lot of data.


\section{Maximum a posteriori and maximum likelihood}
\label{ref:estimationsection}
In non-technical language, "likelihood" is usually a synonym for "probability", but in statistical usage, a clear distinction is: the {\em probability} of some observed outcomes given a set of parameter values is referred to as the {\bf likelihood} of the set of parameter values given the observed outcomes. Therefore $P(\theta\vert X)$ can be interpreted as $L(X\vert\theta)$.

\subsection{Maximum a posteriori}
The Bayes {\bf maximum a posteriori} (MAP) estimate for $\theta$ is given by:
\begin{align*}
\theta_{MAP}=\max\limits_{\theta} \prod_{i=1}^m p(y^{(i)}\vert x^{(i)},\theta)p(\theta)
\end{align*}
The data are distributed {\bf i.i.d.} (independently and identically distributed). $\theta$ is treated as a random variable in Bayesian statistics.
In practical applications, a common choice for the prior $p(\theta)$ is to assume that it has a normal distribution.
Using this choice of prior, the fitted parameters $\theta$ have smaller norm than that selected by {\bf Maximum Likelihood} (ML).
In practice, this causes the MAP estimate to be less susceptible to overfitting than the ML estimate. MAP estimation can therefore be seen as a regularization of ML estimation.
\\
Bayesian classification turns out to be an effective algorithm for text classification, even though in text classification there can be significantly more features than training examples.


\subsection{Maximum likelihood}
{\bf Maximum likelihood} (ML) estimation is related to MAP but simpler:
\begin{align*}
\theta_{ML}=\max\limits_{\theta} \prod_{i=1}^m p(y^{(i)}\vert x^{(i)};\theta)
\end{align*}
This definition is similar to the ML estimate for $\theta$, except for the prior $p(\theta)$ term at the end. 
Also, $\theta$ is treated as a fixed but unknown variable \footnote{Written $ p(y^{(i)}\vert x^{(i)};\theta)$ instead of $ p(y^{(i)}\vert x^{(i)},\theta)$}. For example, ML can be used to derive the least-squares cost function of linear regression assuming that the error terms $\epsilon^{(i)}$ are i.i.d. according to a normal distribution:
\begin{align*}
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
\end{align*}
The density of $\epsilon^{(i)}$ is given by:
\begin{align*}
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})
\end{align*}
This implies that:
\begin{align*}
p(y^{(i)}\vert x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
The likelihood of $\theta$ can then be written:
\begin{align*}
L(\theta)=\prod_{i=1}^m p(y^{(i)}\vert x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
Instead of maximizing $L(\theta)$, any strictly increasing function of $L(\theta)$ can be increased. Often the {\bf log likelihood} is maximized as it allows to change products to summations which are usually easier to maximize:
\begin{align*}
l(\theta)=\mbox{log } L(\theta)=\mbox{log }\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
\begin{align*}
=\sum_{i=1}^m \mbox{log }[\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})]=m\mbox{log }[\frac{1}{\sqrt{2\pi}\sigma}] -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
\end{align*}
Hence, maximizing $l(\theta)$ gives the same answer as {\bf minimizing}:
\begin{align*}
\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
\end{align*}
which is the least-square cost function of linear regression.


\section{Model selection}
\label{ref:modelselection}
{\bf Model selection} is the task of selecting a statistical model from a set of candidate models, given data. For example, different hypotheses for linear regression. Usually, the data is split up in three sets: {\bf training set} (60\%), {\bf cross validation set} (20\%) and {\bf test set} (20\%). A a non-regularized error function can be computed for each of these sets, such as for the test set:

\begin{definition}[Linear regression error function] ~\\
$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$
\end{definition}

The {\bf coefficient of determination}, denoted {\bf $R^2$}, is a statistical measure that indicates how well data points fit a statistical model - sometimes simply a line or curve.
$R^2 \in [0,1]$. An $R^2$ of $1$ indicates that the regression line perfectly fits the data.

\begin{definition}[Linear regression error function] ~\\
$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$
\end{definition}

In this context, $o^{(i)}$ are called the {\bf observed values} and $y^{(i)}$ the {\bf predicted values} by the regression (i.e. $h_{\theta}(x^{(i)})$.

$SS_{res}$ is the sum of squares of residuals (errors):
\begin{align*}
SS_{res} = \sum_i(o^{(i)} - y^{(i)})^2
\end{align*}

$SS_{tot}$ is the total sum of squares which is proportional to the sample variance:
\begin{align*}
SS_{tot} = \sum_i(o^{(i)} - \mu_o)^2
\end{align*}


For logistic regression, the error function is defined as follows:

\begin{definition}[Logistic regression error function] ~\\
$J_{test}(\theta) = -\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}[y_{test}^{(i)}log(h_\theta(x_{test}^{(i)}))+(1-y_{test}^{(i)})log(h_\theta(x_{test}^{(i)}))]$
\end{definition}

Alternatively, the following error function might be easier to interpret for logistic regression:

\begin{definition}[0/1 misclassification error] ~\\
$err(h_\theta(x),y)=
\left\{
\begin{array}{lll}
1  & \mbox{if } h_\theta(x) \geq 0.5, & y=0 \\
  & \mbox{if } h_\theta(x) < 0.5, & y=1 \\
0 & otherwise
\end{array}
\right.$ \\
$J_{test}(\theta) = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y^{(i)})$
\end{definition}


First, the learning algorithm is trained by using the training data for each hypothesis resulting in $\Theta^{(n)}$ for each hypothesis. Second, model {k} with the lowest $J_{CV}(\Theta^{(k)})$ is chosen. Last, the {\bf generalization error} can be computed for the test set $J_{test}(\Theta^{(k)})$.
\\
\\
A similar approach can be done to find a good regularization parameter $\lambda$.

\section{Debugging a learning algorithm}
\label{chapter:debugginglearning}
Implementing a supervised learning algorithm and testing it on new data might reveal unacceptably large errors in its predictions. This section is to give advice on what to do next. Implementing a {\bf diagnostic} might be time-consuming, but doing so can be a very good use of time to get directed into the right direction. In this context, underfitting is called {\bf bias} and overfitting {\bf variance}.

In non-regularized model selection, to diagnose if it is a bias problem or variance problem, the following guidelines may help:
\begin{itemize}
\item Bias: $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\item Variance: $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\end{itemize}

To determine if the regularization parameter $\lambda$ is the cause of bias and variance, the following guidelines may help for a function of $\lambda$:
\begin{itemize}
\item Variance (too small $\lambda$): $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\item Bias (too large $\lambda$): $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\end{itemize}

{\bf Learning curves} can be used to plot a learning's algorithm error for more training examples. Usually, the training error increases as the learning algorithm's hypothesis fails to fit more data. Quite the opposite, the cross validation error decreases because the algorithm generalizes better. For high bias, both errors are high and converge quickly. Therefore, more training data will not help. For high variance, there is a large gap between the cross validation error and the training error. In this case, more data is likely to help.
\\ \\
"Small" {\bf neural networks} which have fewer parameters are prone to underfitting but computationally cheaper. "Large" neural networks have more parameters and are more prone to overfitting (can be addressed by regularization) but computationally more expensive.
\\ \\
In summary, the following actions can help to solve particular problems:
\begin{itemize}
\item Get more training examples: fixes high variance
\item Try smaller set of features: fixes high variance
\item Try getting additional features: fixes high bias
\item Try adding polynomial features: fixes high bias
\item Try decreasing $\lambda$: fixes high bias
\item Try increasing $\lambda$: fixes high variance
\end{itemize}

An alternative to regularization to prevent overfitting is called {\bf early stopping}.
Usually, the CV error decreases initially, followed by an increase as the learning algorithm starts to overfit.
Therefore, training can be stopped at the point of smallest error with respect to the validation data set.

\section{Error analysis}
The recommended approach to develop a machine learning algorithm is to start with a simple algorithm that can be implemented quickly.
Then, it can be tested on a small set of data, e.g. a couple of hundreds of records. The cross-validation data can be used to plot the learning curves.
This allows to decide if more data, more features etc. are likely to help. In the {\bf error analysis} the examples on which the algorithm made errors on are manually examined.
This can help to spot any systematic trend in what type of examples it makes errors on, e.g. stemming, capitalization etc.
\\ \\
{\bf Precision} and {\bf recall} allow to describe classification errors mathematically as described in Figure~\ref{ref:classificationerror}.
Precision and and recall can be changed by amending the threshold of the classifier such as the sigmoid function of logistic regression. Usually, a tradeoff needs to be made. The {\bf $F_1$ score} allows to compare precision and recall.

\begin{figure}[h!]
\centering
\begin{tabular}{r||ccc}
& & Actual class & \\
\hline
\hline
& & 1 & 0 \\
Predicted & 1 & True positive & False positive \\
class & 0 & False negative & True negative \\
\end{tabular}
\caption{Classification and errors}
\label{ref:classificationerror}
\end{figure}


\begin{definition}[Precision]
$\frac{True\:positive}{True\:positive + False\:positive}$
\end{definition}

\begin{definition}[Recall]
$\frac{True\:positive}{True\:positive + False\:negative}$
\end{definition}

\begin{definition}[$F_1$ score]
$2\frac{PR}{P+R}$
\end{definition}

\section{Support vector machines}
A {\bf support vector machine} (SVM) is a large margin linear classifier.
\begin{definition}[SVM hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tx \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

In order to get a large margin, the motivation for the cost function definition is:
\begin{itemize}
\item If $y=1$, we want $\theta^Tx\ge 1$ (not just $\ge 0$)
\item If $y=0$, we want $\theta^Tx\le -1$ (not just $\le 0$)
\end{itemize}

\begin{definition}[SVM cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\\
$C = \frac{1}{\lambda}$
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \ge 1 \\
-z + 1  & \mbox{else}
\end{array}
\right.$ \\
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \le 1 \\
z + 1  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

SVMs can be used for non-linear classification. On the one hand, higher order polynomials can be used as in logistic regression which is relatively complex. On the other hand, SVMs can use the {\bf kernel trick} which implicitly maps inputs into high-dimensional feature spaces. In some sources, linear SVMs use a so-called "linear kernel".

\begin{definition}[Gaussian kernel]
$e^{-\frac{\lVert x - l^{(i)} \rVert^2}{2\sigma^2}}$
\end{definition}

Given $x$, new features $f$ depending on proximity to landmarks of the training set $l^{(1)}, l^{(2)}, ..., l^{(m)}$ can be computed:
\begin{itemize}
\item $f_0 = 1$
\item $f_1 = similarity(x,l^{(1)})=e^{-\frac{\lVert x - l^{(1)} \rVert^2}{2\sigma^2}}$
\item $f_2 = similarity(x,l^{(2)})=e^{-\frac{\lVert x - l^{(2)} \rVert^2}{2\sigma^2}}$
\item ...
\item $f_m = similarity(x,l^{(m)})=e^{-\frac{\lVert x - l^{(m)} \rVert^2}{2\sigma^2}}$
\end{itemize}

$f = [f_0\:f_1\:...\:f_m]^T$. $f_1^{(i)}=sim(x^{(i)},l^{(1)})$. $f^{(i)}$ is a vector of $sim(x^{(i)},l^{(k)})$ for $k \in [0,m]$. \\
If $x\approx l^{(i)}$, then $f_i\approx 1$. If $x$ far from $l^{(i)}$, then $f_i\approx 0$.

\begin{definition}[SVM kernel hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tf \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

\begin{definition}[SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\end{definition}

To optimize the training, the regularization part is slightly changed:
\begin{definition}[Optimized SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^{\bf m}\theta_j^2$
\end{definition}

Changing the SVM parameters $C(=\frac{1}{\lambda})$ and $\sigma^2$ has impact on bias an variance.  A large $C$ results in lower bias and higher variance whereas a small $C$ results in higher bias and lower variance. A large $\sigma^2$ results in higher bias and lower variance because the features $f_i$ vary more smoothly. In contrast, a small $\sigma^2$ results in lower bias and higher variance.

\section{Multiclass classification}
In {\bf multiclass classification} examples can be classified into more than two classes. Some classification algorithms permit to do this natively. Generally, this can be done using {\bf one-vs.-all} (one-vs.-rest) classification which trains one binary classifier for each class. Finally, on a new input to make a prediction, the largest classifier is picked.

\section{Instance-based learning}
{\bf Eager learning} methods construct general (one-fits-all), explicit (input independent) description of the target function based on the provided training examples.
{\bf Lazy learning} methods simply store the data and generalizing beyond these data is postponed until an explicit request is made.
Lazy learning methods can construct a different approximation to the target function for each encountered query instance. Eager learning methods use the same approximation to the target function, which must be learned based on training examples and before input queries are observed.
Lazy learning is very suitable for complex and incomplete problem domains, where a complex target function can be represented by a collection of less complex local approximations.
{\bf Parametric models} such as linear and logistic regression require a constant size of parameters that is indecent of the number of training examples. They also allow to throw away the training examples once the parameters (e.g. $\theta$ of linear regression) of the hypothesis have been computed as they summarize by the parameters. In comparison, {\bf non-parametric models} require a set of parameters that may grow if the size of the training set grows. Non-parametric learning is also called {\bf instance-based learning} or {\bf memory-based learning}.

\subsection{k-nearest neighbors}
{\bf K-nearest neighbors} KNN can be used for classification and regression. In classification, the class of an example is determined by selecting the majority class of the $K$ nearest training examples. In regression, the mean value of the $K$ nearest examples is chosen. Usually, nearest is interpreted geometrically by computing the norm.
\\
$K$ is a smoothing parameter. The larger $K$, the smoother the output.

\subsection{Case-based reasoning}
{\bf Case-based reasoning} (CBR) is reasoning by remembering: previously solved cases are used to suggest solutions for novel but similar problems. Cases contain knowledge about previous experiences (solved problems).
A case is typically composed of the problem description and the problem solution. Problem description should contain enough data for an accurate and efficient case retrieval.
Problem solution can be either atomic (e.g. an action) or compound (e.g. a sequence of actions). Cases can be either monolithic (e.g., observation $\implies$ action) or compound (e.g., a set of observations $\implies$ a sequence of actions).
Cases can be represented in various ways: feature vectors, semantic nets, objects, frames, rules.

\section{Applying PCA}
The {\bf principal component analysis} (PCA) presented in Chapter~\ref{ref:pcasection} can be used in supervised learning.
It allows to map high-dimensional data (such as images or emails in spam filters) to lower dimensions to speed up a learning algorithm.
The mapping $x^{(i)}\rightarrow z^{(i)}$ should be defined by running PCA only on the training set.
This mapping can be applied as well to the examples $x^{(i)}_{cv}$ and $x^{(i)}_{test}$ in the cross validation and test sets. \\

PCA should only be used if the amount of initial dimensions significantly slows down the learning algorithm. PCA should not be used to prevent overfitting by reducing the amount of features. This might work but drops potentially relevant information. Regularization is a better way to address overfitting.


\section{Recommender systems}
{\bf Recommender systems} recommend items to users based on item ratings. There are $n_u$ number of users, $n_m$ number of items, $r(i,j)=1$ if user $j$ has rated item $i$ and $y^{(i,j)}$ is the rating given by user $j$. All ratings are on the same scale $[0,max]$, e.g. $max=5$.

\begin{figure}[h!]
\centering
\begin{tabular}{c||cccc||cc}
Item & $User_A$ & $User_B$ & $User_C$ & $User_D$ & $x_1\:(Feature_A)$ & $x_2\:(Feature_B)$ \\
\hline
\hline
$Item_A$ & 5 & 5 & 0 & 0 & 0.9 & 0 \\
$Item_B$ & 5 & ? & ? & 0 & 1.0 & 0.01 \\
$Item_C$ & ? & 4 & 0 & ? & 0.99 & 0 \\
$Item_D$ & 0 & 0 & 5 & 4 & 0.1 &1.0 \\
$Item_E$ & 0 & 0 & 5 & ? & 0 & 0.9 \\
\end{tabular}
\caption{Sample item ratings}
\label{ref:sampleitemratings}
\end{figure}

Figure~\ref{ref:sampleitemratings} contains a sample set of items, users and features. $n_u=4$, $n_m=5$, $x^{(1)}= [1\:0.9\:0]^T$ (as $x_0=1$). To predict rating of item $i$ of user $j$:
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})
\end{align*}

$i:r(i,j)$ stands for all items $i$ that user $j$ has rated. Vice versa, $j:r(i,j)$ stands for all users $j$ that rated item $i$. Collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$.

\begin{definition}[$\theta^{(1)},...,\theta^{(n_u)}$ estimation minimization objective]~\\
 $\min\limits_{\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[$x^{(1)},...,x^{(n_m)}$ estimation minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)}}\:\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2$
\end{definition}


Both definitions are very related to linear regression. In contrast, there is no $m$ in the denominators as it is in linear regression. This constant is irrelevant to the minimization objective. \\

$\theta^{(1)},...,\theta^{(n_u)}$ can be estimated given $x^{(1)},...,x^{(n_m)}$. Vice versa, $x^{(1)},...,x^{(n_m)}$ can be estimated given $\theta^{(1)},...,\theta^{(n_u)}$. This is a chicken or egg problem. One way to solve this problem is: guess $\theta \rightarrow x \rightarrow \theta \rightarrow x  \rightarrow \theta \rightarrow ...$. This algorithm is called {\bf collaborative filtering}. The term means that with every user rating items (collaboration), the algorithm learns better features. The algorithm is inefficient and can be improved in a way to compute $x^{(1)},...,x^{(n_m)}$ and $\theta^{(1)},...,\theta^{(n_u)}$ simultaneously as defined in Algorithm~\ref{ref:collaborativefiltering}.

\begin{definition}[Simultaneous collaborative filtering minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[Simultaneous collaborative filtering gradient descent]~\\
$x_k^{(i)}:=x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$ \\
$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\end{definition}

\begin{algorithm}
\caption{Collaborative filtering}
\label{ref:collaborativefiltering}
\begin{algorithmic}
\State 1. Initialize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$ to small random values \Comment{Break symmetry}
\State 2. Minimize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$
\State $x_k^{(i)}\gets x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$
\State $\theta_k^{(j)}\gets \theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\State 3. For a user with parameters $\theta$ and an item with (learned) features $x$, predict a rating
\State $y^{(i,j)}\gets (\theta^{(j)})^T(x^{(i)})$
\end{algorithmic}
\end{algorithm}

This algorithm needs to break symmetry by randomly initializing $x^{(1)},...,x^{(n_m)},$ $\theta^{(1)},...,\theta^{(n_u)}$ which is related to neural networks. As the algorithm does not iterate over $x_0$ or $\theta_0$, all elements are regularized. Alternatively, the last step of Algorithm~\ref{ref:collaborativefiltering}, can be vectorized:
\begin{align*}
Y = X\Theta^T
\end{align*}

This is also called {\bf low rank matrix factorization}.
\\
\\
As collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$, the most related items $j$ to item $i$ can be found by taking the $j$ items with smallest $\lVert x^{(i)} - x^{(j)}\rVert$ as a small $\lVert x^{(i)} - x^{(j)}\rVert$ indicates "similarity".
\\
\\
In practice, some users might have not rated any items. In that case, all predictions for this user are $0$. {\bf Mean normalization} can be applied to matrix $Y$:
\begin{align*}
\mu_i = \frac{1}{m^{(i)}} \sum_{j:r(i,j)=1}y^{(i,j)}
\end{align*}
\begin{align*}
Y = Y - \mu
\end{align*}
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})+\mu_i
\end{align*}
Mean normalization allows to predict a mean value per item which is more meaningful than $0$. It only takes rated items into account with $m^{(i)}$ number users who rated item $i$. Feature scaling is not necessary as all ratings are already on the same scale.


\section{Hidden Markov models}
\label{section:HMM}
{\bf Hidden Markov models} (HMMs) are used to analyze and predict states and time series. They are related to Markov chains which are simpler Markov models and are explained in Chapter~\ref{ref:markovchains}. In contrast, in a {\em hidden} Markov model, the states ares not directly visible, but measurements, dependent on the state, are visible.
\\
Figure~\ref{fig:samplehiddenmarkov} contains a hidden Markov model in which the measurements are happy (H) or grumpy (G) depending on the weather rainy (R) or sunny (S). Given measurements, the state probabilities can be calculated using Bayes rule, e.g.: $P(R_1\vert H_1)=\frac{P(H_1\vert R_1)P(R_1)}{P(H_1)}$.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (R) {R};
\node[mynode, right=1.5cm of R] (S) {S};
\node[mynode, below left=1.5cm and 0.1 of R] (H1) {H};
\node[mynode, below right=1.5cm and 0.1 of R] (G1) {G};
\node[mynode, below left=1.5cm and 0.1 of S] (H2) {H};
\node[mynode, below right=1.5cm and 0.1 of S] (G2) {G};
\path (R) edge [->,bend left] node[above] {0.4} (S)
(R) edge [loop above] node {0.6} (R)
(S) edge [->,bend left] node[below] {0.2} (R)
(S) edge [loop above] node {0.8} (S)
(R) edge [->, dashed] node[left] {0.4} (H1)
(R) edge [->, dashed] node[right] {0.6} (G1)
(S) edge [->, dashed] node[left] {0.9} (H2)
(S) edge [->, dashed] node[right] {0.1} (G2)
;
\end{tikzpicture}
\caption{Sample hidden Markov model}
\label{fig:samplehiddenmarkov}
\end{figure}


HMMs are used for example in {\bf localization} and {\bf tracking} which is covered in Chapter~\ref{chapter:localizationandtracking}. There are many other possible application such as in finance.


\section{Invariances}
\label{chapter:invariances}
{\bf Invariances} are covered in Chapter~\ref{chapter:cvinvariances}. This section covers how to make learning algorithms invariant. \\
For example, classifying handwritten digits should work irrespective the digits' position within the image of its size i.e. translation and scale invariance.
These transformations have significant differences in the raw data, expressed in different intensities at each of the pixel in the image.
The classifier shall still classify the digits properly.
If there are sufficiently many training examples that contain these transformations, the learning algorithm can learn the invariance approximately.
This approach requires many different training examples of objects at many different positions.
If there are many different transformations, then this approach becomes impractical as there are usually not enough training examples. \\
There are alternative approaches for encouraging an adaptive model to exhibit the required invariances, including:
\begin{enumerate}
\item The training set is augmented by replicating and transforming the training samples to the desired invariances.
This method can be computationally expensive, but can lead to significant improvements in generalization.
\item Invariance is implemented into the preprocessing by extracting features that are invariant under the respective transformations.
The subsequent learning algorithms use these features and are therefore invariant.
Given that the extraction works well, this method usually can perform much better on data that is not similar to the training data.
However, it is usually difficult and time-consuming to manually find these features.
\item A regularization term is added to the cost function to penalize changes in the output model when the input is transformed. This method is called {\bf tangent propagation}.  A example is given in Chapter~\ref{chapter:nninvariances} for neural networks.
\item Last, invariance properties can be built into the learning algorithm. A example is given in Chapter~\ref{chapter:nninvariances} for neural networks.
\end{enumerate}

{\bf Tangent propagation} can be used to regularize a learning algorithm in order to encourage models to be invariant of transformations of the input.
Provided that a transformation is continuous (e.g. a rotation or translation in contrast to mirror reflection), then the transformed patterns sweep out a {\bf manifold} $M$ within the $n$-dimensional input space.
Suppose, the transformation is governed by a single parameter $\xi$.
Then, the subspace $M$ swept out by $x$ is one-dimensional and is parameterized by $\xi$.
The transformation can be denoted by $s(x,\xi)$ which is defined so that $s(x,0)=x$.
Then, the tangent to the curve $M$ is given by the derivative:

\begin{align*}
\tau = \frac{\partial s}{\partial \xi}
\end{align*}

and the tangent vector at point $x$ (where $\xi=0$) is given by:

\begin{align*}
\tau = \frac{\partial s(x,\xi)}{\partial \xi}\left.{\!\!\frac{}{}}\right |_{\xi=0}
\end{align*}

Under a transformation of the input vector, the output of a learning algorithm will usually change.
The derivative of an output $y_k$ with respect to $\xi$ is given by:
\begin{align*}
\frac{\partial y_k}{\partial \xi}\left.{\!\!\frac{}{}}\right |_{\xi=0}
\end{align*}

Given the exact learning algorithm, the cost function can be enriched with a regularization term:

\begin{align*}
J(\theta)_{reg} = J(\theta) + \lambda \frac{1}{2} \sum_{i=1}^{m} \sum_{k=1}^{K} (\frac{\partial y_k}{\partial \xi}\left.{\!\!\frac{}{}}\right |_{\xi=0})^2
\end{align*}

This allows to encourage local invariance in the neighborhood of the data points: for zero transformation, errors are high for non-invariant training examples.
The regularization function is zero when the learning algorithm is invariant under the transformation in the neighborhood of each pattern vector.
\\
In a practical implementation, the tangent vector can be approximated if there is not analytic solution.
This can be done using finite differences using the equations covered in Chapter~\ref{chapter:learningweights}.
\\
The manifold $M$ has dimensionality $P$ if the transformation is governed by $P$ parameters.
The regularizer is then given by the sum of terms, one for each transformation.
A learning algorithm that is made invariant to each transformation separately is also locally invariant to combinations of the transformations.

\section{Semi-supervised learning}
{\bf Semi-supervised learning} makes use of labeled and unlabeled data.
Therefore, it falls in between of supervised and unsupervised learning.
First, the learning algorithm is given labeled training data. Second, it is given unlabeled data which it then classifies.
All of the unlabeled data is of one of the classes that are used in the labeled training data.
Making use of this addition unlabeled data allows to capture better the shape of the underlying data distribution and generalize better to new samples. \\
Semi-supervised learning is not widely used as usually unlabeled data is of more classes that are unknown and not part of the labeled training data.

\chapter{Unsupervised learning}
Unsupervised learning tries to find hidden structure in {\bf unlabeled data} $\{x^{(1)}, x^{(2)},$ $..., x^{(m)}\}$. In machine learning research, unsupervised learning has received less attention than supervised learning but is likely to receive more in the future because of its potential to work on unlabeled data.

\section{Clustering}
{\bf Cluster analysis} or {\bf clustering} is the task of grouping a set of objects in such a way that objects in the same group (called a {\bf cluster}) are more similar (in some sense or another) to each other than to those in other groups (clusters).
It is a main task of exploratory {\bf data mining} and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.


\subsection{k-means}
{\bf k-means} partitions $m$ observations $x^{(i)}$ into $K$ clusters $\mu_k$ as defined in Algorithm~\ref{ref:kmeans}. The optimization objective is to minimize the cost function. The initial cluster coordinates are random which makes this algorithm non-deterministic. A recommended strategy is to randomly pick $K$ training examples as the initial coordinates to make sure that the initial coordinates are not too far away from the actual data. k-means can be run multiple times on the same data to pick the result with the lowest cost. Choosing the number of clusters $K$ is usually done manually depending on the use case.

\begin{algorithm}
\caption{k-means}
\label{ref:kmeans}
\begin{algorithmic}
\State $\mu_1, \mu_2, ..., \mu_K \in \mathbb{R}^n \gets$ random initialization
\Repeat
\For{$i=1$ to $m$}
\State $c^{(i)} \gets$ index of cluster centroid closest to $x^{(i)}$
\EndFor
\For{$k=1$ to $K$}
\State $\mu_k \gets$ mean of points assigned to cluster $k$
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\begin{definition}[k-means cost function]~\\
$J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K)=\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-\mu_{c^{(i)}}\rVert^2 $
\end{definition}

\subsection{Fuzzy c-means}
In {\bf fuzzy clustering}, every point has a degree of belonging to clusters, as in fuzzy logic, rather than belonging completely to just one cluster.
Thus, points on the edge of a cluster, may be in the cluster to a lesser degree than points in the center of cluster.
The degree to which element $x^{(i)}$ belongs $k$ is:
\begin{align*}
w_k(x) = \frac{1}{\sum_j(\frac{\lVert \mu_k - x \Vert}{\lVert \mu_j - x \Vert})^{2/({\bf m}-1)}}
\end{align*}

The fuzzifier $m$ determines the level of cluster fuzziness.
A large $m$ results in smaller memberships $w$ and hence, fuzzier clusters.
In the limit $m = 1$, the memberships $w$ converge to $0$ or $1$, which implies a crisp partitioning.
In the absence of experimentation or domain knowledge, $m$ is commonly set to $2$. \\

With {\bf fuzzy c-means} defined in Algorithm~\ref{alg:fuzzycmeans}, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster $k$ with $K$ clusters in total:

\begin{align*}
\mu(k) = \frac{\sum_x w_k(x)^mx}{\sum_x w_k(x)^m}
\end{align*}

\begin{algorithm}
\caption{Fuzzy c-means}
\label{alg:fuzzycmeans}
\begin{algorithmic}
\State $w_{i, k} \gets$ random initialization \Comment{Point coefficients for being in the clusters}
\Repeat
\For{$k=1$ to $K$}
\State $\mu_k \gets \mu(k)$ \Comment{Update centroid of each cluster}
\EndFor
\For{$i=1$ to $m$}
\State $w_{i, k} \gets w_k(x^{(i)}) $ \Comment{Update point coefficients for being in the clusters}
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\subsection{Expectation maximization}
{\bf Expectation maximization} (EM) is a generalization of k-means using Gaussian models to smoothen the class memberships. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters.

\subsection{Density-based clustering}
In {\bf density-based clustering} clusters are defined as areas of higher density than the remainder of the data set.
Objects in these sparse areas - that are required to separate clusters - are usually considered to be noise and border points.
Most of the previously discussed algorithms do not perform well on density-based clusters.
{\bf DBSCAN} is the most popular clustering method.
On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.


\section{Principal component analysis}
\label{ref:pcasection}
"{\bf Principal component analysis} (PCA) computes the most meaningful basis to re-express a noisy, garbled data set"\footnote{\url{http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf}} and can be used for {\bf dimensionality reduction}. In this process, higher dimensional data is mapped to a lower dimension, the {\bf number of principal components}, as defined in Algorithm~\ref{ref:pca}. The goal of PCA is to minimize the {\bf average squared projection error} of all original data and their projections. PCA can be used to reduce memory and time consumption of complex computations. Also, it allows to plot higher dimensional data in 2D or 3D. PCA requires some underlying advanced linear algebra such as the {\bf singular value decomposition} (SVD) which is not further explained in this document. {\bf Matlab} or {\bf Octave} offer functions to compute this. Data can  approximately be mapped back to the origin dimension ($z^{(i)}\in \mathbb{R}^K$ to $x_{approx}^{(i)} \in \mathbb{R}^n$):
\begin{align*}
x_{approx}^{(i)} = U_{reduce}*z^{(i)}
\end{align*}

\begin{definition}[Average squared projection error]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2$
\end{definition}

\begin{definition}[Total variation in data]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2$
\end{definition}

\begin{algorithm}
\caption{Principal component analysis}
\label{ref:pca}
\begin{algorithmic}
\State $x_j^{(i)} \gets \frac{x_j^{(i)} - \mu_j}{\sigma_j} $ \Comment{Mean normalization and feature scaling}
\State $\Sigma \gets \frac{1}{m}X^TX$ \Comment{Covariance matrix}
\State $[U,S,V] \gets svd(\Sigma)$ \Comment{Singular value decomposition}
\State $U_{reduce} \gets$ first $K$ columns of $U$ \Comment{Number $K$ of {\bf principal components}}
\State $z^{(i)} \gets U_{reduce}^Tx^{(i)}$ \Comment{Maps data $x^{(i)} \in \mathbb{R}^n$ to $z^{(i)}\in \mathbb{R}^K$}
\end{algorithmic}
\end{algorithm}

Choosing the number of principal components $K$ comes with a loss of information. Usually, a fixed percentage such as 99\% or 95\% of the {\bf variance} must be {\bf retained}:
\begin{align*}
\frac{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2}{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2} \le 0.01\:\mbox(1\%)
\end{align*}
This measure is more useful than the actual amount of dimensions that were reduced. Then, the smallest $K$ to satisfy this equation is chosen. Computing $K$ this way is inefficient as PCA may need to be performed a lot times to choose the smallest $K$. Alternatively, the retained variance can be computed for a given $K$:
\begin{align*}
\frac{\sum_{i=1}^{K}S_{ii}}{\sum_{i=1}^{n}S_{ii}} \ge 0.99\:\mbox(99\%)
\end{align*}

Usually, the large variances occur in the first $K<n$ principal components and then drops off. Therefore, the most relevant dynamics occur only in the first $K$ dimensions. \\
PCA is adequate if the data are Gaussian, linear, and stationary, {\bf independent component analysis} (ICA) works on non-Gaussian and very noisy data\footnote{\url{http://www.colorado.edu/engineering/CAS/courses.d/ASEN6519.d/Lectures.d/Lecture10_11.6519.pdf}}.


\section{Sparse coding}
\label{chapter:sparsecoding}
Given training data $x^{(1)},x^{(2)},...,x^{(m)}$, {\bf sparse coding} learns bases (features) $\phi_1, \phi_2, ..., \phi_k$ of same dimension as the training data so that each input $x$ can be approximately decomposed as:
\begin{align*}
x \approx \sum_{i=1}^k a_i \phi_i
\end{align*}
such that most $a_j$s are zero ("sparse"). \\
Applied to computer vision, it automatically learns to represent an image in terms of the edges (bases) that appear in it.
This gives a more succinct, higher-level representation that the raw pixels\footnote{Andrew Ng. {\em Deep Learning, Self-Taught Learning and Unsupervised Feature Learning}. \url{http://www.youtube.com/watch?v=n1ViNeWhC24}}.


\section{Anomaly detection}
{\bf Anomaly detection} allows to find data that does not conform to an expected pattern. The cluster-based Algorithm~\ref{ref:anomalydetection} assumes {\bf independence} between selected features which are representative for anomaly detection. These features take unusually large or small values in the event of an anomaly and have a Gaussian distribution:
\begin{align*}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
\end{align*}

Non-Gaussian features can be approximated as Gaussian through operations such as $log(x), \sqrt{x}, x^{1/3}$ etc.

\begin{algorithm}
\caption{Anomaly detection}
\label{ref:anomalydetection}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu_j \gets \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}$ \Comment{Fits parameters $\mu_1,...,\mu_n,\sigma_1^2,...,\sigma_n^2$}
\State $\sigma_j^2 \gets \frac{1}{m}\sum_{i=1}^{m}(x_j^{(i)}-\mu_j)^2$
\State For new example $x$: $p(x) \gets \prod_{j=1}^np(x_j,\mu_j,\sigma_j^2)$ \Comment{Assumes independence between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

A good way to evaluate this algorithm is to use it on some labeled data of anomalous and non-anomalous ($y=0$ if normal, $y=1$ if anomalous). The training set only contains non-anomalous examples whereas the cross validation and test sets contain also a few anomalous examples. On a cross validation or test example $x$, predict
\begin{align*}
y = \left\{
\begin{array}{lll}
1  & \mbox{if } p(x) < \epsilon \mbox{ anomaly} \\
0  & \mbox{if } p(x) \ge \epsilon \mbox{ normal}
\end{array}
\right.
\end{align*}

Then, evaluation metrics such as precision/recall and $F_1$ score can be applied to the result. The cross validation set can also be used to choose $\epsilon$.

\subsection{Anomaly detection vs. supervised learning}
{\bf Anomaly detection} is used for a very small number of positive examples and large number of negative exempts. It is also used for many different kinds of anomalies as it is hard for any algorithm to learn from just a few positive examples what the anomalies might look like. There may be also future anomalies which may look completely different to any of the anomalous examples learned so far. {\bf Supervised learning} is used for large numbers of both positive and negative examples. It is also used when there are enough positive examples so that the algorithm can get a sense of what positive examples look like and future positive examples are likely to be similar to the ones in the training set.

\subsection{Correlated features}
Algorithm~\ref{ref:anomalydetection} assumes independence between the features for which the contours are along the axes. If features are {\bf correlated}, the algorithm performs poorly as the contours should rather be along the correlations. In such a case, some anomalies are incorrectly classified as normal. One could add new features to represent this correlation (e.g. $x_{extra}=\frac{x_1}{x_2}$) which requires manual effort but is computationally cheap. \\

Alternatively, {\bf multivariate Gaussian distribution} helps:

\begin{align*}
p(x)=\frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
\end{align*}

where $\vert \Sigma \vert$ is the determinant of the {\bf covariance matrix} $\Sigma$. Algorithm~\ref{ref:anomalydetectionmulti} is a {\bf anomaly detection with multivariate Gaussian} which is a generalization of Algorithm~\ref{ref:anomalydetection}.

\begin{algorithm}
\caption{Anomaly detection with multivariate Gaussian}
\label{ref:anomalydetectionmulti}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu \gets \frac{1}{m}\sum_{i=1}^m x^{(i)}$
\State $\Sigma \gets \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$
\State For new example $x$: $p(x) \gets \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))$ \Comment{Assumes correlation between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

This algorithm automatically captures correlations between features. It is computationally more expensive and requires $m>n$ to invert $\Sigma$. There may be further problems that prevent inversion such as linear dependency.


\chapter{Neural networks}
\label{chapter:neuralnetworks}
{\bf Neural networks} are a powerful learning method that is inspired by the brain.
A neuron has {\bf inputs} and an {\bf output} as seen in Figure~\ref{ref:neuron}.
A {\bf neural network} consists of layers of {\bf neurons}:
Each neural network has an {\bf input layer}, $\ge 0$ {\bf hidden layers} and an {\bf output layer} as seen in Figure~\ref{ref:neuralnetwork}.
Each layer has a {\bf bias term} of value 1. The parameters $\theta_j$ are called {\bf weights} in some sources.


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=2cm and 1cm of C] (X0) {$1$};
\node[mynode,above left=1cm and 1cm of C] (X1) {$x_1$};
\node[mynode,above left=0cm and 1cm of C] (X2) {$x_2$};
\node[mynode,below left=0cm and 1cm of C,draw=white] (X3) {...};
\node[mynode,below left=1cm and 1cm of C] (XN) {$x_n$};
\node[mynode,right=1cm and 1cm of C,draw=white] (h) {$h_{\theta}(x)$};
\path (X0) edge[-latex] (C)
(X1) edge[-latex] (C)
(X2) edge[-latex] (C)
(X3) edge[-latex] (C)
(XN) edge[-latex] (C)
(C) edge[-latex] (h);
\end{tikzpicture}
\caption{Neuron with bias term $x_0=1$}
\label{ref:neuron}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A12) {$a_1^{(2)}$};
\node[mynode,below=0.5cm of A12] (A22) {$a_2^{(2)}$};
\node[mynode,below=0.5cm of A22] (A32) {$a_3^{(2)}$};
\node[mynode,left=0cm and 1cm of A12] (X1) {$x_1$};
\node[mynode,left=0cm and 1cm of A22] (X2) {$x_2$};
\node[mynode,left=0cm and 1cm of A32] (X3) {$x_3$};
\node[mynode,right=0cm and 1cm of A22] (L3) {};
\node[mynode,right=0cm and 1cm of L3,draw=white] (h) {$h_{\Theta}(x)$};
\path (X1) edge[-latex] (A12)
(X1) edge[-latex] (A22)
(X1) edge[-latex] (A32)
(X2) edge[-latex] (A12)
(X2) edge[-latex] (A22)
(X2) edge[-latex] (A32)
(X3) edge[-latex] (A12)
(X3) edge[-latex] (A22)
(X3) edge[-latex] (A32)
(A12) edge[-latex] (L3)
(A22) edge[-latex] (L3)
(A32) edge[-latex] (L3)
(L3) edge[-latex] (h);
\end{tikzpicture}
\caption{Neural network with three layers (bias terms not displayed)}
\label{ref:neuralnetwork}
\end{figure}

\section{Perceptrons}
A {\bf perceptron} is a linear binary classified. In the context of neural networks, it is an artificial neuron using the {\bf heaviside step function} as the activation function.
The perceptron hypothesis maps an input $x$ to an output value with $x_0=1$.

\begin{definition}[Heaviside step function]
$heaviside(x) = \left\{
\begin{array}{lll}
1  & \mbox{if } x \ge 0 \\
0  & \mbox{otherwise}
\end{array}
\right.$
\end{definition}

\begin{definition}[Perceptron hypothesis]
$h(x) = heaviside(w\cdot x)$
\end{definition}


The {\bf perceptron learning rule} is defined in Algorithm~\ref{alg:perceptron} with learning rate $\alpha$. The update step is:
\begin{itemize}
\item If the output is correct, i.e. $y = h(x)$, then the weights are not changed.
\item If $y$ is $1$ but $h(x)$ is $0$, then $w$ is increased when the corresponding input $x^{(i)}$ is positive and decreased when $x^{(i)}$ is negative. This makes sense, because we want to make $w\cdot x$ bigger so that $h(x)$ outputs a $1$.
\item If $y$ is 0 but $h(x)$ is $1$, then w is decreased when the corresponding input $x^{(i)}$ is positive and increased when $x^{(i)}$ is negative. This makes sense, because we want to make $w\cdot x$ smaller so that $h(x)$ outputs a $0$.
\end{itemize}

The hard nature of the threshold causes some problems: the hypothesis $h(x)$ is not differentiable and is in fact a discontinuous function of its inputs and its weights. This makes learning with the perceptron rule a very unpredictable adventure. \\
Nonetheless, the perceptron is very fast and can be applied to large data sets.

\begin{algorithm}[h!]
\caption{Perceptron learning rule}
\label{alg:perceptron}
\begin{algorithmic}
\State Training set $\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}$
\State $w \gets [0,...,0]^T$ \Comment{Length is the number $n+1$ of features}

\For{$i=1$ to $m$}
\State $w_j \gets w_j + \alpha(y^{(i)} - h(x^{(i)})) x^{(i)}_j$ \Comment{For all features $j$}
\EndFor

\end{algorithmic}
\end{algorithm}

There are various other definitions including the following ones:
First, the training is continued until the algorithm converges.
Second, the training examples can be chosen randomly from the training set.
Third, the learning rate can be changed during the learning to converge faster.


\section{Feed-forward neural networks}
Figure~\ref{ref:neuralnetwork} contains a {\bf fully-conntected} {\bf feed-forward neural network}.
This kind of neural networks are acyclic graphs of neurons that propagate input values through to the output layer.
Feed-forward neural networks are also sometimes called a {\bf multi-layer perceptrons} with the difference that the neurons use the {\bf Sigmoid activation function} in contrast to the heaviside step function used in perceptrons.

\begin{definition}[Sigmoid activation function]
$g = h_\theta(x) = \frac{1}{1+e^{-\theta^{T}x}}$
\end{definition}

The value $a_i^{(j)}$ is called {\bf activation} or {\bf forward propagation} of unit $i$ in layer $j$.
$\Theta^{(j)}$ is a matrix of weights to map from layer $j$ to layer $j+1$.
If the network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ is of dimension $s_{j+1}\times (s_j + 1)$. \\

For the neural network in Figure~\ref{ref:neuralnetwork}, the mapping is defined in Figure~\ref{ref:neuralnetworkmapping}. 
\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$z_1^{(2)}$ & $=$ & $\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3$ \\
$a_1^{(2)}$ & $=$ & $g(z_1^{(2)})$ \\
$z_2^{(2)}$ & $=$ & $\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3$ \\
$a_2^{(2)}$ & $=$ & $g(z_2^{(2)})$ \\
$z_3^{(2)}$ & $=$ & $\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3$ \\
$a_3^{(2)}$ & $=$ & $g(z_3^{(2)})$ \\
$h_{\Theta}(x)$ & $=$ & $a_1^{(3)}$ $=$ $g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$ \\
\end{tabular}
\caption{Activation of layers in a neural network}
\label{ref:neuralnetworkmapping}
\end{figure}

This can be generalized to for the first hidden layer (layer $2$):
\begin{align*}
z_i^{(j+1)} = \sum_k^{s_{j}+1} \Theta_{ik}^{(j)}x_k \\
a_i^{(j+1)} = g(z_i^{(j+1)})
\end{align*}

Figure~\ref{ref:neuralnetworkmappingvectorized} contains a vectorized activation of this network. \\

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$a^{(1)}$ & $=$ & $x$ \\
$z^{(2)}$ & $=$ & $\Theta^{(1)}a^{(1)}$ \\
$a^{(2)}$ & $=$ & $g(z^{(2)})$ (add $a_0^{(2)}$) \\
$z^{(3)}$ & $=$ & $\Theta^{(2)}a^{(2)}$ \\
$h_{\Theta}(x)$ & $=$ & $a^{(3)}$ $=$ $g(z^{(3)})$ (add $a_0^{(3)}$) \\
\end{tabular}
\caption{Vectorized activation of layers in a neural network}
\label{ref:neuralnetworkmappingvectorized}
\end{figure}

Once trained, a neural network predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). \\

The cost function for $K$ output units and $L$ layers is a {\bf a cross-entropy error function} and is defined as follows (with $h_\Theta(x) \in \mathbb{R}^K$ and $h_\Theta(x)_k = k^{th}$ output):

\begin{definition}[Cost function] ~\\
$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}log\:h_{\Theta}(x^{(i)})_k+(1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)})_k)]$
\end{definition}


The cross-entropy error function is the best maximum likelihood estimate for a neural network.
Using it instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization.


\subsection{Learning weights}
\label{chapter:learningweights}
Learning the weights of a neural network is difficult as the cost function is complex and often has many local minima.
The simplest method to approximate the gradient is to utilize finite differences:
\begin{align*}
\frac{\partial}{\partial\theta_i}J(\theta)\approx\frac{J(...,\theta_{i-1},\theta_{i}+\epsilon,\theta_{i+1},...)-J(\theta)}{\epsilon}
\end{align*}

for small $\epsilon$. The accuracy of the approximation of the derivates can be improved by making $\epsilon$ smaller.
Numerical roundoff problems will arise at some point.
The accuracy can be significantly improved by using symmetrical {\bf central differences}:
\begin{align*}
\frac{\partial}{\partial\theta_i}J(\theta)\approx\frac{J(...,\theta_{i-1},\theta_{i}+\epsilon,\theta_{i+1},...)-J(...,\theta_{i-1},{\bf \theta_{i}-\epsilon},\theta_{i+1},...)}{{\bf 2}\epsilon}
\end{align*}

This approach requires approximately the double amount of computational steps.
In general, each dimension of $\theta$ requires two evaluations of the cost function and this can be expensive.
Therefore, efficient {\bf backpropagation} is used to approximate the gradients because this gives the greatest accuracy and numerical efficiency.


\subsection{Backpropagation}
Training a neural network requires to $\min\limits_{\Theta}J(\Theta)$, e.g. through gradient descent which needs to know $J(\Theta)$ and $\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$.
{\bf Backpropagation} is a method to efficiently compute the gradient as described in Algorithm~\ref{alg:backpropagation}. This algorithm can also be vectorized.

\begin{algorithm}[h!]
\caption{Backpropagation}
\label{alg:backpropagation}
\begin{algorithmic}
\State Training set $\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}$
\State $\Delta_{ij}^{(l)} \gets 0$ (for all $l,i,j$)

\For{$i=1$ to $m$}
\State $a^{(1)} \gets x^{(i)}$
\State Perform forward propagation to compute $a^{(l)}$ for $l=2,3,...,L$
\State Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$ \Comment {"error"}
\State Compute $\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$: $\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}.*g^{'}(z^{(l)})$
\State $\Delta^{(l)} \gets \Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$  \Comment {Matrix of errors for units of a layer}
\EndFor
\State $\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta) \gets \frac{1}{m}\Delta_{ij}^{(l)}$

\end{algorithmic}
\end{algorithm}

$\Theta_{ij}^{(l)}$ needs to have an initial value. If $\Theta$ was entirely 0, {\bf symmetry} would create a highly redundant neural network with poor learning capabilities.
{\bf Random initialization} allows symmetry breaking by initializing each $\Theta_{ij}^{(l)}$ to a random value in $[-\epsilon,\epsilon]$. \\

$\delta^{(l)}$ is a vector of errors of the units of layer $l$.
The errors of the output layer are the differences of the activation values $a^{(L)}$ and the observed values of the $i$th training example: $\delta^{(L)}=a^{(L)}-y^{(i)}$.
The errors of the hidden layers are: 

\begin{align*}
\delta^{(2)} = \frac{\partial cost(i)}{\partial z^{(l)}} =(\Theta^{(l)})^T\delta^{(l+1)}.*g^{'}(z^{(l)})
\end{align*}

($.*$ is the element-wise multiplication). With the cost for training example $x^{(i)}$ being:
\begin{align*}
cost(i) = y^{(i)}log\:h_{\Theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\Theta}(x^{(i)}))
\end{align*}

The derivative of the Sigmoid function is defined as follows:
\begin{align*}
g^{'}(x)=g(x)(1-g(x))
\end{align*}

There is no $\delta^{(1)}$ as no error term is associated with the input values.


\subsection{Gradient checking}
As backpropagation is a complex algorithm, subtile bugs might not be caught and gradient descent might not reach the intended global minimum but only a more expensive minimum.
{\bf Gradient checking} allows to approximate the gradient and compare it to the gradient returned by backpropagation.
The approximation is approximated as discussed in Chapter~\ref{chapter:learningweights} for each feature $\theta_i$:
\begin{definition}[Gradient checking] ~\\
$\frac{\partial}{\partial\theta_i}J(\theta)\approx\frac{J(...,\theta_{i-1},\theta_{i}+\epsilon,\theta_{i+1},...)-J(...,\theta_{i-1},\theta_{i}-\epsilon,\theta_{i+1},...)}{2\epsilon}$ for small $\epsilon$
\end{definition}

\subsection{Regularization}
Regularization can prevent a neural network from overfitting, the cost function then penalizes large weights:

\begin{definition}[Regularized cost function] ~\\
$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}log\:h_{\Theta}(x^{(i)})_k+(1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)})_k)] \\
+ \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l+1}}\sum_{j=1}^{s_{l}}(\Theta_{ij}^{(l)})^2$
\end{definition}

Also, the final step of backpropagation takes regularization into account for the non-bias units when approximating the gradient:
\begin{align*}
\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta) \gets \frac{1}{m}\Delta_{ij}^{(l)}+\frac{\lambda}{m}\Theta_{ij}^{(l)}\:\mbox{if}\:j\ne0
\end{align*}


\subsection{Picking a network architecture}
The following rules can be followed to decide on the network architecture:
\begin{itemize}
\item No. of input units: dimension of features $x^{(i)}$
\item No. output units: number of classes
\item No. of hidden layers: reasonable default: $1$ hidden layer, if $>1$, same number of units in every layer
\item No. of units per layer: usually the more the better (but also computationally more expensive)
\end{itemize}
The more units in the network, the more likely it is to overfit.
Methods to prevent overfitting including more training data, regularization and early stopping as discussed in Chapter~\ref{chapter:debugginglearning}.


\subsection{Multi-class classification}
In multi-class classification, the {\bf output layer} can use the {\bf softmax function} instead of Sigmoid:

\begin{align*}
o_i^{(out)} = \sum_k^{s_{out-1}+1} \Theta_{ik}^{(out-1)}x_k \\
a_i^{(out)} = softmax(o_i^{(out)}) = \frac{e^{o_i^{(out)}}}{\sum_{i=1}^{K} e^{o_i^{(out)}}}
\end{align*}

Softmax converts a raw value into a posterior probability.
Each $a_i^{(out)} \in [0,1]$ and $\sum_i a_i^{(out)} = 1$.\\
The derivative of the softmax function is and therefore related to the derivative of the Sigmoid function:
\begin{align*}
\frac{\partial softmax(o_i)}{\partial o_i} = softmax(o_i)(1-softmax(o_i))
\end{align*}

\subsection{Invariances}
\label{chapter:nninvariances}
Chapter~\ref{chapter:invariances} generally covers how to make learning algorithms invariant. This section covers how to make neural networks invariant.
\\
\subsubsection{Tangent propagation}
{\bf Tangent propagation} can be done in neural networks:
\begin{align*}
\frac{\partial y_k}{\partial \xi}\left.{\!\!\frac{}{}}\right |_{\xi=0} = \sum_{i=1}^{m} J_{ki}\tau_{i}
\end{align*}

where $J_{ki}$ is the $(k,i)$ element of the Jacobian matrix for output $k$ and all training examples $x^{(i)}$.
In neural networks, the Jacobian matrix provides a measure of the local sensitivity of the outputs to changes in each of the input variables.
The matrix can elements can either be approximated using finite elements or can be computed analytically by an amended backpropagation.
The analytical solution is not provided in this document.

The result can be used to compute the regularized cost function:

\begin{align*}
J(\theta)_{reg} = J(\theta) + \lambda \frac{1}{2} \sum_{i=1}^{m} \sum_{k=1}^{K} (\sum_{i=1}^{m} J_{ki}\tau_{i})^2
\end{align*}

\subsubsection{Convolutional neural networks}
Alternatively, neural networks can include invariance properties in their structure.
These kind of neural networks are called {\bf convolutional neural networks} (CNNs) which tolerate translation of the input for example.
They are widely applied for image and speech data. \\
{\bf Feature maps} are a key to CNNs. A feature map is a layer in a CNN whose units share the same parametrization (weights). This property is called {\bf weight sharing}.
Replicating units in this way allows for features to be detected regardless of their position in the visual field.\\
The subsampling layer takes inputs and computes the average of those inputs, multiplied by a weight and finally applies the Sigmoid function to the value.
In this way, the response of a unit in the subsampling layer is relatively insensitive to small shifts of the image in the corresponding regions of the input space.


Due to the constraints of weights, the number of weights in the network is smaller than in a fully-connected network.
Also, the number of independent parameters to be learned is significantly smaller. \\
CNNs require a slight modification of the backpropagation algorithm for weight sharing in the convolutional layers and averaging and in the subsampling layers.

\section{Recurrent neural networks}
{\bf Recurrent neural networks} (RNNs) are cyclic graphs of neurons as displayed in Figure~\ref{figure:recurrentneuralnetwork}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A12) {$a_1^{(2)}$};
\node[mynode,below=0.5cm of A12] (A22) {$a_2^{(2)}$};
\node[mynode,below=0.5cm of A22] (A32) {$a_3^{(2)}$};
\node[mynode,left=0cm and 1cm of A12] (X1) {$x_1$};
\node[mynode,left=0cm and 1cm of A22] (X2) {$x_2$};
\node[mynode,left=0cm and 1cm of A32] (X3) {$x_3$};
\node[mynode,right=0cm and 1cm of A22] (L3) {};
\node[mynode,right=0cm and 1cm of L3,draw=white] (h) {$h_{\Theta}(x)$};
\path (X1) edge[-latex] (A12)
(X1) edge[-latex] (A22)
(X1) edge[-latex] (A32)
(X2) edge[-latex] (A12)
(X2) edge[-latex] (A22)
(X2) edge[-latex] (A32)
(X3) edge[-latex] (A12)
(X3) edge[-latex] (A22)
(X3) edge[-latex] (A32)
(A12) edge[-latex] (L3)
(A22) edge[-latex] (L3)
(A32) edge[-latex] (L3)
(A32) edge [->,bend left] node {} (X3)
(L3) edge[-latex] (h);
\end{tikzpicture}
\caption{Recurrent neural network}
\label{figure:recurrentneuralnetwork}
\end{figure}

They have increased representational power as they create an internal state of the network which allows them to exhibit dynamic temporal behavior. There are several variants of RNNs that have different capabilities.
\\
Training RNNs is more complex as this depends on their structure. The RNN in Figure~\ref{figure:recurrentneuralnetwork} can be trained using a simple variant of backpropagation.
In practice, recurrent networks are more difficult to train than feedforward networks and do not generalize as reliably.


\section{Deep learning}
Most of today's machine learning tends to be just curve fitting and statistics.
The idea of {\bf deep learning}\footnote{Andrew Ng. {\em Deep Learning, Self-Taught Learning and Unsupervised Feature Learning}. \url{http://www.youtube.com/watch?v=n1ViNeWhC24}} is to make learning algorithms much better and easier to use and to make revolutionary advances in machine learning and AI.
Many problems in machine learning such as image classification, speech recognition and machine translation require a lot of work due to proper feature extraction (e.g. represent handles and wheels to detect motorcycles).
In contrast, much of human intelligence can be explained by a single learning algorithm. This is called the {\bf "one learning algorithm" hypothesis}. \\
A deep neural network for face recognition detects for example edges in the first hidden layer, face parts (combination of edges) in the second hidden layer and face models in the last hidden layer.
Sparse encoding from Chapter~\ref{chapter:sparsecoding} can be used for the training of the layers.


\chapter{Reinforcement learning}
\label{chapter:reinforcementlearning}

Supervised learning algorithms approximate a mapping of their inputs to the labels $y$ of the training set.
These labels give an unambiguous "right answer" for the inputs $x$. In many problems, it is difficult to provide such an explicit supervision to a learning problem.
In {\bf reinforcement learning}, the algorithm is only provided a reward (feedback) function which provides a reward or penalty depending on in which state the learning agent goes.
Reinforcement learning is in between of supervised and unsupervised learning as there is some supervision, but significantly less than in supervised learning.
The {\bf credit assignment problem} is to figure out after getting a reward what an algorithm did right or wrong to get this reward.
For example, a chess algorithm may lose after 30 steps and only gets then a penalty.
This makes it hard to solve the credit assignment problem as the cause of the loss might have happened many steps before.

\section{Markov decision processes}
{\bf Markov decision processes} (MDP) provide a formalism in which reinforcement learning problems are usually posed. A MDP is a tuple $(S,A,\{P_{sa}\},\gamma, R)$, where:
\begin{itemize}
\item $S$ is a set of states
\item $A$ is a set of actions
\item $P_{sa}$ are the state transition probabilities which is a distribution over the state space for each state $s\in S$ and action $a\in A$. $P_{sa}(s')$ is the probability of $s'$ being the subsequent state of $s$ given action $a$. $\sum_{s'}P_{sa}(s')=1, P_{sa}(s')\ge 0$ .
\item $\gamma \in [0,1)$ is called the discount factor
\item $R:S \mapsto \mathbb{R}$ is the reward function
\end{itemize}

The dynamics of an MDP proceeds as follows: Start in some state $s_0$ and choose an action $a_0\in A$. The MDP then randomly transitions to some successor state $s_1\sim P_{s_0a_0}$. Then choose another action $a_1$ and randomly transition to $s_2\sim P_{s_1a_1}$: $s_0\xrightarrow{a_0}s_1\xrightarrow{a_1}s_2\xrightarrow{a_2}s_3\xrightarrow{a_3}...$. The total {\bf payoff} is: $R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+...$. This means that later rewards are given less weight than earlier rewards, called {\bf discounting}. The {\bf goal} of reinforcement learning is to maximize:
\begin{align*}
E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...]
\end{align*}
The goal of MDP is to compute a {\bf policy} $\pi:S\mapsto A$ which is a mapping from state to actions. The {\bf value function} of a policy $\pi$ is:
\begin{align*}
V^{\pi}(s) = E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma (R(s_1)+\gamma (R(s_2)+...))\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma V^{\pi}(s_1)\vert s_0=s,\pi]
\end{align*}
The value function is the expected sum of discounted rewards upon starting in state $s$ and taking actions according to $\pi$.
$R(s_o)$ is called the {\bf immediate reward} and $\gamma V^{\pi}(s_1)$ the {\bf future rewards}.
More precisely, the {\bf value function} can be defined recursively which satisfies the {\bf Bellman equations}:
\begin{align*}
V^{\pi}(s) = R(s)+\gamma \sum_{s'\in S} P_{s\pi(s)}(s')V^{\pi}(s')
\end{align*}

Bellman's equations can be used to efficiently solve $V^{\pi}$. Specifically, in a finite-state MDP ($\vert S\vert < \infty$), such one such equation for $V^{\pi}(s)$ can be written down for every state $s$. This gives a set of $\vert S\vert$ linear equations in $\vert S\vert$ variables (the unknown $V^{\pi}(s)$'s, one for each state) which can be efficiently solved for the $V^{\pi}(s)$'s.

The {\bf optimal value function} is:
\begin{align*}
V^{*}(s) = \max\limits_{\pi} V^{\pi}(s)
\end{align*}

There is also a version of Bellman's equations for the optimal value function:
\begin{align*}
V^{*}(s) = R(s)+\max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

The {\bf optimal policy} is:
\begin{align*}
\pi^{*}(s) = \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

One way to get $\pi^{*}$ is to compute $V^{*}$ by iterating all possible policies $\pi$. Then, it can be plugged into the definition of $\pi^{*}$. As there is an exponential amount of policies $(\vert A\vert^{\vert S \vert})$, this is impractical.

\section{Value iteration and policy iteration}
{\bf Value iteration} is an efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:valueiterationalgorithm}. Starting at 0, it repeatedly improves the value function. Eventually, $V$ converges to $V^{*}$. Then $\pi^{*}$ can be computed using the previous equation.

\begin{algorithm}[h!]
\caption{Value iteration}
\label{ref:valueiterationalgorithm}
\begin{algorithmic}
\State $\forall s\in S: V(s) \gets 0$
\Repeat
\State $\forall s\in S:$ update $V(s) \gets R(s) + \max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{convergence}
\end{algorithmic}
\end{algorithm}

{\bf Policy iteration} is another efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:policyiterationalgorithm}. It starts with a random policy. First, in each iteration, a fixed policy is given and the algorithm solves for the value function of this policy. This solves a set of $\vert S\vert$ linear equations in $\vert S\vert$ variables. Second, the algorithm maximizes the policy for this value function. This is simple as the value function $S$ was computed in the step before. Eventually, $V$ and $\pi$ converge to $V^{*}$ and $\pi^{*}$ respectively.

\begin{algorithm}[h!]
\caption{Policy iteration}
\label{ref:policyiterationalgorithm}
\begin{algorithmic}
\State Initialize $\pi$ randomly
\Repeat
\State $V\gets V^{\pi}$
\State $\forall s\in S:\:\pi(s)\gets  \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{convergence}
\end{algorithmic}
\end{algorithm}

In comparison, policy iteration is computationally expensive for large MDPs because of the $V\gets V^{\pi}$ step. Nonetheless, it is very fast and converges quickly for small MDPs. In practice, value iteration is used more often because of large state spaces.

\section{Model learning}
In many real-world problems, some parts of an MDP are unknown, e.g. state transition probabilities and rewards. These need to be estimated from data. Trials of transitions:
\begin{align*}
s_0^{(1)}\xrightarrow{a_0^{(1)}}s_1^{(1)}\xrightarrow{a_1^{(1)}}s_2^{(1)}\xrightarrow{a_2^{(1)}}s_3^{(1)}\xrightarrow{a_3^{(1)}}...
\end{align*}
\begin{align*}
s_0^{(2)}\xrightarrow{a_0^{(2)}}s_1^{(2)}\xrightarrow{a_1^{(2)}}s_2^{(2)}\xrightarrow{a_2^{(2)}}s_3^{(2)}\xrightarrow{a_3^{(2)}}...
\end{align*}
can be used to estimate the state transition probabilities:
\begin{align*}
P_{sa}(s')=\frac{\#\mbox{times action } a\mbox{ was taken in state } s \mbox{ and got to }s'} {\#\mbox{times action } a\mbox{ was taken in state } s}
\end{align*}
This ratio might be "$0/0$" in case of never having taken action $a$ in state $s$ before, $P_{as}(s')$ can be estimated: $\frac{1}{\vert S\vert}$. A similar procedure can be used if $R$ is unknown. \\
Having learned a model for the MDP, either value iteration or policy iteration can be used to solve the MDP using the estimated transition probabilities and rewards. For example, model learning and value iteration can be put together and repeated until convergence.

\section{Continuous MDPs}
The algorithms discussed so far work on MDPs with a finite number of states. Many real-world problem need an infinite amount of states e.g. to represent positions and orientations. {\bf Discretization} maps the state space to a finite number of buckets on which then value iteration or policy iteration work.
This approach comes with two downsides: First, there is a loss of information which can be partially treated by using a very fine discretization which creates a huge amount of state.
The second downside is called the {\bf curse of dimensionality}. Suppose $S=\mathbb{R}^n$ and each of the $n$ dimensions is discretized into $k$ values. In total this would result in $\vert S_{discretized}\vert=k^n$ states.
This grows exponentially in the dimension of the state space $n$ which does not scale well to large problems. Discretization works generally only well for 1d or 2d problems. Many problems are higher-dimensional such as representing a driving car on a 2d plane as $(x, y, \theta, x', y', \theta')$ which is 6d. There are more advanced algorithms such as {\bf fitted value iteration} and {\bf Q-learning} that work on continuous spaces.

\section{Examples}
\subsection{Routing in a sample world}
This example is to apply the definitions on a small problem. The sample world in Figure~\ref{ref:sampleworld} contains a positive reward in cell $(4, 3)$ and a penalty in $(4, 2)$. It has 11 states and $A = \{N, S, E, W\}$.
\begin{figure}[h!]
\centering
\begin{tabular}{c|c|c|c|c|}
\hline
3 & & & & +1 \\
\hline
2 & & \cellcolor{black} & & -1 \\
\hline
1 & & & & \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\caption{Sample world}
\label{ref:sampleworld}
\end{figure}

Usually, a small penalty is assigned to all other states:
\begin{align*}
R(S)=
\left\{
\begin{array}{ll}
+1 & \mbox{if } S = (4,3) \\
-1  & \mbox{if } S = (4,2) \\
-0.02 & \mbox{otherwise}
\end{array}
\right.
\end{align*}

The state transition probability is defined as follows for all valid movements (invalid movements are 0): the intended outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles to the intended direction (0.1 to the left and 0.1 to to right if intended north).

The optimal policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy}.
The $V^{*}$ values were computed through value iteration.
The action of $(3, 1)$ is not obvious. If it was north, there would be a 0.1 chance to go to -1 from $(3, 2)$.
Therefore, it is left to route away from this potential action.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\uparrow$ & \cellcolor{black} & $\uparrow$ & -1 \\
\hline
1 & $\uparrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .86 & .90 & .93 & +1 \\
\hline
2 & .82 &\cellcolor{black} & .69 & -1 \\
\hline
1 & .78 & .75 & .71 & .49 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Optimal policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy}
\end{figure}

A bad policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy1}. A sample value function definition part is:
\begin{align*}
V^{\pi}((3,1))=R((3,1)) + \gamma[0.8V^{\pi}((3,2))+0.1V^{\pi}((4,1))+0.8V^{\pi}((2,1))]
\end{align*}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\downarrow$ & \cellcolor{black} & $\rightarrow$ & -1 \\
\hline
1 & $\rightarrow$ & $\rightarrow$ & $\uparrow$ & $\uparrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .52 & .73 & .77 & +1 \\
\hline
2 & -.90 &\cellcolor{black} & -.82 & -1 \\
\hline
1 & -.88 & -.87 & -.85 & -1.00 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Bad policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy1}
\end{figure}

\subsection{Further examples}
Reinforcement learning can be applied to a variety of real-world problems such as self-driving cars, self-flying helicopters or stock market prediction (rewards are profits/losses).


\chapter{Further machine learning topics}
This chapter contains further machine learning-related topics which do not fit into the previous chapters.

\section{Large scale machine learning}
Large data sets  help to increase the accuracy of {\bf low-bias} learning algorithms. In some cases, the actual algorithm may matter less as others perform similarly well given large data sets:
\begin{center}
\em{"It's not who has the best algorithm that wins. It's who has the most data."}
\end{center}
One way to find out if the algorithm has low bias is to randomly pick a lower amount (e.g. 1000) of  examples of this set and plot its learning curves.

\subsection{Gradient descent}
\label{chapter:gradientdescent}
(Batch) gradient descent as defined in Algorithm~\ref{ref:gradientdescent} often converges within a few steps for low-dimensional spaces.

\begin{algorithm}
\caption{Batch gradient descent}
\label{ref:gradientdescent}
\begin{algorithmic}
\Repeat
\For{$i=1$ to $m$}
\State $\theta_j  \gets \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}

The direction is chosen by taking the negative of the gradient and the distance is determined by the learning rate $\alpha$.
Through this step, the parameters $\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\theta)$.
If the {\bf learning rate} $\alpha$ is too large, gradient descent might not converge and overshoot the minimum after a certain amount of iterations.
Gradient descent might generally not find the global but the local minimum.
It returns the global minimum for convex or bowl-shaped cost functions which do not have local optima.
{\bf Feature normalization} allows to perform gradient descent quickly by normalizing each feature to the mean and scaling it to its standard deviation.

\begin{definition}[Feature normalization]
$x_j' = \frac{x_j - \mu_j}{\sigma_j}$
\end{definition}

Large data sets come with new problems, e.g. a lack of {\bf scalability}.
For high-dimensional spaces such as support vector machines or neural networks, it can require tens of thousands of iterations.
In each iteration, the summation update step of gradient descent can become computationally expensive for large data sets, e.g. 100 million records.
The entire set cannot be stored in RAM and needs to be retrieved from hard disk or streamed.


\subsection{Stochastic gradient descent}

{\bf Stochastic gradient descent} or {\bf online gradient descent} or {\bf sequential gradient descent} computes an approximation of gradient descent as described in Algorithm~\ref{ref:stochasticgradientdescent}.
First, the randomization splits up the algorithm in many cases.
Second, the update statement in the inner loop allows the algorithm to improve the parameters quickly without summing up all $m$ training data.
The resulting learning curve is {\bf not convex}.
In general, the algorithm wanders around the global minimum and moves the parameters towards the region of it. \\
Also, stochastic gradient descent handles redundancy very well. Assuming that all training examples are duplicated, the error function gets multiplied by a factor of two. Batch methods require double the computational effort to evaluate the batch error function gradient. In contrast, stochastic gradient descent is unaffected.
Last, stochastic gradient descent allows to escape from a local minima since a stationary point with respect to the error function for the whale data set is generally not a stationary point for each data point individually.

\begin{algorithm}
\caption{Stochastic gradient descent}
\label{ref:stochasticgradientdescent}
\begin{algorithmic}
\State Randomly shuffle data set
\Repeat
\For{$i=1$ to $m$}
\State $\theta_j  \gets \theta_j - \alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}


\subsection{Mini-batch gradient descent}
{\bf Mini-batch gradient descent} combines batch gradient descent and stochastic gradient descent as described in Algorithm~\ref{ref:minibatchgradientdescent}. It computes the parameters for buckets of $b$ training examples of $m$. It can still be vectorized which is an advantage over stochastic gradient descent and can outperform it.

\begin{algorithm}
\caption{Mini-batch gradient descent}
\label{ref:minibatchgradientdescent}
\begin{algorithmic}
\State $b\gets $ step size
\Repeat
\For{$i$ in $m, step=n$}
\State $\theta_j  \gets \theta_j - \alpha \frac{1}{\vert b\vert}\sum_{k=i}^{i+9} (h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}


\subsection{MapReduce gradient descent}
{\bf MapReduce} can be used to speed up gradient descent by distributing the summation in the gradient descent upgrade step as defined in Algorithm~\ref{ref:mapreducegradientdescent}. The data set is split up in $K$ parts which are sent to different computational units such as cores or servers.

\begin{algorithm}
\caption{MapReduce gradient descent}
\label{ref:mapreducegradientdescent}
\begin{algorithmic}
\State $K\gets $ split count
\Repeat
\For{$k=1..K$}
\State Distribute computation: $temp_j^{(k)}\gets \sum_{i=(k-1)\frac{m}{K}+1}^{i+\frac{m}{K}-1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\State $\theta_j  \gets \theta_j - \alpha \frac{1}{m}\sum_{k=1}^{\frac{m}{K}}temp_j^{(k)}$ (for all $j$)
\Until{forever}
\end{algorithmic}
\end{algorithm}

\subsection{Other optimization methods}
Gradient descent can become very slow for complex parameter spaces.
There are more efficient methods such as {\bf conjugate gradients} and {\bf quasi-Newton} methods which are much more robust and much faster than simple gradient descent.
Unlike gradient descent, these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum. \\
In comparison to gradient descent's learning rate $\alpha$, {\bf line search} follows a different approach choosing the distance for the weight update.
Once a line is chosen which specifies the direction of the update, the update distance is chosen by finding the minion of the error function along this line.
The position of the point along the line that minimizes the error has impact on the update which can result in a very large or very small weight update.

\subsection{Conjugate descent}
{\bf Conjugate descent} builds on top of the idea of line search. It performs a sequence of line searcher to search for a minimum in the error surface.
As the first step, the direction is chosen by similar to gradient descent by selecting the negative gradient.
For each subsequent sequence, a new direction is chosen to that the component of the error gradient that has just been made zero, remains zero.
Assuming exact arithmetic, conjugate descent converges in at most $n$ steps where $n$ is the amount of features.


\subsection{Quasi-Newton methods}
{\bf Quasi-Newton methods} such as {\bf BFGS} are based on {\bf Newton's method}.
Newton's method approximates $f(x)$ by a quadratic function around $x_n$ through Taylor expansion and then takes a step towards the maximum/minimum of the quadratic function.
The equation
\begin{align*}
x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}, n = 0, 1, ...
\end{align*}
converges towards a root of $f'$, i.e. $x_*$ for which $f'(x_*)=0$. \\
This scheme can be generalized to several dimensions by replacing the derivative with the gradient and the reciprocal of the second derivative with the inverse of the {\bf Hessian matrix}. \\
Quasi-Newton methods do not compute the Hessian matrix, but approximate it by analyzing successive gradient vectors instead.


\section{Online learning}
{\bf Online learning} is useful when there is a continuous stream of new data and if the entire data set is too large for efficient re-calculation of the parameters. It is defined in Algorithm~\ref{ref:onlinelearning}. Online learning learns on the fly and also adapts to change of data (e.g. rising housing prices).

\begin{algorithm}
\caption{Online learning}
\label{ref:onlinelearning}
\begin{algorithmic}
\State Get new data
\Repeat
\State Update $\theta$ using new data: $\theta_j\gets \theta_j - \alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\Until{forever}
\end{algorithmic}
\end{algorithm}


\chapter{Localization and tracking}
\label{chapter:localizationandtracking}
{\bf Localization} can be done by using the global positioning system (GPS). Unfortunately, GPS is not accurate. Robot localization requires higher accuracy. This chapter provides information on such techniques.
{\bf Tracking} allows to detect and track other vehicles to avoid collisions.
Table \ref{ref:complocalization} summarizes and compares properties of the methods discussed in this chapter. \\
This chapter covers various {\bf filters} which are related or make use of {\bf hidden Markov models} which are explained in Chapter~\ref{section:HMM}.
\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c|c|c}
 & State space & Belief & Efficiency & In robotics \\
\hline
\hline
Markov localization & discrete & multi-modal & exponential & approximate \\
\hline
Kalman filter & continuous & uni-modal & quadratic & approximate \\
\hline
Particle filter & continuous & multi-modal & depends on use case & approximate \\

\end{tabular}
\end{center}
\caption{Comparison of localization and tracking methods}
\label{ref:complocalization}
\end{table}

\section{Markov localization}
{\bf Markov localization} is a {\bf discrete} grid-based localization which uses histogram to represent the belief distribution. It is {\bf multi-modal} meaning that there are multiple possible states in which the robot could be. Localization starts in an initial {\bf belief state} and consists of the two steps {\bf sense} and {\bf move} which are executed cyclically. Sense gains information on the current localization. In contrast, move loses information as movements are inaccurate. \\
The computation of the sense step is: \\
\begin{align*}
P(x'_{i,t}\vert z_t) = \frac{P(z_t\vert x'_{i,t})P(x'_{i,t})}{P(z_t)}
\end{align*}
$P(x'_{i,t})$ is the belief state before the update (i.e. $P(x_{i,t}\vert a_t)$). $P(z_t\vert x'_{i,t})$ is the probability of getting measurement $z_t$ from state $x'_{i,t}$. $P(z_t)$ is the probability of a sensor measurement $z_t$. Calculated so that the sum over all states $x_{i,j}$ equals 1.

The computation of the move step is:
\begin{align*}
P(x'_{i,t}) = P(x_{i,t}\vert a_t) = \sum_{j=1}^{n} P(x_{i,t}\vert x_{j,t-1},a_t)P(x_{j,t-1})
\end{align*}
It maps from a belief state $P(x_{j,t-1})$ and action $a_t$ to a new predicted belief state $P(x'_{i,t})$ by
summing over all possible ways (i.e. from all states $x_{j,t-1}$) in which the robot may have reached $x'_{i,t}$. \\
Markov localization complexity grows exponentially. This is because any grid that is defined over $k$-dimensions will end up having exponentially many grid cells in the number of dimensions, which does not allow to represent high dimensional grids very well. \\
Algorithm~\ref{ref:markovlocalizationpython} implements Markov localization for a sample cyclic 5-cell world. The final belief state is:
\begin{align*}
[0.0789, 0.0753, 0.2247, {\bf 0.4329}, 0.1882]
\end{align*}

\begin{algorithm}
\caption{Sample Markov localization}
\label{ref:markovlocalizationpython}
\begin{python}
p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'red']
motions = [1,1]
pHit = 0.6
pMiss = 0.2
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    q = []
    for i in range(len(p)):
        s = pExact * p[(i-U) % len(p)]
        s = s + pOvershoot * p[(i-U-1) % len(p)]
        s = s + pUndershoot * p[(i-U+1) % len(p)]
        q.append(s)
    return q

for k in range(len(measurements)):
    p = sense(p, measurements[k])
    p = move(p, motions[k])

print p
\end{python}
\end{algorithm}


\section{Kalman filter}
{\bf Kalman filters} allow to estimate the state of a dynamic system and are often used to track other vehicles. In comparison to Markov localization, they are {\bf continuous} and {\bf uni-modal}. Kalman filters use a {\bf Gaussian distribution} to represent the belief state. Kalman filters have two steps: {\bf measurement update} and {\bf motion update} (prediction). \\

The measurement update of a 1d Kalman filter is a Gaussian multiplication and gains information:
\begin{align*}
\mu\prime = \frac{\sigma_2^2\mu_1 + \sigma_1^2\mu_2}{\sigma_1^2 + \sigma_2^2} \\
\sigma^{2}\prime = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{align*}

The motion update of a 1d Kalman filter is defined as follows and loses information:
\begin{align*}
\mu\prime = \mu_1 + \mu_2 \\
\sigma^{2}\prime = \sigma_1^2 + \sigma_2^2
\end{align*}
The motion itself is uncertain and can be described by a Gaussian $\mu_2, \sigma_2^2$. Algorithm~\ref{ref:1dkalmanfilterpython} implements a sample 1d Kalman filter. The velocity is given in that example. The final estimations are:
\begin{align*}
\mu = 10.99 \\
\sigma^2 = 4.006
\end{align*}

\begin{algorithm}
\caption{Sample 1d Kalman filter}
\label{ref:1dkalmanfilterpython}
\begin{python}
def update(mean1, var1, mean2, var2):
    new_mean = (var2 * mean1 + var1 * mean2) / (var1 + var2)
    new_var = (var1 * var2) / (var1 + var2)
    return [new_mean, new_var]

def predict(mean1, var1, mean2, var2):
    new_mean = mean1 + mean2
    new_var = var1 + var2
    return [new_mean, new_var]

measurements = [5., 6., 7., 9., 10.]
# Inferred motions
motion = [1., 1., 2., 1., 1.]
measurement_sig = 4.
motion_sig = 2.

# Very uncertain initial position belief
mu = 0
sig = 10000

for i in range(len(motion)):
    mu, sig = update(mu, sig, measurements[i], measurement_sig)
    mu, sig = predict(mu, sig, motion[i], motion_sig)

print [mu, sig]

\end{python}
\end{algorithm}


Multi-dimensional Kalman filters allow to infer properties to predict the future, e.g. the velocity in 2d measurements. In that case, this helps a robot to keep track of other vehicles to avoid accidents. They can efficiently be computed through matrix operations. The following elements are needed: {\bf estimate} $x$, {\bf uncertainty covariance} $P$, {\bf state transition matrix} $F$, {\bf motion vector} $u$, {\bf measurement} $z$, {\bf measurement function} $H$ and {\bf measurement noise} $R$. \\

The measurement update is defined as follows:
\begin{align*}
y = z - Hx \\
S = H(PH^T) + R \\
K = P(H^T S^{-1}) \\
x\prime = x + Ky \\
P\prime = (I - KH)P
\end{align*}


The motion update is defined as follows:
\begin{align*}
x\prime = Fx + u \\
P\prime = F(PF^T)
\end{align*}

The Kalman filter is quadratic. It is fully represented by a vector - the mean - and the covariance matrix, which is quadratic. This makes the Kalman filter a lot more efficient because it can operate on a state space with more dimensions. \\

Algorithm~\ref{ref:2dkalmanfilterpython} implements a sample 2d Kalman filter which infers the velocity of the object. The final estimations are:
\begin{align*}
x = 3.99 \\
\dot{x} = 0.99
\end{align*}

\begin{algorithm}
\caption{Sample 2d Kalman filter}
\label{ref:2dkalmanfilterpython}
\begin{python}
# initial state (location and velocity)
x = matrix([[0.], [0.]])
# initial uncertainty
P = matrix([[1000., 0.], [0., 1000.]])
# external motion
u = matrix([[0.], [0.]])
# next state function
F = matrix([[1., 1.], [0, 1.]])
# measurement function
H = matrix([[1., 0.]])
# measurement uncertainty
R = matrix([[1.]]) ]
# identity matrix
I = matrix([[1., 0.], [0., 1.]])

measurements = [1, 2, 3]

def kalman_filter(x, P):
    for n in range(len(measurements)):

        # measurement update
        y = matrix([[measurements[n]]]) - H * x
        S = H * (P * H.transpose()) + R
        K = P * (H.transpose() * S.inverse())
        x = x + K * y
        P = (I - K * H) * P

        # motion update
        x = F * x + u
        P = F * (P * F.transpose())

kalman_filter(x, P)
\end{python}
\end{algorithm}

\section{Particle filter}
{\bf Particle filters} are {\bf continuous} and {\bf multi-modal}. The key advantage of particle filters is their simple implementation. Particle filters maintain a set of $N$ random guesses as to where the robot might be. In the first step, all particles then do a certain {\bf movement} in the space. Then, the most relevant particles are kept and duplicated so that the sum of particles remains $N$. This step is called {\bf resampling}. \\
Particle filters require some noise and uncertainty in the movements and measurements. Noise and uncertainty is important to slightly remix the particles. Also, particle filters do not work well in high-dimensional spaces. Last, particle filters require a good amount of particles and usually do not work well on just a few particles.
\\
Algorithm~\ref{ref:particlefilterpython} implements a particle filter for robot localization and used the robot defined in Algorithm~\ref{ref:robotpython}. Relevance is defined as similar position measured by the distance to four landmarks. The measurement is also uncertain and therefore Gaussian. Efficient resampling is performed by a {\bf resampling wheel}.
A more intuitive but inefficient resampling algorithm is defined in Algorithm~\ref{alg:simpleresampling}.

\begin{algorithm}
\caption{Sample particle filter}
\label{ref:particlefilterpython}
\begin{python}
landmarks  = [[20.0, 20.0], [80.0, 80.0],
              [20.0, 80.0], [80.0, 20.0]]
world_size = 100.0
myrobot = robot()
myrobot = myrobot.move(0.1, 5.0)
T = 2

N = 1000
p = []
for i in range(N):
    x = robot()
    x.set_noise(0.05, 0.05, 5.0)
    p.append(x)

def move():
    global p
    p2 = []
    for i in range(N):
        p2.append(p[i].move(0.1, 5.0))
    p = p2

w = []
def measure_weights():
    Z = myrobot.sense()
    for i in range(N):
        w.append(p[i].measurement_prob(Z))

def resample():
    global p
    p3 = []
    index = int(random.random() * N)
    beta = 0.0
    mw = max(w)
    for i in range(N):
        beta += random.random() * 2.0 * mw
        while beta > w[index]:
            beta -= w[index]
            index = (index + 1) % N
        p3.append(p[index])
    p = p3

def run_particle_filter():
    move()
    measure_weights()
    resample()

for i in range(T):
    run_particle_filter()
    myrobot = myrobot.move(0.1, 5.0)
\end{python}
\end{algorithm}

\begin{algorithm}
\caption{Sample robot}
\label{ref:robotpython}
\begin{python}
class robot:
    # Initializes the robot with random position and direction
    def __init__(self):
        ...

    def move(self, turn, forward):
        ...
        orientation = self.orientation + float(turn) + random.gauss(0.0, self.turn_noise)
        orientation %= 2 * pi
        ...
        # move, and add randomness to the motion command
        dist = float(forward) + random.gauss(0.0, self.forward_noise)
        ...

    def sense(self):
        Z = []
        for i in range(len(landmarks)):
            dist = sqrt((self.x - landmarks[i][0]) ** 2
                     + (self.y - landmarks[i][1]) ** 2)
            dist += random.gauss(0.0, self.sense_noise)
            Z.append(dist)
        return Z

    def Gaussian(self, mu, sigma, x):
        # calculates the probability of x for 1-dim Gaussian
        # with mean mu and var. sigma
        return exp(- ((mu - x) ** 2) / (sigma ** 2) / 2.0)
                     / sqrt(2.0 * pi * (sigma ** 2))

    def measurement_prob(self, measurement):
        # calculates how likely a measurement should be
        prob = 1.0;
        for i in range(len(landmarks)):
            dist = sqrt((self.x - landmarks[i][0]) ** 2
                     + (self.y - landmarks[i][1]) ** 2)
            prob *= self.Gaussian(dist, self.sense_noise,
                                 measurement[i])
        return prob

def eval(r, p):
    sum = 0.0;
    for i in range(len(p)): # calculate mean error
        dx = (p[i].x - r.x + (world_size/2.0)) % world_size - (world_size/2.0)
        dy = (p[i].y - r.y + (world_size/2.0)) % world_size - (world_size/2.0)
        err = sqrt(dx * dx + dy * dy)
        sum += err
    return sum / float(len(p))
\end{python}
\end{algorithm}


\begin{algorithm}
\caption{Simple resampling}
\label{alg:simpleresampling}
\begin{python}
def get_weighted_choice(weights):
    totals = []
    running_total = 0
    for weight in weights:
        running_total += weight
        totals.append(running_total)
    def f():
        rnd = random.random() * running_total
        return bisect.bisect(totals, rnd)

    return f

p3 = []
sampler = get_weighted_choice(w)
for i in range(N):
    j = sampler()
    p3.append(p2[j])
\end{python}
\end{algorithm}

\chapter{Computer vision}
{\bf Computer vision} is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. \\
Many goals of vision can be described as {\bf inverse problems}, because inferences from data are needed to derive models of the world properties that projected those image data.

\section{Introduction}
\label{chapter:cvinvariances}
{\bf Object recognition} maps an input image to an object. Images have variation and a detection algorithm is invariant if the variance does not influence the result. There are various kinds of {\bf invariance}:
\begin{itemize}
\item Scale
\item Illumination
\item Rotation
\item Deformation
\item Occlusion
\item View point
\end{itemize}
Building a truly invariant object recognition algorithm is a unsolved major computer science problem.
\\
{\bf Greyscale images} are used in most computer science algorithms as they contain the relations between objects and are easier to process than RGB images. Values usually range from 0 to 255 which represent white and black respectively. \\
Vertical {\bf feature extraction} is possible using a trivial {\bf mask} (or {\bf filter} or {\bf kernel}) $[+1\vert -1]$ which is applied on an $m\times n$ image and returns an $(m-1)\times n$ images with all columns containing the difference of the original columns. This shift can be avoided using the following mask: $[+1\vert 0\vert -1]$.
Usually, the absolute value of the difference is computed. \\
A {\bf gradient image} is the aggregation of vertical and horizontal filtering. A pixel value threshold can be applied to reduce noise. \\
{\bf Blurring} is useful for multiple use cases. First, it allows to do {\bf anti-aliasing} before downsampling an image. Second, it reduces noise which can improve edge detection. A {\bf Gaussian kernel} blurs an image:
\begin{align*}
\frac{1}{159} \begin{pmatrix}
2 & 4 & 5 & 4 & 2 \\
4 & 9 & 12 & 9 & 4 \\
5 & 12 & 15 & 12 & 5 \\
4 & 9 & 12 & 9 & 4 \\
2 & 4 & 5 & 4 & 2
\end{pmatrix}
\end{align*}


\end{document}