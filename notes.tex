\documentclass{report}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}
\usepackage{xcolor,colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage[chapter]{algorithm}

\setlength{\parindent}{0cm}

\title{Artificial Intelligence Notes}
\author{Patrick Oliver GLAUNER \\
	\texttt{patrick.oliver.glauner@gmail.com}}

\date{\today}

\newtheorem{definition}{Definition}[section]

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
{\em Artificial Intelligence (AI)} is a versatile field and there are many different definitions for it. My favorite definition is:
\begin{flushright}
{\em "AI is the science of knowing what to do when you don't know what to do." (Peter Norvig)}\footnote{\url{https://www.youtube.com/watch?v=rtmQ3xlt-4A4m45}}
\end{flushright}
~\\
This document aggregates definitions and findings that are related to AI with a strong focus on {\em Machine Learning}. It will continuously be updated in the future.
~\\~\\
Most of the content is based on:
\begin{itemize}
\item Stuart Russell and Peter Norvig. {\em Artificial Intelligence: A Modern Approach}. Third edition.\footnote{\url{http://aima.cs.berkeley.edu/}}
\item Andrew Ng. {\em Machine Learning}. Stanford University.\footnote{\url{https://www.coursera.org/course/ml}}
\end{itemize}

~\\~\\~\\~\\
\begin{flushright}
Patrick GLAUNER
\end{flushright}

\end{abstract}


\chapter{General}
\section{Terminology}
\subsection{Intelligent agents}
An {\bf agent} operates autonomously. A {\bf rational agent} acts to achieve the best outcome or expected outcome if there is uncertainty. An {\bf agent program} implements the {\bf agent function} which maps perceptions to actions.

\subsection{Task environments}
{\bf Single vs. multi agent}: the agent is the only one. {\bf Fully observable} vs. {\bf partially observable}: the agent's sensors perceive the complete state of the relevant environment at each point in time. {\bf Deterministic} vs. {\bf stochastic}: the agent's state and action uniquely determine the next state. {\bf Discrete} vs. {\bf continuous}: finite amount of states, actions and outcomes. {\bf Benign} vs. {\bf adversarial}: there is no opponent. {\bf Known vs. unknown}: the agent's knowledge about the "law of physics" of the environment, the outcomes (or probabilities) for all actions are given.



\chapter{Probabilities}
\section{Introduction} 

\begin{definition}[Complement]
$P(\neg A) = 1 - P(A)$
\end{definition}

\begin{definition}[Inclusion-exclusion principle]
$P(A\vee B) =P(A) + P(B) - P(A,B)$
\end{definition}


\section{Independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {A};
\node[mynode, right=1.5cm of A] (B) {B};
\end{tikzpicture}
\caption{Independence}
\label{ref:independence}
\end{figure}

As seen in Figure~\ref{ref:independence}, $A$ and $B$ are independent:
\begin{definition}
P(A,B) = P(A)P(B)
\end{definition}

\begin{definition}
$P(A\vert B) = P(A)$ or $P(B\vert A) = P(B)$ 
\end{definition}


\subsection{Conditional independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,below left=1cm of C] (A) {A};
\node[mynode,below right=1cm of C] (B) {B};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence1}
\end{figure}


As seen in Figure~\ref{ref:condindependence1}, $A$ and $B$ are conditionally independent, given $C$:
\begin{definition}
$P(A,B\vert C) = P(A\vert C)P(B\vert C)$
\end{definition}

\begin{definition}
$P(A\vert B,C) = P(A\vert C)$ and $P(B\vert A,C) = P(B\vert C)$
\end{definition}

$A\independent B \vert C \neq A\independent B$
\\

As seen in Figure~\ref{ref:condindependence2}, $A\independent B$ when $C$ is unknown. When $C$ is known, $A$ and $B$ are dependent. Therefore, independence does not imply conditional independence.


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence2}
\end{figure}


\section{Conditional probabilities}
\begin{definition}[Complement]
$P(\neg A\vert B) = 1 - P(A\vert B)$
\end{definition}

For $a$ and $b$ to be true, $b$ needs to be true and $a$ needs to be true given $b$:
\begin{definition}[Product rule]
$P(A,B) =P(A\vert B)P(B)$
\end{definition}

\subsection{Total probability}
\begin{definition}
$P(A) = \sum_{b\in B}{P(A,b)} = \sum_{b\in B}{P(A\vert b)P(b)}$
\end{definition}

\begin{definition}[For conditional variables]
$P(A\vert C) = \sum_{b\in B}{P(A\vert C,b)P(b\vert C)}$
\end{definition}

\subsection{Bayes' rule}
Applying product rule and total probability:
\begin{definition}
$P(A\vert B) = \frac{P(B\vert A)P(A)}{\sum_{a\in A}{P(B\vert a)P(a)}} = \frac{P(B\vert A)P(A)}{P(B)}$
\end{definition}

The terminology is: $Posterior = \frac{Prior\times Likelihood}{Normalizer}$\\

The normalizer might be difficult to calculate. It can be substituted with pseudo probabilities:
\begin{enumerate}
\item $P'(A\vert B) = P(B\vert A)P(A)$ and $P'(\neg A\vert B) = P(B\vert \neg A)P(\neg A)$
\item $P(A\vert B) = \alpha P'(A\vert B)$ and $P(\neg A\vert B) = \alpha P'(\neg A\vert B)$ with $\alpha = \frac{1}{P'(A\vert B) + P'(\neg A\vert B)}$
\end{enumerate}

\begin{definition}[General Bayes' rule]
$P(A\vert B,e) = \frac{P(B\vert A,e)P(A\vert e)}{P(B\vert e)}$
\end{definition}


\section{Bayes networks}
Bayes networks define probability distributions over random variables and allow compact specification of full joint distributions. The joint probability of the network shown in Figure~\ref{ref:samplenetwork} is: $P(A,B,C,D,E)=P(A)(B)P(C\vert A,B)P(D\vert C)P(E\vert C)$. The general equation is:
\begin{definition}
$P(x_1,...,x_n) = \prod_{i=1}^{n}{P(x_i\vert parents(X_i))}$
\end{definition}


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\node[mynode,below left=1cm of C] (D) {D};
\node[mynode,below right=1cm of C] (E) {E};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D)
(C) edge[-latex] (E);
\end{tikzpicture}
\caption{Sample Bayes network}
\label{ref:samplenetwork}
\end{figure}

\subsection{d-separation}
Stands for direction-dependent separation. d-separated variables are independent. $X$ and $Y$ are d-separated if there is no active path between them. Paths consists of {\bf triplets} as defined in Table~\ref{ref:triplets}. An inactive triplet makes an entire path inactive.

\begin{table}[h!]
\begin{center}
\begin{tabular}{c|c}
Active & Inactive \\
\hline
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
causal chain
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A, fill=black] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
common cause
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode, fill=black] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode,fill=black] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
common effect
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\node[mynode,below=0.5cm of C,color=white] (D) {};
\node[mynode,below=0cm of D, fill=black] (E) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D);
\end{tikzpicture}
&
\\
\end{tabular}
\end{center}
\caption{Active and inactive triplets with known variables filled}
\label{ref:triplets}
\end{table}


\chapter{Problem Solving}
\section{Introduction}
A {\bf problem solving agent} requires a deterministic and fully observable environment. Please see Chapter~\ref{ref:chapterplanning} for planning in more complex environments. Table~\ref{ref:search} compares general search algorithms.

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c|c}
 & Breadth-first & Uniform-cost & Depth-first\\
\hline
\hline
Expansion order
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {3};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {4};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {5};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {4};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {2};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {7};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {6};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {5};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {3};
\path (A) edge[-latex] node[left] {5} (B)
(A) edge[-latex] node[right] {2} (C)
(B) edge[-latex] node[left] {3} (D)
(B) edge[-latex] node[right] {2} (E)
(C) edge[-latex] node[left] {4} (F)
(C) edge[-latex] node[right] {2} (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {5};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {3};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {4};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
\\
\hline
Expansion strategy & shallowest & cheapest & deepest\\
\hline
Optimal & yes & yes & no\\
\hline
Complete & yes & yes & no\\
\hline
Frontier size ($n$ levels) & $2n$ & $2n$ & $n$\\
\end{tabular}
\end{center}
\caption{Comparison of search algorithms}
\label{ref:search}
\end{table}

\section{A* search}
$A*$ expands the path with that has the minimum value $f$.

\begin{definition}[A* cost function]
$f = g + h$ with $g(path) =$ current path cost and $h(path) = h(s) =$ estimated distance to goal
\end{definition}

$f$ is a {\bf heuristic function}. $A*$ finds lowest cost path if: $h(s) <$ true cost. Subsequently, $h$ never overestimates and is optimistic/admissible. See Figure~\ref{ref:heuristic} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
S &\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
9&8&7&6&5&4 \\
\hline
8&7&6&5&4&3 \\
\hline
7&6&5&4&3&2 \\
\hline
6&5&4&3&2&1 \\
\hline
5&4&3&2&1&0 \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and heuristic}
\label{ref:heuristic}
\end{figure}


\section{Policy}
A {\bf policy} specifies what an agent should do for any state that the agent might reach. An {\bf optimal policy} is a policy that yields the highest expected {\bf utility} (meaning "the quality of being useful"). See Figure~\ref{ref:policy} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&\cellcolor{black}&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\rightarrow$&$\rightarrow$&$\uparrow$&$\uparrow$&&* \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and policy}
\label{ref:policy}
\end{figure}


\chapter{Logic}
\section{Introduction}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c}
& World & Beliefs \\
\hline
\hline
Probability theory & facts & $[0,1]$ \\
\hline
Propositional logic & facts & true, false, unknown \\
\hline
First-order logic & relations, objects, functions & true, false, unknown \\
\end{tabular}
\end{center}
\caption{Comparison of formal languages}
\label{ref:complang}
\end{table}


\section{Propositional logic}
Equivalent to Boolean Algebra. There are efficient inference algorithms to determine validity and satisfiability. Limitations:
\begin{itemize}
  \item It can only handle true and false values
  \item No capability to handle uncertainty
  \item No objects
  \item No shortcuts to express similar properties, only like: $A_1 \wedge A_2 \wedge ... \wedge A_n$
\end{itemize}


\section{First-order logic}
First-order logic is sufficiently expressive to represent a reasonable amount of our commonsense knowledge. It is also used in many further representation languages and has been studied intensively for many decades. The first-order logic syntax is defined in Figure~\ref{ref:firstordersyntax}.

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$Sentence$ & $\rightarrow$ & $AtomicSentence$ $\vert$ $ComplexSentence$ \\
$AtomicSentence$ & $\rightarrow$ & $Predicate$ $\vert$ $Predicate(Term,...)$ $\vert$ $Term=Term$ \\
$ComplexSentence$ & $\rightarrow$ & $(Sentence)$ $\vert$ $[Sentence]$ \\
 & $\vert$ & $\neg Sentence$ \\
 & $\vert$ & $Sentence \wedge Sentence$ \\
 & $\vert$ & $Sentence \vee Sentence$ \\
 & $\vert$ & $Sentence \implies Sentence$ \\
 & $\vert$ & $Sentence \iff Sentence$ \\
 & $\vert$ & $Quantifier$ $Variable, ...$ $Sentence$ \\
\\
$Term$ & $\rightarrow$ & $Function(Term,...)$ \\
 & $\vert$ & $Constant$ \\
 & $\vert$ & $Variable$ \\
 \\
$Quantifier$ & $\rightarrow$ & $\forall$ $\vert$ $\exists$ \\
$Constant$ & $\rightarrow$ & $A$ $\vert$ $X_1$ $\vert$ $John$ $\vert$ $...$ \\
$Variable$ & $\rightarrow$ & $a$ $\vert$ $x$ $\vert$ $s$ $\vert$ $..$ \\
$Predicate$ & $\rightarrow$ & $True$ $\vert$ $False$ $\vert$ $After$ $\vert$ $Loves$ $\vert$ $Raining$ $\vert$ $...$\\
$Function$ & $\rightarrow$ & $FullName$ $\vert$ $PhoneNumber$ $\vert$ $...$ \\
\\
$OPERATOR$ $PRECEDENCE$ & $:$ & $\neg,=,\wedge,\vee,\implies,\iff$ \\
\end{tabular}
\caption{First-order logic syntax}
\label{ref:firstordersyntax}
\end{figure}



\section{Higher-order logic}
Views relations and functions as objects themselves. Therefore, it allows more powerful expressions, e.g. relations on relations: $\forall_R$ $Transitive(R)\iff \forall a,b,c$ $R(a,b)\wedge R(b,c) \implies R(a,c)$


\chapter{Planning}
\label{ref:chapterplanning}
A {\bf planning agent} interleaves planning and execution. This is necessary for environments which are {\bf stochastic}, {\bf multi agent}, {\bf partially observable} or {\bf unknown}. Plans might be {\bf hierarchical} which consist of actions on different levels. Some of these levels can be planned ahead, others while executing the plan. Example: A* can find a route from A to Z but the route might change due to accidents. Low-level actions such as turning the steering wheel or pressing the pedals cannot be planned well-ahead in time in the real world.


\chapter{Learning}

{\bf Machine learning} is the field of study that gives computers the ability to learn without being explicitly programmed. This means that machine learning algorithms learn models from data. This is different to Bayes networks which reasons with known models.

What:
\begin{itemize}
  \item Parameters
  \item Structure
  \item Hidden concepts
\end{itemize}

What from:
\begin{itemize}
  \item Supervised
  \item Unsupervised
  \item Reinforcement
\end{itemize}

What for:
\begin{itemize}
  \item Prediction
  \item Diagnostics
  \item Summarization
  \item ...
\end{itemize}

How:
\begin{itemize}
  \item Passive: learning agent is just an observer and has no impact on the data itself
  \item Active
  \item Online: learning while data is being generated
  \item Offline: learning by processing data in batch
\end{itemize}

Outputs:
\begin{itemize}
  \item Classification: discrete values
  \item Regression: continuous
\end{itemize}

Details:
\begin{itemize}
  \item Generative: model data as generally as possible
  \item Descriminative: seek to distinguish data
\end{itemize}


\chapter{Supervised learning}
Supervised learning learns from {\bf labeled training examples}. If there are not sufficiently many features, the learning algorithm may {\bf underfit}. In contrast, it may {\bf overfit} for too many features meaning it only matches the training data well but fails to generalize to new examples. The vast majority of research in machine learning has been done in supervised learning.

\section{Linear regression}
{\bf Linear regression} computes a linear regression function for $m$ {\bf training examples} having $n$ {\bf features}. The objective is to minimize the cost function:
\begin{definition}[Cost function]
$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
\end{definition}

Where the hypothesis $h_\theta(x^{(i)})$ is given by the linear model:
\begin{definition}[Hypothesis]
$h_\theta(x^{(i)}) = \theta^{T}x = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n$ with $x_0 = 1$
\end{definition}

The parameters of the model to be learned are the $\theta_j$ values. One way to do this is to use the batch {\bf gradient descent} algorithm. Through is step, the parameters $\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\theta)$. If the {\bf learning rate} $\alpha$ is too large, gradient descent might not converge and overshoot the minimum after a certain amount of iterations. Gradient descent might generally not find the global but the local minimum. Gradient descent returns the global minimum for convex or bowl-shaped cost functions which do not have local optima. {\bf Feature normalization} allows to perform gradient descent quickly by normalizing each feature to the mean and scaling it to its standard deviation.


\begin{definition}[Gradient descent] ~\\
loop until converge \{ \\
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (simultaneously update $\theta_j$ for all $j$) \\
\}
\end{definition}

\begin{definition}[Feature normalization]
$x_j' = \frac{x_j - \mu_j}{\sigma_j}$
\end{definition}

{\bf Vectorization} allows to do the updates simultaneously through various matrix operations. Matrix multiplications can be executed very efficiently (Strassen algorithm). Matrix $X$ contains $m$ transposed training data columns and $n$ features where feature $x_0$ contains entirely 1s:
$X$ = $\begin{pmatrix}
\mbox{------} (x^{(1)})^T \mbox{------} \\
\mbox{------} (x^{(2)})^T \mbox{------} \\
... \\
\mbox{------} (x^{(m)})^T \mbox{------}
\end{pmatrix}$


The cost function and gradient descent then look:

\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{2m}(X\theta-y)^T(X\theta-y)$\\
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(h_{\theta}(x)-y) := \theta - \alpha \frac{1}{m}X^{T}(X\theta-y)$
\end{definition}

The {\bf normal equation} is a closed-form solution for linear regression which requires no loop, no learning rate and no feature normalization. It may be too slow for very large matrices as inversion is $O(n^3)$. $X$ may be singular and cannot be inverted. It is usually recommended to calculate the pseudo inverse.
\begin{definition}[Normal equation]
$\theta = (X^{T}X)^{-1}X^{T}y$
\end{definition}


\section{Logistic regression}

Logistic regression allows linear classification. The parameters $\theta_j$ comprise the linear separator called decision boundary. The hypothesis is defined as:
\begin{definition}[Hypothesis]
$h_\theta(x) = g(\theta^{T}x)$
\end{definition}

The hypothesis can be more complex to use logistic regression for non-linear classification. Once trained, logistic regression predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). $g$ is the {\bf sigmoid function} which is defined as:
\begin{definition}[Sigmoid function]
$g(z) = \frac{1}{1 + e^{-z}}$
\end{definition}

The cost function in logistic regression is convex:
\begin{definition}[Cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$
\end{definition}

\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{m}[-y^T log(g(X\theta))-(1-y)^T log(1-g(X\theta))]$
\end{definition}

This gradient descent looks identical to the linear regression gradient but the formula is actually different because of the different definition of $h_{\theta}(x)$.
\begin{definition}[Gradient descent]
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(g(X\theta)-y)$
\end{definition}


\section{Regularization}
{\bf Regularization} allows to prevent overfitting by "penalizing" large $\theta_j$ values. Feature $x_0$ is usually not affected by regularization. If $\lambda$ is set too large, the learning algorithm may underfit. For linear regression, the definitions change to:

\begin{definition}[Regularized linear regression cost function] ~\\
$J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+ {\bf \lambda \sum_{j=1}^n\theta_j^2}]$
\end{definition}

\begin{definition}[Regularized linear regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} {\bf +\frac{\lambda}{m}\theta_j}]$
\end{definition}

Regularization also allows the normal equation to be invertible by adding a $(n+1)\times (n+1)$ matrix:
\begin{definition}[Regularized normal equation] ~\\
$\theta = (X^{T}X+\lambda
\begin{pmatrix}
0 & & & \\
&1& &\\
& & ... & \\
& & & 1
\end{pmatrix}
)^{-1}X^{T}y$
\end{definition}

For logistic regression, the definitions change to:

\begin{definition}[Regularized logistic regression cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))] {\bf + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2}$
\end{definition}

\begin{definition}[Regularized logistic regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ) {\bf + \frac{\lambda}{m}\theta_j}]$
\end{definition}

\section{Naive Bayes}
{\bf Bayes classifiers} rely on Bayes rule which maps a vector $X$ to a discrete random variable $C$ - a class. Training such a classifier is impractical because of the necessary exponential amount of training examples. A {\bf naive Bayes classifier} relaxes conditional dependency by assuming conditional independence between the parameters in $X$ given $C$. Naive Bayes is a powerful classifier that is often used in text classification (e.g. spam filtering) and is a dependable baseline when comparing different classifiers. Usually, it works on a so-called {\bf bag of words} which is a frequency of words in a text.
\begin{align*}
P(C\vert X) = P(C\vert x_1...x_n) = \frac{P(C)P(x_1...x_n\vert C)}{P(x_1...x_n)}
\end{align*}
\begin{align*}
= \frac{P(C)P(x_1\vert C)...P(x_n\vert C)}{P(x_1)...P(x_n)}
\end{align*}

To get the most likely class, the denominator can get dropped as it is independent of $C$.

\begin{align*}
C = \max\limits_{C} P(C)\prod_iP(x_i\vert C)
\end{align*}

\subsection{Using naive Bayes}
The probability:
\begin{align*}
P(x_i) = \frac{count(x_i)}{N}
\end{align*}
might be zero and would then reduce the entire product to zero. {\bf Laplacian smoothing} can deal with this overfitting problem:
\begin{align*}
P(x_i) = \frac{count(x_i+k)}{N+k\vert X\vert}
\end{align*}
It adds $k$ to the numerator and normalizes by adding $k$ to every single class of $X$. This change to the normalizer guarantees that $\sum_i P(x_i) = 1$.
Using a {\bf cross validation set} helps to find the best $k$, see Chapter~\ref{ref:modelselection}.
\\
\\
To prevent numerical underflow while multiplying lots of probabilities, naive Bayes can add the logarithms of these probabilities:
\begin{align*}
C = \max\limits_{C} log(P(C))+\sum_i log(P(x_i\vert C))
\end{align*}

\subsection{Spam classification example}
In the section, naive Bayes is demonstrated on a spam filter example. Bad messages are called "Spam", good messages are "ham". The sample messages are shown in Figure~\ref{ref:spamsample}.

\begin{figure}[h!]
\centering
\begin{tabular}{c||l|l}
& SPAM & HAM \\
\hline
\hline
1 & OFFER IS SECRET & PLAY SPORTS TODAY \\
2 & CLICK SECRET LINK & WENT PLAY SPORTS \\
3 & SECRET SPORTS LINK & SECRET SPORTS EVENT \\
4 & & SPORTS IS TODAY \\
5 & & SPORTS COSTS MONEY \\
\end{tabular}
\caption{Sample messages}
\label{ref:spamsample}
\end{figure}

The following non-smoothed ($K=0$) probabilities can then be computed which are based on the empirical count:
\begin{itemize}
\item $P(SPAM) = 3/8$
\item $P("SECRET"\vert SPAM) = 1/3$
\item $P("SECRET"\vert HAM) = 1/15$
\item $P(SPAM\vert "SPORTS") = \frac{P("SPORTS"\vert SPAM)P(SPAM)}{P("SPORTS")}=\frac{1/9 * 3/8}{6/24}=3/18$
\item $P(SPAM\vert "SECRET\,IS\,SECRET")= \frac{P("SECRET\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("SECRET\:IS\:SECRET")}$\\
$=\frac{1/3*1/9*1/3*3/8}{1/3*1/9*1/3*3/8+1/15*1/15*1/15*5/8}=\frac{1/216}{1/216+1/5400}=\frac{25}{26}$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{0*1/9*1/3*3/8}{0*1/9*1/3*3/8+2/15*1/15*1/15*5/8}=0$
\end{itemize}

The last example demonstrates a clear overfit. The following smoothed ($K=1$) probabilities can be computed. The last examples how the model deals with with the nonexistence of "TODAY" in the spam messages.
\begin{itemize}
\item $P(SPAM) = \frac{3+{\bf 1}}{8+{\bf 2}} = 2/5$
\item $P(HAM) = \frac{5+{\bf 1}}{8+{\bf 2}} = 3/5$
\item $P("TODAY"\vert SPAM) = \frac{0+{\bf 1}}{9+{\bf 12}} = 1/21$
\item $P("TODAY"\vert HAM) = \frac{2+{\bf 1}}{15+{\bf 12}} = 1/9$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{1/21*2/21*4/21*2/5}{1/21*2/21*4/21*2/5+3/27*2/27*2/27*3/5}=0.4858$
\end{itemize}

%+ Limitations, but other factors can be added

\subsection{Comparison to logistic regression}
Naive Bayes requires only a small amount of data, converges quickly and is very fast. In comparison, logistic regression requires more data and computations are more expensive. Nonetheless, it can outperform naive Bayes on a lot of data.


\section{Neural network}
A neuron has {\bf inputs} and an {\bf output} as seen in Figure~\ref{ref:neuron}. A {\bf neural network} consists of layers of {\bf neurons}. Each layer has a {\bf bias term} of value 1. The parameters $\theta_j$ are called {\bf weights} in some sources. The hypothesis is called {\bf Sigmoid activation function}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=2cm and 1cm of C] (X0) {$1$};
\node[mynode,above left=1cm and 1cm of C] (X1) {$x_1$};
\node[mynode,above left=0cm and 1cm of C] (X2) {$x_2$};
\node[mynode,below left=0cm and 1cm of C,draw=white] (X3) {...};
\node[mynode,below left=1cm and 1cm of C] (XN) {$x_n$};
\node[mynode,right=1cm and 1cm of C,draw=white] (h) {$h_{\theta}(x)$};
\path (X0) edge[-latex] (C)
(X1) edge[-latex] (C)
(X2) edge[-latex] (C)
(X3) edge[-latex] (C)
(XN) edge[-latex] (C)
(C) edge[-latex] (h);
\end{tikzpicture}
\caption{Neuron with bias term $x_0=1$}
\label{ref:neuron}
\end{figure}

\begin{definition}[Sigmoid activation function]
$h_\theta(x) = \frac{1}{1+e^{-\theta^{T}x}}$
\end{definition}

Each neural network has an {\bf input layer}, $\ge 0$ {\bf hidden layers} and an {\bf output layer} as seen in Figure~\ref{ref:neuralnetwork}. The value $a_i^{(j)}$ is called {\bf activation} or {\bf forward propagation} of unit $i$ in layer $j$. $\Theta^{(j)}$ is a matrix of weights to map from layer $j$ to layer $j+1$. If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1}\times (s_j + 1)$. For the neural network in Figure~\ref{ref:neuralnetwork}, the mapping is defined in Figure~\ref{ref:neuralnetworkmapping}. Once trained, a neural network predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). \\

The cost function for $K$ output units and $L$ layers is defined as follows (with $h_\Theta(x) \in \mathbb{R}^K$ and $(h_\Theta(x))_i = i^{th}$ output):

\begin{definition}[Regularized cost function] ~\\
$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}log(h_{\Theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)}))_k] \\
+ \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}(\Theta_{ij}^{(l)})^2$
\end{definition}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A12) {$a_1^{(2)}$};
\node[mynode,below=0.5cm of A12] (A22) {$a_2^{(2)}$};
\node[mynode,below=0.5cm of A22] (A32) {$a_3^{(2)}$};
\node[mynode,left=0cm and 1cm of A12] (X1) {$x_1$};
\node[mynode,left=0cm and 1cm of A22] (X2) {$x_2$};
\node[mynode,left=0cm and 1cm of A32] (X3) {$x_3$};
\node[mynode,right=0cm and 1cm of A22] (L3) {};
\node[mynode,right=0cm and 1cm of L3,draw=white] (h) {$h_{\Theta}(x)$};
\path (X1) edge[-latex] (A12)
(X1) edge[-latex] (A22)
(X1) edge[-latex] (A32)
(X2) edge[-latex] (A12)
(X2) edge[-latex] (A22)
(X2) edge[-latex] (A32)
(X3) edge[-latex] (A12)
(X3) edge[-latex] (A22)
(X3) edge[-latex] (A32)
(A12) edge[-latex] (L3)
(A22) edge[-latex] (L3)
(A32) edge[-latex] (L3)
(L3) edge[-latex] (h);
\end{tikzpicture}
\caption{Neural network with three layers (bias terms not displayed)}
\label{ref:neuralnetwork}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$a_1^{(2)}$ & $=$ & $g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)$ \\
$a_2^{(2)}$ & $=$ & $g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)$ \\
$a_3^{(2)}$ & $=$ & $g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)$ \\
$h_{\Theta}(x)$ & $=$ & $a_1^{(3)}$ $=$ $g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$ \\
\end{tabular}
\caption{Activation of layers in a neural network}
\label{ref:neuralnetworkmapping}
\end{figure}

\subsection{Backpropagation}
Training a neural network requires to $\min\limits_{\Theta}J(\Theta)$, e.g. through gradient descent which requires $J(\Theta)$ and $\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$. {\bf Backpropagation} is a method to compute the gradient as described in Figure~\ref{ref:backpropagation}. This algorithm can also be vectorized. The derivative of the Sigmoid function is defined as follows:
\begin{definition}[Derivative of Sigmoid function] ~\\
$g^{'}(x)=g(x)*(1-g(x))$
\end{definition}

\begin{figure}[h!]
Training set $\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}$ \\
Set $\Delta_{ij}^{(l)}=0$ (for all $l,i,j$). \\
For $i=1$ to $m$
\begin{tabbing}
~~~~ \= Set $a^{(i)}=x^{(i)}$ \\
\> Perform forward propagation to compute $a^{(l)}$ for $l=2,3,...,L$ \\
\> Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$ ("error") \\
\> Compute $\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$, e.g. $\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}.*g^{'}(z^{(2)})$ (element-wise multiplication) \\
\> $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i{(l+1)}$ \\
$D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\frac{\lambda}{m}\Theta_{ij}^{(l)}$ if $j\ne0$ (no regularization of bias terms) \\
$D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}$ if $j=0$
\\
\\
$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$
\end{tabbing}
\caption{Backpropagation algorithm}
\label{ref:backpropagation}
\end{figure}

$\Theta_{ij}^{(l)}$ needs to have an initial value. If $\Theta$ was entirely 0, {\bf symmetry} would create a highly redundant neural network with poor learning capabilities. {\bf Random initialization} allows symmetry breaking by initializing each $\Theta_{ij}^{(l)}$ to a random value in $[-\epsilon,\epsilon]$.

\subsection{Gradient checking}
As backpropagation is a complex algorithm, subtile bugs might not be caught and gradient descent might not reach the intended global minimum but only a more expensive minimum. {\bf Gradient checking} allows to approximate the gradient and compare it to the gradient returned by backpropagation. The approximation is computed as follows for each feature $\theta_i$:
\begin{definition}[Gradient checking] ~\\
$\frac{\partial}{\partial\theta_i}J(\theta)\approx\frac{J(...,\theta_{i-1},\theta_{i}+\epsilon,\theta_{i+1},...)-J(...,\theta_{i-1},\theta_{i}-\epsilon,\theta_{i+1},...)}{2\epsilon}$ for small $\epsilon$
\end{definition}

\subsection{Picking a network architecture}
The following rules can be followed to decide on the network architecture:
\begin{itemize}
\item No. of input units: dimension of features $x^(i)$
\item No. output units: number of classes
\item No. of hidden layers: reasonable default: $1$ hidden layer, if $>1$, same number of units in every layer
\item No. of units per layer: usually the more the better (but also computationally more expensive)
\end{itemize}

\section{Model selection}
\label{ref:modelselection}
{\bf Model selection} is the task of selecting a statistical model from a set of candidate models, given data. For example, different hypotheses for linear regression. Usually, the data is split up in three sets: {\bf training set} (60\%), {\bf cross validation set} (20\%) and {\bf test set} (20\%). A a non-regularized error function can be computed for each of these sets, such as for the test set:

\begin{definition}[Linear regression error function] ~\\
$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$
\end{definition}

\begin{definition}[Logistic regression error function] ~\\
$J_{test}(\theta) = -\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}[y_{test}^{(i)}log(h_\theta(x_{test}^{(i)}))+(1-y_{test}^{(i)})log(h_\theta(x_{test}^{(i)}))]$
\end{definition}

Alternatively, the following error function might be easier to interpret for logistic regression:

\begin{definition}[0/1 misclassification error] ~\\
$err(h_\theta(x),y)=
\left\{
\begin{array}{lll}
1  & \mbox{if } h_\theta(x) \geq 0.5, & y=0 \\
  & \mbox{if } h_\theta(x) < 0.5, & y=1 \\
0 & otherwise
\end{array}
\right.$ \\
$J_{test}(\theta) = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y^{(i)})$
\end{definition}


First, the learning algorithm is trained by using the training data for each hypothesis resulting in $\Theta^{(n)}$ for each hypothesis. Second, model {k} with the lowest $J_{CV}(\Theta^{(k)})$ is chosen. Last, the {\bf generalization error} can be computed for the test set $J_{test}(\Theta^{(k)})$.
\\
\\
A similar approach can be done to find a good regularization parameter $\lambda$.

\section{Debugging a learning algorithm}
Implementing a supervised learning algorithm and testing it on new data might reveal unacceptably large errors in its predictions. This section is to give advice on what to do next. Implementing a {\bf diagnostic} might be time-consuming, but doing so can be a very good use of time to get directed into the right direction. In this context, underfitting is called {\bf bias} and overfitting {\bf variance}.

In non-regularized model selection, to diagnose if it is a bias problem or variance problem, the following guidelines may help:
\begin{itemize}
\item Bias: $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\item Variance: $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\end{itemize}

To determine if the regularization parameter $\lambda$ is the cause of bias and variance, the following guidelines may help for a function of $\lambda$:
\begin{itemize}
\item Variance (too small $\lambda$): $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\item Bias (too large $\lambda$): $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\end{itemize}

{\bf Learning curves} can be used to plot a learning's algorithm error for more training examples. Usually, the training error increases as the learning algorithm's hypothesis fails to fit more data. Quite the opposite, the cross validation error decreases because the algorithm generalizes better. For high bias, both errors are high and converge quickly. Therefore, more training data will not help. For high variance, there is a large gap between the cross validation error and the training error. In this case, more data is likely to help.
\\ \\
"Small" {\bf neural networks} which have fewer parameters are prone to underfitting but computationally cheaper. "Large" neural networks have more parameters and are more prone to overfitting (can be addressed by regularization) but computationally more expensive.
\\ \\
In summary, the following actions can help to solve particular problems:
\begin{itemize}
\item Get more training examples: fixes high variance
\item Try smaller set of features: fixes high variance
\item Try getting additional features: fixes high bias
\item Try adding polynomial features: fixes high bias
\item Try decreasing $\lambda$: fixes high bias
\item Try increasing $\lambda$: fixes high variance
\end{itemize}

\section{Error analysis}
The recommended approach to develop a machine learning algorithm is to start with a simple algorithm that can be implemented quickly. Then, it can be tested a small set of data, e.g. a couple of hundreds of records. The cross-validation data can be used to plot the learning curves. This allows to decide if more data, more features etc. are likely to help. In the {\bf error analysis} the examples on which the algorithm made errors on are manually examined. This can help to spot any systematic trend in what type of examples it makes errors on, e.g. stemming, capitalization etc.
\\ \\
{\bf Precision} and {\bf recall} allow to describe classification errors mathematically as described in Figure~\ref{ref:classificationerror}. Precision and and recall can be changed by amending the threshold of the classifier such as the sigmoid function of logistic regression. Usually, a tradeoff needs to be made. The {\bf $F_1$ score} allows to compare precision and recall.

\begin{figure}[h!]
\centering
\begin{tabular}{r||ccc}
& & Actual class & \\
\hline
\hline
& & 1 & 0 \\
Predicted & 1 & True positive & False positive \\
class & 0 & False negative & True negative \\
\end{tabular}
\caption{Classification and errors}
\label{ref:classificationerror}
\end{figure}


\begin{definition}[Precision]
$\frac{True positive}{True positive + False positive}$
\end{definition}

\begin{definition}[Recall]
$\frac{True positive}{True positive + False negative}$
\end{definition}

\begin{definition}[$F_1$ score]
$2\frac{PR}{P+R}$
\end{definition}

\section{Support vector machines}
A {\bf support vector machine} (SVM) is a large margin linear classifier.
\begin{definition}[SVM hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tx \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

In order to get a large margin, the motivation for the cost function definition is:
\begin{itemize}
\item If $y=1$, we want $\theta^Tx\ge 1$ (not just $\ge 0$)
\item If $y=0$, we want $\theta^Tx\le -1$ (not just $\le 0$)
\end{itemize}

\begin{definition}[SVM cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\\
$C = \frac{1}{\lambda}$
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \ge 1 \\
-z + 1  & \mbox{else}
\end{array}
\right.$ \\
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \le 1 \\
z + 1  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

SVMs can be used for non-linear classification. On the one hand, higher order polynomials can be used as in logistic regression which is relatively complex. On the other hand, SVMs can use the {\bf kernel trick} which implicitly maps inputs into high-dimensional feature spaces. In some sources, linear SVMs use a so-called "linear kernel".

\begin{definition}[Gaussian kernel]
$e^{-\frac{\lVert x - l^{(i)} \rVert^2}{2\sigma^2}}$
\end{definition}

Given $x$, new features $f$ depending on proximity to landmarks of the training set $l^{(1)}, l^{(2)}, ..., l^{(m)}$ can be computed:
\begin{itemize}
\item $f_0 = 1$
\item $f_1 = similarity(x,l^{(1)})=e^{-\frac{\lVert x - l^{(1)} \rVert^2}{2\sigma^2}}$
\item $f_2 = similarity(x,l^{(2)})=e^{-\frac{\lVert x - l^{(2)} \rVert^2}{2\sigma^2}}$
\item ...
\item $f_m = similarity(x,l^{(m)})=e^{-\frac{\lVert x - l^{(m)} \rVert^2}{2\sigma^2}}$
\end{itemize}

$f = [f_0\:f_1\:...\:f_m]^T$. $f_1^{(i)}=sim(x^{(i)},l^{(1)})$. $f^{(i)}$ is a vector of $sim(x^{(i)},l^{(k)})$ for $k \in [0,m]$. \\
If $x\approx l^{(i)}$, then $f_i\approx 1$. If $x$ far from $l^{(i)}$, then $f_i\approx 0$.

\begin{definition}[SVM kernel hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tf \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

\begin{definition}[SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\end{definition}

To optimize the training, the regularization part is slightly changed:
\begin{definition}[Optimized SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^{\bf m}\theta_j^2$
\end{definition}

Changing the SVM parameters $C(=\frac{1}{\lambda})$ and $\sigma^2$ has impact on bias an variance.  A large $C$ results in lower bias and higher variance whereas a small $C$ results in higher bias and lower variance. A large $\sigma^2$ results in higher bias and lower variance because the features $f_i$ vary more smoothly. In contrast, a small $\sigma^2$ results in lower bias and higher variance.

\section{Multiclass classification}
In {\bf multiclass classification} examples can be classified into more than two classes. Some classification algorithms permit to do this natively. Generally, this can be done using {\bf one-vs.-all} (one-vs.-rest) classification which trains one binary classifier for each class. Finally, on a new input to make a prediction, the largest classifier is picked.

\section{Applying PCA}
The {\bf principal component analysis} (PCA) presented in Section~\ref{ref:pcasection} can be used in supervised learning. It allows to map high-dimensional data (such as images or emails in spam filters) to lower dimensions to speed up a learning algorithm. The mapping $x^{(i)}\rightarrow z^{(i)}$ should be defined by running PCA only on the training set. This mapping can be applied as well to the examples $x^{(i)}_{cv}$ and $x^{(i)}_{test}$ in the cross validation and test sets. \\

PCA should only be used if the amount of initial dimensions significantly slows down the learning algorithm. PCA should not be used to prevent overfitting by reducing the amount of features. This might work but drops potentially relevant information. Regularization is a better way to address overfitting.


\section{Recommender systems}
{\bf Recommender systems} recommend items to users based on item ratings. There are $n_u$ number of users, $n_m$ number of items, $r(i,j)=1$ if user $j$ has rated item $i$ and $y^{(i,j)}$ is the rating given by user $j$. All ratings are on the same scale $[0,max]$, e.g. $max=5$.

\begin{figure}[h!]
\centering
\begin{tabular}{c||cccc||cc}
Item & $User_A$ & $User_B$ & $User_C$ & $User_D$ & $x_1\:(Feature_A)$ & $x_2\:(Feature_B)$ \\
\hline
\hline
$Item_A$ & 5 & 5 & 0 & 0 & 0.9 & 0 \\
$Item_B$ & 5 & ? & ? & 0 & 1.0 & 0.01 \\
$Item_C$ & ? & 4 & 0 & ? & 0.99 & 0 \\
$Item_D$ & 0 & 0 & 5 & 4 & 0.1 &1.0 \\
$Item_E$ & 0 & 0 & 5 & ? & 0 & 0.9 \\
\end{tabular}
\caption{Sample item ratings}
\label{ref:sampleitemratings}
\end{figure}

Figure~\ref{ref:sampleitemratings} contains a sample set of items, users and features. $n_u=4$, $n_m=5$, $x^{(1)}= [1\:0.9\:0]^T$ (as $x_0=1$). To predict rating of item $i$ of user $j$:
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})
\end{align*}

$i:r(i,j)$ stands for all items $i$ that user $j$ has rated. Vice versa, $j:r(i,j)$ stands for all users $j$ that rated item $i$. Collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$.

\begin{definition}[$\theta^{(1)},...,\theta^{(n_u)}$ estimation minimization objective]~\\
 $\min\limits_{\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[$x^{(1)},...,x^{(n_m)}$ estimation minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)}}\:\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2$
\end{definition}


Both definitions are very related to linear regression. In contrast, there is no $m$ in the denominators as it is in linear regression. This constant is irrelevant for the minimization objective. \\

$\theta^{(1)},...,\theta^{(n_u)}$ can be estimated given $x^{(1)},...,x^{(n_m)}$. Vice versa, $x^{(1)},...,x^{(n_m)}$ can be estimated given $\theta^{(1)},...,\theta^{(n_u)}$. This is a chicken or egg problem. One way to solve this problem is: guess $\theta \rightarrow x \rightarrow \theta \rightarrow x  \rightarrow \theta \rightarrow ...$. This algorithm is called {\bf collaborative filtering}. The term means that with every user rating items (collaboration), the algorithm learns better features. The algorithm is inefficient and can be improved in a way to compute $x^{(1)},...,x^{(n_m)}$ and $\theta^{(1)},...,\theta^{(n_u)}$ simultaneously as defined in Algorithm~\ref{ref:collaborativefiltering}.

\begin{definition}[Simultaneous collaborative filtering minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[Simultaneous collaborative filtering gradient descent]~\\
$x_k^{(i)}:=x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$ \\
$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\end{definition}

\begin{algorithm}
\caption{Collaborative filtering}
\label{ref:collaborativefiltering}
\begin{algorithmic}
\State 1. Initialize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$ to small random values \Comment{Break symmetry}
\State 2. Minimize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$
\State $x_k^{(i)}\gets x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$
\State $\theta_k^{(j)}\gets \theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\State 3. For a user with parameters $\theta$ and an item with (learned) features $x$, predict a rating
\State $y^{(i,j)}\gets (\theta^{(j)})^T(x^{(i)})$
\end{algorithmic}
\end{algorithm}

This algorithm needs to break symmetry by randomly initializing $x^{(1)},...,x^{(n_m)},$ $\theta^{(1)},...,\theta^{(n_u)}$ which is related to neural networks. As the algorithm does not iterate over $x_0$ or $\theta_0$, all elements are regularized. Alternatively, the last step of Algorithm~\ref{ref:collaborativefiltering}, can be vectorized:
\begin{align*}
Y = X\Theta^T
\end{align*}

This is also called {\bf low rank matrix factorization}.
\\
\\
As collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$, the most related items $j$ to item $i$ can be found by taking the $j$ items with smallest $\lVert x^{(i)} - x^{(j)}\rVert$ as a small $\lVert x^{(i)} - x^{(j)}\rVert$ indicates "similarity".
\\
\\
In practice, some users might have not rated any items. In that case, all predictions for this user are $0$. {\bf Mean normalization} can be applied to matrix $Y$:
\begin{align*}
\mu_i = \frac{1}{m^{(i)}} \sum_{j:r(i,j)=1}y^{(i,j)}
\end{align*}
\begin{align*}
Y = Y - \mu
\end{align*}
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})+\mu_i
\end{align*}
Mean normalization allows to predict a mean value per item which is more meaningful than $0$. It only takes rated items into account with $m^{(i)}$ number users who rated item $i$. Feature scaling is not necessary as all ratings are already on the same scale.


\chapter{Unsupervised learning}
Unsupervised learning tries to find hidden structure in {\bf unlabeled data} $\{x^{(1)}, x^{(2)},$ $..., x^{(m)}\}$. In machine learning research, unsupervised learning has received less attention than supervised learning but is likely to receive more in the future because of its potential to work on unlabeled data.

\section{k-means}
{\bf k-means} partitions $m$ observations $x^{(i)}$ into $K$ clusters $\mu_k$ as defined in Algorithm~\ref{ref:kmeans}. The optimization objective is to minimize the cost function. The initial cluster coordinates are random which makes this algorithm non-deterministic. A recommended strategy is to randomly pick $K$ training examples as the initial coordinates to make sure that the initial coordinates are not too far away from the actual data. k-means can be run multiple times on the same data to pick the result with the lowest cost. Choosing the number of clusters $K$ is usually done manually depending on the use case.

\begin{algorithm}
\caption{k-means}
\label{ref:kmeans}
\begin{algorithmic}
\State $\mu_1, \mu_2, ..., \mu_K \in \mathbb{R}^n \gets$ random initialization
\Repeat
\For{$i=1$ to $m$}
\State $c^{(i)} \gets$ index of cluster centroid closest to $x^{(i)}$
\EndFor
\For{$k=1$ to $K$}
\State $\mu_k \gets$ mean of points assigned to cluster $k$
\EndFor
\Until{conversion}
\end{algorithmic}
\end{algorithm}

\begin{definition}[k-means cost function]~\\
$J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K)=\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-\mu_{c^{(i)}}\rVert^2 $
\end{definition}

\section{Principal component analysis}
\label{ref:pcasection}
"{\bf Principal component analysis} (PCA) computes the most meaningful basis to re-express a noisy, garbled data set"\footnote{\url{http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf}} and can be used for {\bf dimensionality reduction}. In this process, higher dimensional data is mapped to a lower dimension, the {\bf number of principal components}, as defined in Algorithm~\ref{ref:pca}. The goal of PCA is to minimize the {\bf average squared projection error} of all original data and their projections. PCA can be used to reduce memory and time consumption of complex computations. Also, it allows to plot higher dimensional data in 2D or 3D. PCA requires some underlying advanced linear algebra such as the {\bf singular value decomposition} (SVD) which is not further explained in this document. {\bf Matlab} or {\bf Octave} offer functions to compute this. Data can  approximately be mapped back to the origin dimension ($z^{(i)}\in \mathbb{R}^K$ to $x_{approx}^{(i)} \in \mathbb{R}^n$):
\begin{align*}
x_{approx}^{(i)} = U_{reduce}*z^{(i)}
\end{align*}

\begin{definition}[Average squared projection error]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2$
\end{definition}

\begin{definition}[Total variation in data]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2$
\end{definition}

\begin{algorithm}
\caption{Principal component analysis}
\label{ref:pca}
\begin{algorithmic}
\State $x_j^{(i)} \gets \frac{x_j^{(i)} - \mu_j}{\sigma_j} $ \Comment{Mean normalization and feature scaling}
\State $\Sigma \gets \frac{1}{m}X^TX$ \Comment{Covariance matrix}
\State $[U,S,V] \gets svd(\Sigma)$ \Comment{Singular value decomposition}
\State $U_{reduce} \gets$ first $K$ columns of $U$ \Comment{Number $K$ of {\bf principal components}}
\State $z^{(i)} \gets U_{reduce}^Tx^{(i)}$ \Comment{Maps data $x^{(i)} \in \mathbb{R}^n$ to $z^{(i)}\in \mathbb{R}^K$}
\end{algorithmic}
\end{algorithm}

Choosing the number of principal components $K$ comes with a loss of information. Usually, a fixed percentage such as 99\% or 95\% of the {\bf variance} must be {\bf retained}:
\begin{align*}
\frac{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2}{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2} \le 0.01 \mbox(1\%)
\end{align*}
This measure is more useful than the actual amount of dimensions that were reduced. Then, the smallest $K$ to satisfy this equation is chosen. Computing $K$ this way is inefficient as PCA may need to be performed a lot times to choose the smallest $K$. Alternatively, the retained variance can be computed for a given $K$:
\begin{align*}
\frac{\sum_{i=1}^{K}S_{ii}}{\sum_{i=1}^{n}S_{ii}} \ge 0.99 \mbox(99\%)
\end{align*}

Usually, the large variances occur in the first $K<n$ principal components and then drops off. Therefore, the most relevant dynamics occur only in the first $K$ dimensions.


\section{Anomaly detection}
{\bf Anomaly detection} allows to find data that does not conform to an expected pattern. The cluster-based Algorithm~\ref{ref:anomalydetection} assumes {\bf independence} between selected features which are representative for anomaly detection. These features take unusually large or small values in the event of an anomaly and have a Gaussian distribution:
\begin{align*}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
\end{align*}

Non-Gaussian features can be approximated as Gaussian through operations such as $log(x), \sqrt{x}, x^{1/3}$ etc.

\begin{algorithm}
\caption{Anomaly detection}
\label{ref:anomalydetection}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu_j \gets \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}$ \Comment{Fits parameters $\mu_1,...,\mu_n,\sigma_1,...,\sigma_n$}
\State $\sigma_j^2 \gets \frac{1}{m}\sum_{i=1}^{m}(x_j^{(i)}-\mu_j)^2$
\State For new example $x$: $p(x) \gets \prod_{j=1}^np(x_j,\mu_j,\sigma_j^2)$ \Comment{Assumes independence between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

A good way to evaluate this algorithm is to use it on some labeled data of anomalous and non-anomalous ($y=0$ if normal, $y=1$ if anomalous). The training set only contains non-anomalous examples whereas the cross validation and test sets contain also a few anomalous examples. On a cross validation or test example $x$, predict
\begin{align*}
y = \left\{
\begin{array}{lll}
1  & \mbox{if } p(x) < \epsilon \mbox{ anomaly} \\
0  & \mbox{if } p(x) \ge \epsilon \mbox{ normal}
\end{array}
\right.
\end{align*}

Then, evaluation metrics such as precision/recall and $F_1$ score can be applied to the result. The cross validation set can also be used to choose $\epsilon$.

\subsection{Anomaly detection vs. supervised learning}
{\bf Anomaly detection} is used for a very small number of positive examples and large number of negative exempts. It is also used for many different kinds of anomalies as it is hard for any algorithm to learn from just a few positive examples what the anomalies might look like. There may be also future anomalies which may look completely different to any of the anomalous examples learned so far. {\bf Supervised learning} is used for large numbers of both positive and negative examples. It is also used when there are enough positive examples so that the algorithm can get a sense of what positive examples look like and future positive examples are likely to be similar to the ones in the training set.

\subsection{Correlated features}
Algorithm~\ref{ref:anomalydetection} assumes independence between the features for which the contours are along the axes. If features are {\bf correlated}, the algorithm performs poorly as the contours should rather be along the correlations. In such a case, some anomalies are incorrectly classified as normal. One could add new features to represent this correlation (e.g. $x_{extra}=\frac{x_1}{x_2}$) which requires manual effort but is computationally cheap. \\

Alternatively, {\bf multivariate Gaussian distribution} helps:

\begin{align*}
p(x)=\frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
\end{align*}

where $\vert \Sigma \vert$ is the determinant of the {\bf covariance matrix} $\Sigma$. Algorithm~\ref{ref:anomalydetectionmulti} is a {\bf anomaly detection with multivariate Gaussian} which is a generalization of Algorithm~\ref{ref:anomalydetection}.

\begin{algorithm}
\caption{Anomaly detection with multivariate Gaussian}
\label{ref:anomalydetectionmulti}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu \gets \frac{1}{m}\sum_{i=1}^m x^{(i)}$
\State $\Sigma \gets \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$
\State For new example $x$: $p(x) \gets \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))$ \Comment{Assumes correlation between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

This algorithm automatically captures correlations between features. It is computationally more expensive and requires $m>n$ to invert $\Sigma$. There may be further problems that prevent inversion such as linear dependency.



\chapter{Reinforcement learning}
Supervised learning algorithms approximate a mapping of their inputs to the labels $y$ of the training set. These labels give an unambiguous "right answer" fear of the inputs $x$. In many problems, it is difficult to provide such an explicit supervision to a learning problem. In {\bf reinforcement learning}, the algorithm is only provided a reward (feedback) function which provides a reward or penalty depending on in which state the learning agent goes. The {\bf credit assignment problem} is to figure out after getting a reward what an algorithm did right or wrong to get this reward. For example, a chess algorithm may lose after 30 steps and only gets then a penalty. This makes it hard to solve the credit assignment problem as the cause of the loss might have happened many steps before.

\section{Markov decision processes}
{\bf Markov decision processes} (MDP) provide a formalism in which reinforcement learning problems are usually posed. A MDP is a tuple $(S,A,\{P_{sa}\},\gamma, R)$, where:
\begin{itemize}
\item $S$ is a set of states
\item $A$ is a set of actions
\item $P_{sa}$ are the state transition probabilities which is a distribution over the state space for each state $s\in S$ and action $a\in A$. $P_{sa}(s')$ is the probability of $s'$ being the subsequent state of $s$ given action $a$. $\sum_{s'}P_{sa}(s')=1, P_{sa}(s')\ge 0$ .
\item $\gamma \in [0,1)$ is called the discount factor
\item $R:S \mapsto \mathbb{R}$ is the reward function
\end{itemize}

The dynamics of an MDP proceeds as follows: Start in some state $s_0$ and choose an action $a_0\in A$. The MDP then randomly transitions to some successor state $s_1\sim P_{s_0a_0}$. Then choose another action $a_1$ and randomly transition to $s_2\sim P_{s_1a_1}$: $s_0\xrightarrow{a_0}s_1\xrightarrow{a_1}s_2\xrightarrow{a_2}a_3\xrightarrow{a_3}...$. The total {\bf payoff} is: $R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+...$. This means that later rewards is given less weight than earlier rewards, called {\bf discounting}. The {\bf goal} of reinforcement learning is to maximize:
\begin{align*}
E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...]
\end{align*}
The goal of MDP is to compute a {\bf policy} $\pi:S\mapsto A$ which is a mapping from state to actions. The {\bf value function} of a policy $\pi$ is:
\begin{align*}
V^{\pi}(s) = E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma (R(s_1)+\gamma R(s_2)+...)\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma (V^{\pi}(s_1))\vert s_0=s,\pi]
\end{align*}
$R(s_o)$ is called the {\bf immediate reward} and $\gamma (V^{\pi}(s_1))$ the {\bf future rewards}. More precisely, the {\bf value function} can be defined recursively:
\begin{align*}
V^{\pi}(s) = R(s)+\gamma \sum_{s'\in S} P_{s\pi(s)}(s')V^{\pi}(s')
\end{align*}


The {\bf optimal value function} is:
\begin{align*}
V^{*}(s) = \max\limits_{\pi} V^{\pi}(s)
\end{align*}
\begin{align*}
V^{*}(s) = R(s)+\max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

The {\bf optimal policy} is:
\begin{align*}
\pi^{*}(s) = \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

One way to get $\pi^{*}$ is to compute $V^{*}$ and to plug it into the definition of $\pi^{*}$. As there is an exponential amount of policies $(\vert A\vert^{\vert S \vert})$, this is impractical.

\section{Value iteration and policy iteration}
{\bf Value iteration} is an efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:valueiterationalgorithm}. Eventually $V$ converges to $V^{*}$. Then $\pi^{*}$ can be computed using the previous equation.

\begin{algorithm}[h!]
\caption{Value iteration}
\label{ref:valueiterationalgorithm}
\begin{algorithmic}
\State $\forall s\in S: V(s) \gets 0$
\Repeat
\State $\forall s\in S:$ update $V(s) \gets R(s) + \max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{conversion}
\end{algorithmic}
\end{algorithm}

{\bf Value iteration} is another efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:policyiterationalgorithm}. Eventually $V$ and $\pi$ converge to $V^{*}$ and $\pi^{*}$ respectively.

\begin{algorithm}[h!]
\caption{Policy iteration}
\label{ref:policyiterationalgorithm}
\begin{algorithmic}
\State Initialize $\pi$ randomly
\Repeat
\State $V\gets V^{\pi}$
\State $\forall s\in S:\:\pi(s)\gets  \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{conversion}
\end{algorithmic}
\end{algorithm}

Policy iteration is computationally expensive for large MDPs because of the $V\gets V^{\pi}$ step which solves a set of $\vert S\vert$ linear equations in $\vert S\vert$ variables. Nonetheless, it is very fast and converges quickly for small MDPs. In practice, value iteration is used more often.


\section{Example}
This example is to apply the definitions on a small problem. The sample world in Figure~\ref{ref:sampleworld} contains a positive reward in cell $(4, 3)$ and a penalty in $(4, 2)$. It has 11 states and $A = \{N, S, E, W\}$.
\begin{figure}[h!]
\centering
\begin{tabular}{c|c|c|c|c|}
\hline
3 & & & & +1 \\
\hline
2 & & \cellcolor{black} & & -1 \\
\hline
1 & & & & \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\caption{Sample world}
\label{ref:sampleworld}
\end{figure}

Usually, a small penalty is assigned to all other states:
\begin{align*}
R(S)=
\left\{
\begin{array}{ll}
+1 & \mbox{if } S = (4,3) \\
-1  & \mbox{if } S = (4,2) \\
-0.02 & \mbox{otherwise}
\end{array}
\right.
\end{align*}

The state transition probability is defined as follows for all valid movements (invalid movements are 0): the intended outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles to the intended direction (0.1 to the left and 01. to to right if intended north).

The optimal policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy}. The $V*$ values were computed through value iteration. The action of $(3, 1)$ is not obvious. If it was north, there would be a 0.1 chance to go to -1 from $(3, 2)$. Therefore, it is left to route away from this potential action.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\uparrow$ & \cellcolor{black} & $\uparrow$ & -1 \\
\hline
1 & $\uparrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .86 & .90 & .93 & +1 \\
\hline
2 & .82 &\cellcolor{black} & .69 & -1 \\
\hline
1 & .78 & .75 & .71 & .49 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Optimal policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy}
\end{figure}

A bad policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy1}. A sample value function definition part is:
\begin{align*}
V^{\pi}((3,1))=R((3,1)) + \gamma[0.8V^{\pi}((3,2))+0.1V^{\pi}((4,1))+0.8V^{\pi}((2,1))]
\end{align*}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\downarrow$ & \cellcolor{black} & $\rightarrow$ & -1 \\
\hline
1 & $\rightarrow$ & $\rightarrow$ & $\uparrow$ & $\uparrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .52 & .73 & .77 & +1 \\
\hline
2 & -.90 &\cellcolor{black} & -.82 & -1 \\
\hline
1 & -.88 & -.87 & -.85 & -1.00 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Bad policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy1}
\end{figure}


\chapter{Localization}
\section{Grid localization}
\section{Kalman filter}
\section{Particle filter}


\end{document}