\documentclass{report}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}
\usepackage{xcolor,colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[chapter]{algorithm}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
showstringspaces=false            %
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\setlength{\parindent}{0cm}

\title{Artificial Intelligence Notes}
\author{Patrick Oliver GLAUNER \\
	\texttt{patrick.oliver.glauner@gmail.com}}

\date{\today}

\newtheorem{definition}{Definition}[section]

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
{\em Artificial Intelligence (AI)} is a versatile field and there are many different definitions for it. My favorite definition is:
\begin{flushright}
{\em "AI is the science of knowing what to do when you don't know what to do." (Peter Norvig)}\footnote{\url{https://www.youtube.com/watch?v=rtmQ3xlt-4A4m45}}
\end{flushright}
~\\
This document was created during the preparation for my master's studies at Imperial College London\footnote{\url{http://www3.imperial.ac.uk/computing/teaching/mcsml}}. It aggregates definitions and findings that are related to AI with a strong focus on {\em Machine Learning}.
~\\~\\
Most of the content is based on:
\begin{itemize}
\item Andrew Ng. {\em Machine Learning}. Coursera.\footnote{\url{https://www.coursera.org/course/ml}}
\item Andrew Ng. {\em CS229 Machine Learning}. Stanford University.\footnote{\url{http://cs229.stanford.edu/}}
\item Sebastian Thrun. {\em CS373 Artificial Intelligence for Robotics}. Udacity.\footnote{\url{https://www.udacity.com/course/cs373}}
\item Sebastian Thrun and Peter Norvig. {\em CS271 Intro to Artificial Intelligence}. Udacity.\footnote{\url{https://www.udacity.com/course/cs271}}
\item Stuart Russell and Peter Norvig. {\em Artificial Intelligence: A Modern Approach}. Third edition.\footnote{\url{http://aima.cs.berkeley.edu/}}
\end{itemize}

With reference to the purpose of this document, it does not contain original research. In contrast, it contains various statements that were copied directly from other sources.

~\\~\\~\\~\\
\begin{flushright}
Patrick GLAUNER
\end{flushright}

\end{abstract}


\chapter{General}
\section{Terminology}
\subsection{Intelligent agents}
An {\bf agent} operates autonomously. A {\bf rational agent} acts to achieve the best outcome or expected outcome if there is uncertainty. An {\bf agent program} implements the {\bf agent function} which maps perceptions to actions.

\subsection{Task environments}
{\bf Single vs. multi agent}: the agent is the only one. {\bf Fully observable} vs. {\bf partially observable}: the agent's sensors perceive the complete state of the relevant environment at each point in time. {\bf Deterministic} vs. {\bf stochastic}: the agent's state and action uniquely determine the next state. {\bf Discrete} vs. {\bf continuous}: finite amount of states, actions and outcomes. {\bf Benign} vs. {\bf adversarial}: there is no opponent. {\bf Known vs. unknown}: the agent's knowledge about the "law of physics" of the environment, the outcomes (or probabilities) for all actions are given.


\chapter{Mathematics}
This chapter summarizes and explains mathematical foundations which are relevant to machine learning.

\section{Probabilities}
\subsection{Introduction}

\begin{definition}[Complement]
$P(\neg A) = 1 - P(A)$
\end{definition}

\begin{definition}[Inclusion-exclusion principle]
$P(A\vee B) =P(A) + P(B) - P(A,B)$
\end{definition}


\subsection{Independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {A};
\node[mynode, right=1.5cm of A] (B) {B};
\end{tikzpicture}
\caption{Independence}
\label{ref:independence}
\end{figure}

As seen in Figure~\ref{ref:independence}, $A$ and $B$ are independent:
\begin{definition}
P(A,B) = P(A)P(B)
\end{definition}

\begin{definition}
$P(A\vert B) = P(A)$ or $P(B\vert A) = P(B)$ 
\end{definition}


\subsection{Conditional independence}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,below left=1cm of C] (A) {A};
\node[mynode,below right=1cm of C] (B) {B};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence1}
\end{figure}


As seen in Figure~\ref{ref:condindependence1}, $A$ and $B$ are conditionally independent, given $C$:
\begin{definition}
$P(A,B\vert C) = P(A\vert C)P(B\vert C)$
\end{definition}

\begin{definition}
$P(A\vert B,C) = P(A\vert C)$ and $P(B\vert A,C) = P(B\vert C)$
\end{definition}

$A\independent B \vert C \neq A\independent B$
\\

As seen in Figure~\ref{ref:condindependence2}, $A\independent B$ when $C$ is unknown. When $C$ is known, $A$ and $B$ are dependent. Therefore, independence does not imply conditional independence.


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\caption{Conditional independence}
\label{ref:condindependence2}
\end{figure}


\subsection{Conditional probabilities}
\begin{definition}[Complement]
$P(\neg A\vert B) = 1 - P(A\vert B)$
\end{definition}

For $a$ and $b$ to be true, $b$ needs to be true and $a$ needs to be true given $b$:
\begin{definition}[Product rule]
$P(A,B) =P(A\vert B)P(B)$
\end{definition}

\subsection{Total probability}
\begin{definition}
$P(A) = \sum_{b\in B}{P(A,b)} = \sum_{b\in B}{P(A\vert b)P(b)}$
\end{definition}

\begin{definition}[For conditional variables]
$P(A\vert C) = \sum_{b\in B}{P(A\vert C,b)P(b\vert C)}$
\end{definition}

\subsection{Bayes' rule}
Applying product rule and total probability:
\begin{definition}
$P(A\vert B) = \frac{P(B\vert A)P(A)}{\sum_{a\in A}{P(B\vert a)P(a)}} = \frac{P(B\vert A)P(A)}{P(B)}$
\end{definition}

The terminology is: $Posterior = \frac{Likelihood\times Prior}{Normalizer}$\\

The normalizer might be difficult to calculate. It can be substituted with pseudo probabilities:
\begin{enumerate}
\item $P'(A\vert B) = P(B\vert A)P(A)$ and $P'(\neg A\vert B) = P(B\vert \neg A)P(\neg A)$
\item $P(A\vert B) = \alpha P'(A\vert B)$ and $P(\neg A\vert B) = \alpha P'(\neg A\vert B)$ with $\alpha = \frac{1}{P'(A\vert B) + P'(\neg A\vert B)}$
\end{enumerate}

\begin{definition}[General Bayes' rule]
$P(A\vert B,e) = \frac{P(B\vert A,e)P(A\vert e)}{P(B\vert e)}$
\end{definition}


\subsection{Bayes networks}
Bayes networks define probability distributions over random variables and allow compact specification of full joint distributions. The joint probability of the network shown in Figure~\ref{ref:samplenetwork} is: $P(A,B,C,D,E)=P(A)(B)P(C\vert A,B)P(D\vert C)P(E\vert C)$. The general equation is:
\begin{definition}
$P(x_1,...,x_n) = \prod_{i=1}^{n}{P(x_i\vert parents(X_i))}$
\end{definition}


\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=1cm of C] (A) {A};
\node[mynode,above right=1cm of C] (B) {B};
\node[mynode,below left=1cm of C] (D) {D};
\node[mynode,below right=1cm of C] (E) {E};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D)
(C) edge[-latex] (E);
\end{tikzpicture}
\caption{Sample Bayes network}
\label{ref:samplenetwork}
\end{figure}

\subsection{d-separation}
Stands for direction-dependent separation. d-separated variables are independent. $X$ and $Y$ are d-separated if there is no active path between them. Paths consists of {\bf triplets} as defined in Table~\ref{ref:triplets}. An inactive triplet makes an entire path inactive.

\begin{table}[h!]
\begin{center}
\begin{tabular}{c|c}
Active & Inactive \\
\hline
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
causal chain
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A) {};
\node[mynode, right=0.5cm of A, fill=black] (B) {};
\node[mynode, right=0.5cm of B] (C) {};
\path (A) edge[-latex] (B)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
common cause
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode, fill=black] (C) {};
\node[mynode,below left=0.5cm of C] (A) {};
\node[mynode,below right=0.5cm of C] (B) {};
\path (C) edge[-latex] (A)
(C) edge[-latex] (B);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode,fill=black] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
common effect
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C);
\end{tikzpicture}
\\
\hline
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {};
\node[mynode,above left=0.5cm of C] (A) {};
\node[mynode,above right=0.5cm of C] (B) {};
\node[mynode,below=0.5cm of C,color=white] (D) {};
\node[mynode,below=0cm of D, fill=black] (E) {};
\path (A) edge[-latex] (C)
(B) edge[-latex] (C)
(C) edge[-latex] (D);
\end{tikzpicture}
&
\\
\end{tabular}
\end{center}
\caption{Active and inactive triplets with known variables filled}
\label{ref:triplets}
\end{table}

\section{Linear algebra}
\subsection{Eigenvalues and eigenvectors}
Let $A$ be an $n\times n$ matrix. The number $\lambda$ is an {\bf eigenvalue} of $A$ if there exists a non-zero vector $x$ such that:
\begin{align*}
Ax = \lambda x
\end{align*}
In this case, vector $x$ is called an {\bf eigenvector} of $A$ corresponding to $\lambda$. In analytic geometry, for example, a three-element vector may be seen as an arrow in three-dimensional space starting at the origin. In that case, an eigenvector $v$ is an arrow whose direction is either preserved or exactly reversed after multiplication by $A$ The corresponding eigenvalue determines how the length of the arrow is changed by the operation, and whether its direction is reversed or not, determined by whether the eigenvalue is negative or positive. The condition can be rewritten:
\begin{align*}
(A -\lambda I)x = 0
\end{align*}
Where $I$ is the $n\times n$ identity matrix. In order for a non-zero vector $x$ to satisfy this equation, $A -\lambda I$ must not be invertible. That is, the determinant of $A -\lambda I$ must be equal $0$. $p(\lambda)=\mbox{det}(A -\lambda I)$. Thus, $\lambda = [\lambda_1, ..., \lambda_n]^T$ are the eigenvalues of $A$. To find eigenvectors $x = [x_1,...,x_n]^T$ corresponding to an eigenvalue $\lambda$, the system of linear equations needs to be solved:
\begin{align*}
(A -\lambda I)x = 0
\end{align*}

Example:
$A = \begin{pmatrix}
2 & -4 \\
-1 & -1
\end{pmatrix}$. Then $p(\lambda)=\mbox{det}\begin{pmatrix}
2 - \lambda & -4 \\
-1 & -1 - \lambda
\end{pmatrix}$ \\
$= (2-\lambda)(-1-\lambda)-(-4)(-1)$ \\
$= \lambda^2 - \lambda - 6$ \\
$= (\lambda - 3)(\lambda + 2)$. \\
Thus, $\lambda_1=3$ and $\lambda_2=-2$. \\
To find the eigenvectors corresponding to $\lambda_1=3$, $x=[x_1, x_2]^T$, then $(A -3I)x = 0$ gives:\\
\begin{align*}
\begin{pmatrix}
2-3 & -4 \\
-1 & -1-3
\end{pmatrix} \begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} = \begin{pmatrix}
0 \\
0
\end{pmatrix}
\end{align*}
From which the duplicate equations can be obtained:
\begin{align*}
-x_1-4x_2 = 0 \\
-x_1-4x_2 = 0
\end{align*}
Therefore, all eigenvectors corresponding to $\lambda_1=3$ are multiples of $\begin{pmatrix}
-4 \\
1
\end{pmatrix}$ which is a {\bf basis} of the eigenspace corresponding to $\lambda_1=3$.\\
Repeating this process with $\lambda_2=-2$:
\begin{align*}
4x_1-4x_2 = 0 \\
-x_1+x_2 = 0
\end{align*}
Thus, an eigenvector corresponding to $\lambda_2=-2$ is: $\begin{pmatrix}
1 \\
1
\end{pmatrix}$ which is a basis of the eigenspace corresponding to $\lambda_2=-2$.



\section{Statistics}

\subsection{Foundations}
The {\bf mean} or {\bf expected value} is defined:
\begin{align*}
\mu = E[X] = \sum_i^m x_iP(x_i)
\end{align*}

For a data set, the mean is:
\begin{align*}
\mu = E[X] = \frac{1}{m}\sum_i^m x_i
\end{align*}

The {\bf variance} $\sigma^2$ measures how far a set of numbers is spread out:
\begin{align*}
\sigma^2 = E[(X-\mu)^2] = \frac{1}{m}\sum_i^m (x_i-\mu)^2
\end{align*}

The {\bf unbiased sample variance} is defined:
\begin{align*}
\sigma^2 = \frac{1}{m-1}\sum_i^m (x_i-\mu)^2
\end{align*}

The {\bf standard deviation} $\sigma$ shows how much variation or dispersion from the average exists:
\begin{align*}
\sigma = \sqrt{\sigma^2(X)}
\end{align*}

\subsection{Covariance}
{\bf Covariance} is a measure of how much two random variables change together:
\begin{align*}
Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = \frac{1}{m}\sum_i^m (x_i-\mu_x)(y_i-\mu_y)
\end{align*}
Variance is a special case of the covariance when the two variables are identical:
\begin{align*}
Cov(X,X) = \sigma^2(X)
\end{align*}

The {\bf covariance matrix} $\Sigma$ is a matrix whose element in the $i$, $j$ position is the covariance between the $i$th and the $j$th elements of a random vector:
\begin{align*}
\Sigma_{ij} = Cov(X_i, X_j)
\end{align*}
\begin{align*}
\Sigma = \frac{1}{m}\sum_i^m (x_i-\mu)^T(x_i-\mu)
\end{align*}

Example:
\begin{align*}
X = \begin{pmatrix}
3 & 8\\
4 & 7\\
5 & 5\\
6 & 3\\
7 & 2
\end{pmatrix}
\end{align*}

\begin{align*}
\mu_1 = 5, \mu_2 = 5
\end{align*}

\begin{align*}
\Sigma = \begin{pmatrix}
2 & -3.2\\
-3.2 & 5.2\\
\end{pmatrix}
\end{align*}


\subsection{Gaussian distribution}
{\bf Gaussian distribution} or {\bf normal distribution} is defined:
\begin{align*}
f(x\vert \mu,\sigma^2)= \frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(x_i-\mu)^2}{2\sigma^2})
\end{align*}
The area under the curve is equal to one. The {\bf 68-95-99.7 rule} states that 68\% of values drawn from a normal distribution are within one $\sigma$ away from the mean, 95\% and 99.7\% within two and three $\sigma$ respectively.
\\
The product of two Gaussians $\mu_1, \sigma_1^2, \mu_2, \sigma_2^2$ is proportional to a Gaussian:
\begin{align*}
\mu\prime = \frac{\sigma_2^2\mu_1 + \sigma_1^2\mu_2}{\sigma_1^2 + \sigma_2^2} \\
\sigma^{2}\prime = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{align*}


{\bf Multivariate Gaussian} distribution is defined:
\begin{align*}
f(x\vert \mu,\Sigma)=\frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}\mbox{exp}(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
\end{align*}

$\Sigma$ is the covariance matrix and $\mu$ is a vector of means.

\subsection{Estimation of mean and variance}
This section shows how to derive estimations for mean and variance using maximum likelihood estimation defined in Chapter~\ref{ref:estimationsection}.\\

The data $X$ are distributed {\bf i.i.d.} (independently and identically distributed). Therefore:
\begin{align*}
p(x_1...x_m\vert \mu,\sigma^2)=\prod_i^m f(x_i\vert \mu,\sigma^2)=\prod_i^m (\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(x_i-\mu)^2}{2\sigma^2}))
\end{align*}
\begin{align*}
=(\frac{1}{2\pi\sigma^2})^{\frac{m}{2}}\mbox{exp}(-\frac{\sum_i(x_i-\mu)^2}{2\sigma^2})
\end{align*}
Applying log:
\begin{align*}
g = \frac{m}{2}\mbox{log }(\frac{1}{2\pi\sigma^2})-\frac{1}{2\sigma^2}\sum_i^m(x_i-\mu)^2
\end{align*}

The mean estimation can be derived:
\begin{align*}
\frac{\partial g}{\partial \mu} = \frac{1}{\sigma^2}\sum_i^m (x_i - y) = 0
\end{align*}
\begin{align*}
m\mu = \sum_i^m x_i \implies \mu = \frac{1}{m}\sum_i^m x_i
\end{align*}

The variance estimation can be derived:
\begin{align*}
\frac{\partial g}{\partial \sigma} = -\frac{m}{\sigma}+\frac{1}{\sigma^3}\sum_i^m (x_i - y)^2 = 0
\end{align*}
\begin{align*}
\sigma^2 = \frac{1}{m}\sum_i^m (x_i - y)^2
\end{align*}



\chapter{Problem Solving}
\section{Introduction}
A {\bf problem solving agent} requires a deterministic and fully observable environment. Please see Chapter~\ref{ref:chapterplanning} for planning in more complex environments. Table~\ref{ref:search} compares general search algorithms.

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c|c}
 & Breadth-first & Uniform-cost & Depth-first\\
\hline
\hline
Expansion order
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {3};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {4};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {5};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {4};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {2};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {7};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {6};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {5};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {3};
\path (A) edge[-latex] node[left] {5} (B)
(A) edge[-latex] node[right] {2} (C)
(B) edge[-latex] node[left] {3} (D)
(B) edge[-latex] node[right] {2} (E)
(C) edge[-latex] node[left] {4} (F)
(C) edge[-latex] node[right] {2} (G);
\end{tikzpicture}
&
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center},
  font=\small
]
\node[mynode] (A) {1};
\node[mynode,below left=0.5cm and 0.4 of A] (B) {2};
\node[mynode,below right=0.5cm and 0.4 of A] (C) {5};
\node[mynode,below left=0.5cm and 0.05 of B] (D) {3};
\node[mynode,below right=0.5cm and 0.05 of B] (E) {4};
\node[mynode,below left=0.5cm and 0.05 of C] (F) {6};
\node[mynode,below right=0.5cm and 0.05 of C] (G) {7};
\path (A) edge[-latex] (B)
(A) edge[-latex] (C)
(B) edge[-latex] (D)
(B) edge[-latex] (E)
(C) edge[-latex] (F)
(C) edge[-latex] (G);
\end{tikzpicture}
\\
\hline
Expansion strategy & shallowest & cheapest & deepest\\
\hline
Optimal & yes & yes & no\\
\hline
Complete & yes & yes & no\\
\hline
Frontier size ($n$ levels) & $2n$ & $2n$ & $n$\\
\end{tabular}
\end{center}
\caption{Comparison of search algorithms}
\label{ref:search}
\end{table}

\section{A* search}
$A*$ expands the path with that has the minimum value $f$.

\begin{definition}[A* cost function]
$f = g + h$ with $g(path) =$ current path cost and $h(path) = h(s) =$ estimated distance to goal
\end{definition}

$f$ is a {\bf heuristic function}. $A*$ finds lowest cost path if: $h(s) <$ true cost. Subsequently, $h$ never overestimates and is optimistic/admissible. See Figure~\ref{ref:heuristic} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
S &\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
9&8&7&6&5&4 \\
\hline
8&7&6&5&4&3 \\
\hline
7&6&5&4&3&2 \\
\hline
6&5&4&3&2&1 \\
\hline
5&4&3&2&1&0 \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and heuristic}
\label{ref:heuristic}
\end{figure}


\section{Policy}
A {\bf policy} specifies what an agent should do for any state that the agent might reach. An {\bf optimal policy} is a policy that yields the highest expected {\bf utility} (meaning "the quality of being useful"). See Figure~\ref{ref:policy} for an example.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&\cellcolor{black}&&&& \\
\hline
&&&&\cellcolor{black}&G \\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\downarrow$&&$\rightarrow$&$\rightarrow$&$\rightarrow$&$\downarrow$ \\
\hline
$\rightarrow$&$\rightarrow$&$\uparrow$&$\uparrow$&&* \\
\hline
\end{tabular}
\end{subfigure}
\caption{Sample world and policy}
\label{ref:policy}
\end{figure}


\chapter{Logic}
\section{Introduction}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c}
& World & Beliefs \\
\hline
\hline
Probability theory & facts & $[0,1]$ \\
\hline
Propositional logic & facts & true, false, unknown \\
\hline
First-order logic & relations, objects, functions & true, false, unknown \\
\end{tabular}
\end{center}
\caption{Comparison of formal languages}
\label{ref:complang}
\end{table}


\section{Propositional logic}
Equivalent to Boolean Algebra. There are efficient inference algorithms to determine validity and satisfiability. Limitations:
\begin{itemize}
  \item It can only handle true and false values
  \item No capability to handle uncertainty
  \item No objects
  \item No shortcuts to express similar properties, only like: $A_1 \wedge A_2 \wedge ... \wedge A_n$
\end{itemize}


\section{First-order logic}
First-order logic is sufficiently expressive to represent a reasonable amount of our commonsense knowledge. It is also used in many further representation languages and has been studied intensively for many decades. The first-order logic syntax is defined in Figure~\ref{ref:firstordersyntax}.

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$Sentence$ & $\rightarrow$ & $AtomicSentence$ $\vert$ $ComplexSentence$ \\
$AtomicSentence$ & $\rightarrow$ & $Predicate$ $\vert$ $Predicate(Term,...)$ $\vert$ $Term=Term$ \\
$ComplexSentence$ & $\rightarrow$ & $(Sentence)$ $\vert$ $[Sentence]$ \\
 & $\vert$ & $\neg Sentence$ \\
 & $\vert$ & $Sentence \wedge Sentence$ \\
 & $\vert$ & $Sentence \vee Sentence$ \\
 & $\vert$ & $Sentence \implies Sentence$ \\
 & $\vert$ & $Sentence \iff Sentence$ \\
 & $\vert$ & $Quantifier$ $Variable, ...$ $Sentence$ \\
\\
$Term$ & $\rightarrow$ & $Function(Term,...)$ \\
 & $\vert$ & $Constant$ \\
 & $\vert$ & $Variable$ \\
 \\
$Quantifier$ & $\rightarrow$ & $\forall$ $\vert$ $\exists$ \\
$Constant$ & $\rightarrow$ & $A$ $\vert$ $X_1$ $\vert$ $John$ $\vert$ $...$ \\
$Variable$ & $\rightarrow$ & $a$ $\vert$ $x$ $\vert$ $s$ $\vert$ $..$ \\
$Predicate$ & $\rightarrow$ & $True$ $\vert$ $False$ $\vert$ $After$ $\vert$ $Loves$ $\vert$ $Raining$ $\vert$ $...$\\
$Function$ & $\rightarrow$ & $FullName$ $\vert$ $PhoneNumber$ $\vert$ $...$ \\
\\
$OPERATOR$ $PRECEDENCE$ & $:$ & $\neg,=,\wedge,\vee,\implies,\iff$ \\
\end{tabular}
\caption{First-order logic syntax}
\label{ref:firstordersyntax}
\end{figure}



\section{Higher-order logic}
Views relations and functions as objects themselves. Therefore, it allows more powerful expressions, e.g. relations on relations: $\forall_R$ $Transitive(R)\iff \forall a,b,c$ $R(a,b)\wedge R(b,c) \implies R(a,c)$


\chapter{Planning}
\label{ref:chapterplanning}
A {\bf planning agent} interleaves planning and execution. This is necessary for environments which are {\bf stochastic}, {\bf multi agent}, {\bf partially observable} or {\bf unknown}. Plans might be {\bf hierarchical} which consist of actions on different levels. Some of these levels can be planned ahead, others while executing the plan. Example: A* can find a route from A to Z but the route might change due to accidents. Low-level actions such as turning the steering wheel or pressing the pedals cannot be planned well-ahead in time in the real world.


\chapter{Learning}

{\bf Machine learning} is the field of study that gives computers the ability to learn without being explicitly programmed. This means that machine learning algorithms learn models from data. This is different to Bayes networks which reasons with known models.

What:
\begin{itemize}
  \item Parameters
  \item Structure
  \item Hidden concepts
\end{itemize}

What from:
\begin{itemize}
  \item Supervised
  \item Unsupervised
  \item Reinforcement
\end{itemize}

What for:
\begin{itemize}
  \item Prediction
  \item Diagnostics
  \item Summarization
  \item ...
\end{itemize}

How:
\begin{itemize}
  \item Passive: learning agent is just an observer and has no impact on the data itself
  \item Active
  \item Online: learning while data is being generated
  \item Offline: learning by processing data in batch
\end{itemize}

Outputs:
\begin{itemize}
  \item Classification: discrete values
  \item Regression: continuous
\end{itemize}

Details:
\begin{itemize}
  \item Generative: model data as generally as possible
  \item Descriminative: seek to distinguish data
\end{itemize}


\chapter{Supervised learning}
Supervised learning learns from {\bf labeled training examples}. If there are not sufficiently many features, the learning algorithm may {\bf underfit}. In contrast, it may {\bf overfit} for too many features meaning it only matches the training data well but fails to generalize to new examples. The vast majority of research in machine learning has been done in supervised learning.

\section{Linear regression}
{\bf Linear regression} computes a linear regression function for $m$ {\bf training examples} having $n$ {\bf features}. The objective is to minimize the cost function:
\begin{definition}[Cost function]
$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
\end{definition}

Where the hypothesis $h_\theta(x^{(i)})$ is given by the linear model:
\begin{definition}[Hypothesis]
$h_\theta(x^{(i)}) = \theta^{T}x = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n$ with $x_0 = 1$
\end{definition}

The parameters of the model to be learned are the $\theta_j$ values. One way to do this is to use the batch {\bf gradient descent} algorithm. Through is step, the parameters $\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\theta)$. If the {\bf learning rate} $\alpha$ is too large, gradient descent might not converge and overshoot the minimum after a certain amount of iterations. Gradient descent might generally not find the global but the local minimum. Gradient descent returns the global minimum for convex or bowl-shaped cost functions which do not have local optima. {\bf Feature normalization} allows to perform gradient descent quickly by normalizing each feature to the mean and scaling it to its standard deviation.


\begin{definition}[Gradient descent] ~\\
loop until converge \{ \\
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (simultaneously update $\theta_j$ for all $j$) \\
\}
\end{definition}

\begin{definition}[Feature normalization]
$x_j' = \frac{x_j - \mu_j}{\sigma_j}$
\end{definition}

{\bf Vectorization} allows to do the updates simultaneously through various matrix operations. Matrix multiplications can be executed very efficiently (Strassen algorithm). Matrix $X$ contains $m$ transposed training data columns and $n$ features where feature $x_0$ contains entirely 1s:
$X$ = $\begin{pmatrix}
\mbox{------} (x^{(1)})^T \mbox{------} \\
\mbox{------} (x^{(2)})^T \mbox{------} \\
... \\
\mbox{------} (x^{(m)})^T \mbox{------}
\end{pmatrix}$


The cost function and gradient descent then look:

\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{2m}(X\theta-y)^T(X\theta-y)$\\
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(h_{\theta}(x)-y) := \theta - \alpha \frac{1}{m}X^{T}(X\theta-y)$
\end{definition}

The {\bf normal equation} is a closed-form solution for linear regression which requires no loop, no learning rate and no feature normalization. It may be too slow for very large matrices as inversion is $O(n^3)$. $X$ may be singular and cannot be inverted. It is usually recommended to calculate the pseudo inverse.
\begin{definition}[Normal equation]
$\theta = (X^{T}X)^{-1}X^{T}y$
\end{definition}

\section{Logistic regression}

Logistic regression allows linear classification. The parameters $\theta_j$ comprise the linear separator called decision boundary. The hypothesis is defined as:
\begin{definition}[Hypothesis]
$h_\theta(x) = g(\theta^{T}x)$
\end{definition}

The hypothesis can be more complex to use logistic regression for non-linear classification. Once trained, logistic regression predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). $g$ is the {\bf sigmoid function} which is defined as:
\begin{definition}[Sigmoid function]
$g(z) = \frac{1}{1 + e^{-z}}$
\end{definition}

The cost function in logistic regression is convex:
\begin{definition}[Cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$
\end{definition}

\begin{definition}[Vectorized cost function] ~\\
$J(\theta) = \frac{1}{m}[-y^T log(g(X\theta))-(1-y)^T log(1-g(X\theta))]$
\end{definition}

This gradient descent looks identical to the linear regression gradient but the formula is actually different because of the different definition of $h_{\theta}(x)$.
\begin{definition}[Gradient descent]
$\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
\end{definition}

\begin{definition}[Vectorized gradient descent] ~\\
$\theta := \theta - \alpha \frac{1}{m}X^{T}(g(X\theta)-y)$
\end{definition}


\section{Regularization}
{\bf Regularization} allows to prevent overfitting by "penalizing" large $\theta_j$ values. Feature $x_0$ is usually not affected by regularization. If $\lambda$ is set too large, the learning algorithm may underfit. For linear regression, the definitions change to:

\begin{definition}[Regularized linear regression cost function] ~\\
$J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+ {\bf \lambda \sum_{j=1}^n\theta_j^2}]$
\end{definition}

\begin{definition}[Regularized linear regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} {\bf +\frac{\lambda}{m}\theta_j}]$
\end{definition}

Regularization also allows the normal equation to be invertible by adding a $(n+1)\times (n+1)$ matrix:
\begin{definition}[Regularized normal equation] ~\\
$\theta = (X^{T}X+\lambda
\begin{pmatrix}
0 & & & \\
&1& &\\
& & ... & \\
& & & 1
\end{pmatrix}
)^{-1}X^{T}y$
\end{definition}

For logistic regression, the definitions change to:

\begin{definition}[Regularized logistic regression cost function] ~\\
$J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))] {\bf + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2}$
\end{definition}

\begin{definition}[Regularized logistic regression gradient descent] ~\\
$\theta_j := \theta_j - \alpha[ \frac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ) {\bf + \frac{\lambda}{m}\theta_j}]$
\end{definition}


\section{Naive Bayes}
{\bf Bayes classifiers} rely on Bayes rule which maps a vector $X$ to a discrete random variable $C$ - a class. Training such a classifier is impractical because of the necessary exponential amount of training examples. A {\bf naive Bayes classifier} relaxes conditional dependency by assuming conditional independence between the parameters in $X$ given $C$. Naive Bayes is a powerful classifier that is often used in text classification (e.g. spam filtering) and is a dependable baseline when comparing different classifiers. Usually, it works on a so-called {\bf bag of words} which is a frequency of words in a text.
\begin{align*}
P(C\vert X) = P(C\vert x_1...x_n) = \frac{P(C)P(x_1...x_n\vert C)}{P(x_1...x_n)}
\end{align*}
\begin{align*}
= \frac{P(C)P(x_1\vert C)...P(x_n\vert C)}{P(x_1)...P(x_n)}
\end{align*}

To get the most likely class, the denominator can get dropped as it is independent of $C$.

\begin{align*}
C = \max\limits_{C} P(C)\prod_iP(x_i\vert C)
\end{align*}

\subsection{Using naive Bayes}
The probability:
\begin{align*}
P(x_i) = \frac{count(x_i)}{N}
\end{align*}
might be zero and would then reduce the entire product to zero. {\bf Laplacian smoothing} can deal with this overfitting problem:
\begin{align*}
P(x_i) = \frac{count(x_i)+k}{N+k\vert X\vert}
\end{align*}
It adds $k$ to the numerator and normalizes by adding $k$ to every single class of $X$. This change to the normalizer guarantees that $\sum_i P(x_i) = 1$.
Using a {\bf cross validation set} helps to find the best $k$, see Chapter~\ref{ref:modelselection}.
\\
\\
To prevent numerical underflow while multiplying lots of probabilities, naive Bayes can add the logarithms of these probabilities:
\begin{align*}
C = \max\limits_{C} log(P(C))+\sum_i log(P(x_i\vert C))
\end{align*}

\subsection{Spam classification example}
In the section, naive Bayes is demonstrated on a spam filter example. Bad messages are called "Spam", good messages are "ham". The sample messages are shown in Figure~\ref{ref:spamsample}.

\begin{figure}[h!]
\centering
\begin{tabular}{c||l|l}
& SPAM & HAM \\
\hline
\hline
1 & OFFER IS SECRET & PLAY SPORTS TODAY \\
2 & CLICK SECRET LINK & WENT PLAY SPORTS \\
3 & SECRET SPORTS LINK & SECRET SPORTS EVENT \\
4 & & SPORTS IS TODAY \\
5 & & SPORTS COSTS MONEY \\
\end{tabular}
\caption{Sample messages}
\label{ref:spamsample}
\end{figure}

The following non-smoothed ($K=0$) probabilities can then be computed which are based on the empirical count (can be derived by maximum likelihood estimation defined in Chapter~\ref{ref:estimationsection})):
\begin{itemize}
\item $P(SPAM) = 3/8$
\item $P("SECRET"\vert SPAM) = 1/3$
\item $P("SECRET"\vert HAM) = 1/15$
\item $P(SPAM\vert "SPORTS") = \frac{P("SPORTS"\vert SPAM)P(SPAM)}{P("SPORTS")}=\frac{1/9 * 3/8}{6/24}=3/18$
\item $P(SPAM\vert "SECRET\,IS\,SECRET")= \frac{P("SECRET\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("SECRET\:IS\:SECRET")}$\\
$=\frac{1/3*1/9*1/3*3/8}{1/3*1/9*1/3*3/8+1/15*1/15*1/15*5/8}=\frac{1/216}{1/216+1/5400}=\frac{25}{26}$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{0*1/9*1/3*3/8}{0*1/9*1/3*3/8+2/15*1/15*1/15*5/8}=0$
\end{itemize}

The last example demonstrates a clear overfit. The following smoothed ($K=1$) probabilities can be computed. The last examples how the model deals with with the nonexistence of "TODAY" in the spam messages.
\begin{itemize}
\item $P(SPAM) = \frac{3+{\bf 1}}{8+{\bf 2}} = 2/5$
\item $P(HAM) = \frac{5+{\bf 1}}{8+{\bf 2}} = 3/5$
\item $P("TODAY"\vert SPAM) = \frac{0+{\bf 1}}{9+{\bf 12}} = 1/21$
\item $P("TODAY"\vert HAM) = \frac{2+{\bf 1}}{15+{\bf 12}} = 1/9$
\item $P(SPAM\vert "TODAY\,IS\,SECRET")= \frac{P("TODAY\:IS\:SECRET"\vert SPAM)P(SPAM)}{P("TODAY\:IS\:SECRET")}$\\
$=\frac{1/21*2/21*4/21*2/5}{1/21*2/21*4/21*2/5+3/27*2/27*2/27*3/5}=0.4858$
\end{itemize}

%+ Limitations, but other factors can be added

\subsection{Comparison to logistic regression}
Naive Bayes requires only a small amount of data, converges quickly and is very fast. In comparison, logistic regression requires more data and computations are more expensive. Nonetheless, it can outperform naive Bayes on a lot of data.


\section{Maximum a posteriori and maximum likelihood}
\label{ref:estimationsection}
In non-technical language, "likelihood" is usually a synonym for "probability", but in statistical usage, a clear distinction is: the {\em probability} of some observed outcomes given a set of parameter values is referred to as the {\bf likelihood} of the set of parameter values given the observed outcomes. Therefore $P(\theta\vert X)$ can be interpreted as $L(X\vert\theta)$.

\subsection{Maximum a posteriori}
The Bayes {\bf maximum a posteriori} (MAP) estimate for $\theta$ is given by:
\begin{align*}
\theta_{MAP}=\max\limits_{\theta} \prod_{i=1}^m p(y^{(i)}\vert x^{(i)},\theta)p(\theta)
\end{align*}
The data are distributed {\bf i.i.d.} (independently and identically distributed). $\theta$ is treated as a random variable in Bayesian statistics. In practical applications, a common choice for the prior $p(\theta)$ is to assume that it has a normal distribution. Using this choice of prior, the fitted parameters $\theta$ have smaller norm than that selected by ML. In practice, this causes the MAP estimate to be less susceptible to overfitting than the ML estimate. MAP estimation can therefore be seen as a regularization of ML estimation.
\\
Bayesian classification turns out to be an effective algorithm for text classification, even though in text classification there can be significantly more features than training examples.


\subsection{Maximum likelihood}
{\bf Maximum likelihood} (ML) estimation is related to MAP but simpler:
\begin{align*}
\theta_{MAP}=\max\limits_{\theta} \prod_{i=1}^m p(y^{(i)}\vert x^{(i)};\theta)
\end{align*}
This definition is similar to the ML estimate for $\theta$, except for the prior $p(\theta)$ term at the end. 
Also, $\theta$ is treated as a fixed but unknown variable \footnote{Written $ p(y^{(i)}\vert x^{(i)};\theta)$ instead of $ p(y^{(i)}\vert x^{(i)},\theta)$}. For example, ML can be used to derive the least-squares cost function of linear regression assuming that the error terms $\epsilon^{(i)}$ are i.i.d. according to a normal distribution:
\begin{align*}
y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}
\end{align*}
The density of $\epsilon^{(i)}$ is given by:
\begin{align*}
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})
\end{align*}
This implies that:
\begin{align*}
p(y^{(i)}\vert x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
The likelihood of $\theta$ can then be written:
\begin{align*}
L(\theta)=\prod_{i=1}^m p(y^{(i)}\vert x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
Instead of maximizing $L(\theta)$, any strictly increasing function of $L(\theta)$ can be increased. Often the {\bf log likelihood} is maximized as it allows to change products to summations which are usually easier to maximize:
\begin{align*}
l(\theta)=\mbox{log } L(\theta)=\mbox{log }\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}
\begin{align*}
=\sum_{i=1}^m \mbox{log }[\frac{1}{\sqrt{2\pi}\sigma}\mbox{exp}(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})]=m\mbox{log }[\frac{1}{\sqrt{2\pi}\sigma}] -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
\end{align*}
Hence, maximizing $l(\theta)$ gives the same answer as {\bf minimizing}:
\begin{align*}
\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2
\end{align*}
which is the least-square cost function of linear regression.

\section{Neural network}
A neuron has {\bf inputs} and an {\bf output} as seen in Figure~\ref{ref:neuron}. A {\bf neural network} consists of layers of {\bf neurons}. Each layer has a {\bf bias term} of value 1. The parameters $\theta_j$ are called {\bf weights} in some sources. The hypothesis is called {\bf Sigmoid activation function}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (C) {C};
\node[mynode,above left=2cm and 1cm of C] (X0) {$1$};
\node[mynode,above left=1cm and 1cm of C] (X1) {$x_1$};
\node[mynode,above left=0cm and 1cm of C] (X2) {$x_2$};
\node[mynode,below left=0cm and 1cm of C,draw=white] (X3) {...};
\node[mynode,below left=1cm and 1cm of C] (XN) {$x_n$};
\node[mynode,right=1cm and 1cm of C,draw=white] (h) {$h_{\theta}(x)$};
\path (X0) edge[-latex] (C)
(X1) edge[-latex] (C)
(X2) edge[-latex] (C)
(X3) edge[-latex] (C)
(XN) edge[-latex] (C)
(C) edge[-latex] (h);
\end{tikzpicture}
\caption{Neuron with bias term $x_0=1$}
\label{ref:neuron}
\end{figure}

\begin{definition}[Sigmoid activation function]
$h_\theta(x) = \frac{1}{1+e^{-\theta^{T}x}}$
\end{definition}

Each neural network has an {\bf input layer}, $\ge 0$ {\bf hidden layers} and an {\bf output layer} as seen in Figure~\ref{ref:neuralnetwork}. The value $a_i^{(j)}$ is called {\bf activation} or {\bf forward propagation} of unit $i$ in layer $j$. $\Theta^{(j)}$ is a matrix of weights to map from layer $j$ to layer $j+1$. If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1}\times (s_j + 1)$. For the neural network in Figure~\ref{ref:neuralnetwork}, the mapping is defined in Figure~\ref{ref:neuralnetworkmapping}. Once trained, a neural network predicts $1$ (positive classification) if $h_\theta(x) \ge 0.5$ for a new example, otherwise $0$ (negative classification). \\

The cost function for $K$ output units and $L$ layers is defined as follows (with $h_\Theta(x) \in \mathbb{R}^K$ and $(h_\Theta(x))_i = i^{th}$ output):

\begin{definition}[Regularized cost function] ~\\
$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}log(h_{\Theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)}))_k] \\
+ \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}(\Theta_{ij}^{(l)})^2$
\end{definition}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,circle,align=center}
]
\node[mynode] (A12) {$a_1^{(2)}$};
\node[mynode,below=0.5cm of A12] (A22) {$a_2^{(2)}$};
\node[mynode,below=0.5cm of A22] (A32) {$a_3^{(2)}$};
\node[mynode,left=0cm and 1cm of A12] (X1) {$x_1$};
\node[mynode,left=0cm and 1cm of A22] (X2) {$x_2$};
\node[mynode,left=0cm and 1cm of A32] (X3) {$x_3$};
\node[mynode,right=0cm and 1cm of A22] (L3) {};
\node[mynode,right=0cm and 1cm of L3,draw=white] (h) {$h_{\Theta}(x)$};
\path (X1) edge[-latex] (A12)
(X1) edge[-latex] (A22)
(X1) edge[-latex] (A32)
(X2) edge[-latex] (A12)
(X2) edge[-latex] (A22)
(X2) edge[-latex] (A32)
(X3) edge[-latex] (A12)
(X3) edge[-latex] (A22)
(X3) edge[-latex] (A32)
(A12) edge[-latex] (L3)
(A22) edge[-latex] (L3)
(A32) edge[-latex] (L3)
(L3) edge[-latex] (h);
\end{tikzpicture}
\caption{Neural network with three layers (bias terms not displayed)}
\label{ref:neuralnetwork}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{lcl}
$a_1^{(2)}$ & $=$ & $g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)$ \\
$a_2^{(2)}$ & $=$ & $g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)$ \\
$a_3^{(2)}$ & $=$ & $g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)$ \\
$h_{\Theta}(x)$ & $=$ & $a_1^{(3)}$ $=$ $g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$ \\
\end{tabular}
\caption{Activation of layers in a neural network}
\label{ref:neuralnetworkmapping}
\end{figure}

\subsection{Backpropagation}
Training a neural network requires to $\min\limits_{\Theta}J(\Theta)$, e.g. through gradient descent which requires $J(\Theta)$ and $\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)$. {\bf Backpropagation} is a method to compute the gradient as described in Figure~\ref{ref:backpropagation}. This algorithm can also be vectorized. The derivative of the Sigmoid function is defined as follows:
\begin{definition}[Derivative of Sigmoid function] ~\\
$g^{'}(x)=g(x)*(1-g(x))$
\end{definition}

\begin{figure}[h!]
Training set $\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}$ \\
Set $\Delta_{ij}^{(l)}=0$ (for all $l,i,j$). \\
For $i=1$ to $m$
\begin{tabbing}
~~~~ \= Set $a^{(i)}=x^{(i)}$ \\
\> Perform forward propagation to compute $a^{(l)}$ for $l=2,3,...,L$ \\
\> Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$ ("error") \\
\> Compute $\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$, e.g. $\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}.*g^{'}(z^{(2)})$ (element-wise multiplication) \\
\> $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i{(l+1)}$ \\
$D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\frac{\lambda}{m}\Theta_{ij}^{(l)}$ if $j\ne0$ (no regularization of bias terms) \\
$D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}$ if $j=0$
\\
\\
$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$
\end{tabbing}
\caption{Backpropagation algorithm}
\label{ref:backpropagation}
\end{figure}

$\Theta_{ij}^{(l)}$ needs to have an initial value. If $\Theta$ was entirely 0, {\bf symmetry} would create a highly redundant neural network with poor learning capabilities. {\bf Random initialization} allows symmetry breaking by initializing each $\Theta_{ij}^{(l)}$ to a random value in $[-\epsilon,\epsilon]$.

\subsection{Gradient checking}
As backpropagation is a complex algorithm, subtile bugs might not be caught and gradient descent might not reach the intended global minimum but only a more expensive minimum. {\bf Gradient checking} allows to approximate the gradient and compare it to the gradient returned by backpropagation. The approximation is computed as follows for each feature $\theta_i$:
\begin{definition}[Gradient checking] ~\\
$\frac{\partial}{\partial\theta_i}J(\theta)\approx\frac{J(...,\theta_{i-1},\theta_{i}+\epsilon,\theta_{i+1},...)-J(...,\theta_{i-1},\theta_{i}-\epsilon,\theta_{i+1},...)}{2\epsilon}$ for small $\epsilon$
\end{definition}

\subsection{Picking a network architecture}
The following rules can be followed to decide on the network architecture:
\begin{itemize}
\item No. of input units: dimension of features $x^(i)$
\item No. output units: number of classes
\item No. of hidden layers: reasonable default: $1$ hidden layer, if $>1$, same number of units in every layer
\item No. of units per layer: usually the more the better (but also computationally more expensive)
\end{itemize}

\section{Model selection}
\label{ref:modelselection}
{\bf Model selection} is the task of selecting a statistical model from a set of candidate models, given data. For example, different hypotheses for linear regression. Usually, the data is split up in three sets: {\bf training set} (60\%), {\bf cross validation set} (20\%) and {\bf test set} (20\%). A a non-regularized error function can be computed for each of these sets, such as for the test set:

\begin{definition}[Linear regression error function] ~\\
$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$
\end{definition}

\begin{definition}[Logistic regression error function] ~\\
$J_{test}(\theta) = -\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}[y_{test}^{(i)}log(h_\theta(x_{test}^{(i)}))+(1-y_{test}^{(i)})log(h_\theta(x_{test}^{(i)}))]$
\end{definition}

Alternatively, the following error function might be easier to interpret for logistic regression:

\begin{definition}[0/1 misclassification error] ~\\
$err(h_\theta(x),y)=
\left\{
\begin{array}{lll}
1  & \mbox{if } h_\theta(x) \geq 0.5, & y=0 \\
  & \mbox{if } h_\theta(x) < 0.5, & y=1 \\
0 & otherwise
\end{array}
\right.$ \\
$J_{test}(\theta) = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y^{(i)})$
\end{definition}


First, the learning algorithm is trained by using the training data for each hypothesis resulting in $\Theta^{(n)}$ for each hypothesis. Second, model {k} with the lowest $J_{CV}(\Theta^{(k)})$ is chosen. Last, the {\bf generalization error} can be computed for the test set $J_{test}(\Theta^{(k)})$.
\\
\\
A similar approach can be done to find a good regularization parameter $\lambda$.

\section{Debugging a learning algorithm}
Implementing a supervised learning algorithm and testing it on new data might reveal unacceptably large errors in its predictions. This section is to give advice on what to do next. Implementing a {\bf diagnostic} might be time-consuming, but doing so can be a very good use of time to get directed into the right direction. In this context, underfitting is called {\bf bias} and overfitting {\bf variance}.

In non-regularized model selection, to diagnose if it is a bias problem or variance problem, the following guidelines may help:
\begin{itemize}
\item Bias: $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\item Variance: $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\end{itemize}

To determine if the regularization parameter $\lambda$ is the cause of bias and variance, the following guidelines may help for a function of $\lambda$:
\begin{itemize}
\item Variance (too small $\lambda$): $J_{train}(\theta)$ is low (fits training data well) and $J_{CV}(\theta)>>J_{train}(\theta)$ (high error in model selection)
\item Bias (too large $\lambda$): $J_{train}(\theta)$ is high and $J_{CV}(\theta)\approx J_{train}(\theta)$
\end{itemize}

{\bf Learning curves} can be used to plot a learning's algorithm error for more training examples. Usually, the training error increases as the learning algorithm's hypothesis fails to fit more data. Quite the opposite, the cross validation error decreases because the algorithm generalizes better. For high bias, both errors are high and converge quickly. Therefore, more training data will not help. For high variance, there is a large gap between the cross validation error and the training error. In this case, more data is likely to help.
\\ \\
"Small" {\bf neural networks} which have fewer parameters are prone to underfitting but computationally cheaper. "Large" neural networks have more parameters and are more prone to overfitting (can be addressed by regularization) but computationally more expensive.
\\ \\
In summary, the following actions can help to solve particular problems:
\begin{itemize}
\item Get more training examples: fixes high variance
\item Try smaller set of features: fixes high variance
\item Try getting additional features: fixes high bias
\item Try adding polynomial features: fixes high bias
\item Try decreasing $\lambda$: fixes high bias
\item Try increasing $\lambda$: fixes high variance
\end{itemize}

\section{Error analysis}
The recommended approach to develop a machine learning algorithm is to start with a simple algorithm that can be implemented quickly. Then, it can be tested a small set of data, e.g. a couple of hundreds of records. The cross-validation data can be used to plot the learning curves. This allows to decide if more data, more features etc. are likely to help. In the {\bf error analysis} the examples on which the algorithm made errors on are manually examined. This can help to spot any systematic trend in what type of examples it makes errors on, e.g. stemming, capitalization etc.
\\ \\
{\bf Precision} and {\bf recall} allow to describe classification errors mathematically as described in Figure~\ref{ref:classificationerror}. Precision and and recall can be changed by amending the threshold of the classifier such as the sigmoid function of logistic regression. Usually, a tradeoff needs to be made. The {\bf $F_1$ score} allows to compare precision and recall.

\begin{figure}[h!]
\centering
\begin{tabular}{r||ccc}
& & Actual class & \\
\hline
\hline
& & 1 & 0 \\
Predicted & 1 & True positive & False positive \\
class & 0 & False negative & True negative \\
\end{tabular}
\caption{Classification and errors}
\label{ref:classificationerror}
\end{figure}


\begin{definition}[Precision]
$\frac{True positive}{True positive + False positive}$
\end{definition}

\begin{definition}[Recall]
$\frac{True positive}{True positive + False negative}$
\end{definition}

\begin{definition}[$F_1$ score]
$2\frac{PR}{P+R}$
\end{definition}

\section{Support vector machines}
A {\bf support vector machine} (SVM) is a large margin linear classifier.
\begin{definition}[SVM hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tx \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

In order to get a large margin, the motivation for the cost function definition is:
\begin{itemize}
\item If $y=1$, we want $\theta^Tx\ge 1$ (not just $\ge 0$)
\item If $y=0$, we want $\theta^Tx\le -1$ (not just $\le 0$)
\end{itemize}

\begin{definition}[SVM cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\\
$C = \frac{1}{\lambda}$
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \ge 1 \\
-z + 1  & \mbox{else}
\end{array}
\right.$ \\
\\
$cost_1(z)=
\left\{
\begin{array}{lll}
0  & \mbox{if } z \le 1 \\
z + 1  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

SVMs can be used for non-linear classification. On the one hand, higher order polynomials can be used as in logistic regression which is relatively complex. On the other hand, SVMs can use the {\bf kernel trick} which implicitly maps inputs into high-dimensional feature spaces. In some sources, linear SVMs use a so-called "linear kernel".

\begin{definition}[Gaussian kernel]
$e^{-\frac{\lVert x - l^{(i)} \rVert^2}{2\sigma^2}}$
\end{definition}

Given $x$, new features $f$ depending on proximity to landmarks of the training set $l^{(1)}, l^{(2)}, ..., l^{(m)}$ can be computed:
\begin{itemize}
\item $f_0 = 1$
\item $f_1 = similarity(x,l^{(1)})=e^{-\frac{\lVert x - l^{(1)} \rVert^2}{2\sigma^2}}$
\item $f_2 = similarity(x,l^{(2)})=e^{-\frac{\lVert x - l^{(2)} \rVert^2}{2\sigma^2}}$
\item ...
\item $f_m = similarity(x,l^{(m)})=e^{-\frac{\lVert x - l^{(m)} \rVert^2}{2\sigma^2}}$
\end{itemize}

$f = [f_0\:f_1\:...\:f_m]^T$. $f_1^{(i)}=sim(x^{(i)},l^{(1)})$. $f^{(i)}$ is a vector of $sim(x^{(i)},l^{(k)})$ for $k \in [0,m]$. \\
If $x\approx l^{(i)}$, then $f_i\approx 1$. If $x$ far from $l^{(i)}$, then $f_i\approx 0$.

\begin{definition}[SVM kernel hypothesis]~\\
$h_{\theta}(x) =
\left\{
\begin{array}{lll}
1  & \mbox{if } \theta^Tf \ge 0 \\
0  & \mbox{else}
\end{array}
\right.$ \\
\end{definition}

\begin{definition}[SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$
\end{definition}

To optimize the training, the regularization part is slightly changed:
\begin{definition}[Optimized SVM kernel cost function]~\\
$J(\theta)=C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^{\bf m}\theta_j^2$
\end{definition}

Changing the SVM parameters $C(=\frac{1}{\lambda})$ and $\sigma^2$ has impact on bias an variance.  A large $C$ results in lower bias and higher variance whereas a small $C$ results in higher bias and lower variance. A large $\sigma^2$ results in higher bias and lower variance because the features $f_i$ vary more smoothly. In contrast, a small $\sigma^2$ results in lower bias and higher variance.

\section{Multiclass classification}
In {\bf multiclass classification} examples can be classified into more than two classes. Some classification algorithms permit to do this natively. Generally, this can be done using {\bf one-vs.-all} (one-vs.-rest) classification which trains one binary classifier for each class. Finally, on a new input to make a prediction, the largest classifier is picked.

\section{k-nearest neighbors}
{\bf Parametric models} such as linear and logistic regression require a constant size of parameters that is indecent of the number of training examples. They also allow to throw away the training examples once the parameters (e.g. $\theta$ of linear regression) of the hypothesis have been computed as they summarize by the parameters. In comparison, {\bf non-parametric models} such as {\bf k-nearest neighbors} (KNN) require a set of parameters that may grow if the size of the training set grows.
\\
KNN can be used for classification and regression. In classification, the class of an example is determined by selecting the majority class of the $K$ nearest training examples. In regression, the mean value of the $K$ nearest examples is chosen. Usually, nearest is interpreted geometrically by computing the norm.
\\
$K$ is a smoothing parameter. The larger $K$, the smoother the output.

\section{Applying PCA}
The {\bf principal component analysis} (PCA) presented in Section~\ref{ref:pcasection} can be used in supervised learning. It allows to map high-dimensional data (such as images or emails in spam filters) to lower dimensions to speed up a learning algorithm. The mapping $x^{(i)}\rightarrow z^{(i)}$ should be defined by running PCA only on the training set. This mapping can be applied as well to the examples $x^{(i)}_{cv}$ and $x^{(i)}_{test}$ in the cross validation and test sets. \\

PCA should only be used if the amount of initial dimensions significantly slows down the learning algorithm. PCA should not be used to prevent overfitting by reducing the amount of features. This might work but drops potentially relevant information. Regularization is a better way to address overfitting.


\section{Recommender systems}
{\bf Recommender systems} recommend items to users based on item ratings. There are $n_u$ number of users, $n_m$ number of items, $r(i,j)=1$ if user $j$ has rated item $i$ and $y^{(i,j)}$ is the rating given by user $j$. All ratings are on the same scale $[0,max]$, e.g. $max=5$.

\begin{figure}[h!]
\centering
\begin{tabular}{c||cccc||cc}
Item & $User_A$ & $User_B$ & $User_C$ & $User_D$ & $x_1\:(Feature_A)$ & $x_2\:(Feature_B)$ \\
\hline
\hline
$Item_A$ & 5 & 5 & 0 & 0 & 0.9 & 0 \\
$Item_B$ & 5 & ? & ? & 0 & 1.0 & 0.01 \\
$Item_C$ & ? & 4 & 0 & ? & 0.99 & 0 \\
$Item_D$ & 0 & 0 & 5 & 4 & 0.1 &1.0 \\
$Item_E$ & 0 & 0 & 5 & ? & 0 & 0.9 \\
\end{tabular}
\caption{Sample item ratings}
\label{ref:sampleitemratings}
\end{figure}

Figure~\ref{ref:sampleitemratings} contains a sample set of items, users and features. $n_u=4$, $n_m=5$, $x^{(1)}= [1\:0.9\:0]^T$ (as $x_0=1$). To predict rating of item $i$ of user $j$:
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})
\end{align*}

$i:r(i,j)$ stands for all items $i$ that user $j$ has rated. Vice versa, $j:r(i,j)$ stands for all users $j$ that rated item $i$. Collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$.

\begin{definition}[$\theta^{(1)},...,\theta^{(n_u)}$ estimation minimization objective]~\\
 $\min\limits_{\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[$x^{(1)},...,x^{(n_m)}$ estimation minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)}}\:\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2$
\end{definition}


Both definitions are very related to linear regression. In contrast, there is no $m$ in the denominators as it is in linear regression. This constant is irrelevant to the minimization objective. \\

$\theta^{(1)},...,\theta^{(n_u)}$ can be estimated given $x^{(1)},...,x^{(n_m)}$. Vice versa, $x^{(1)},...,x^{(n_m)}$ can be estimated given $\theta^{(1)},...,\theta^{(n_u)}$. This is a chicken or egg problem. One way to solve this problem is: guess $\theta \rightarrow x \rightarrow \theta \rightarrow x  \rightarrow \theta \rightarrow ...$. This algorithm is called {\bf collaborative filtering}. The term means that with every user rating items (collaboration), the algorithm learns better features. The algorithm is inefficient and can be improved in a way to compute $x^{(1)},...,x^{(n_m)}$ and $\theta^{(1)},...,\theta^{(n_u)}$ simultaneously as defined in Algorithm~\ref{ref:collaborativefiltering}.

\begin{definition}[Simultaneous collaborative filtering minimization objective]~\\
 $\min\limits_{x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}}\:\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$
\end{definition}

\begin{definition}[Simultaneous collaborative filtering gradient descent]~\\
$x_k^{(i)}:=x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$ \\
$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\end{definition}

\begin{algorithm}
\caption{Collaborative filtering}
\label{ref:collaborativefiltering}
\begin{algorithmic}
\State 1. Initialize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$ to small random values \Comment{Break symmetry}
\State 2. Minimize $x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$
\State $x_k^{(i)}\gets x_k^{(i)}-\alpha(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})$
\State $\theta_k^{(j)}\gets \theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)})$
\State 3. For a user with parameters $\theta$ and an item with (learned) features $x$, predict a rating
\State $y^{(i,j)}\gets (\theta^{(j)})^T(x^{(i)})$
\end{algorithmic}
\end{algorithm}

This algorithm needs to break symmetry by randomly initializing $x^{(1)},...,x^{(n_m)},$ $\theta^{(1)},...,\theta^{(n_u)}$ which is related to neural networks. As the algorithm does not iterate over $x_0$ or $\theta_0$, all elements are regularized. Alternatively, the last step of Algorithm~\ref{ref:collaborativefiltering}, can be vectorized:
\begin{align*}
Y = X\Theta^T
\end{align*}

This is also called {\bf low rank matrix factorization}.
\\
\\
As collaborative filtering learns a feature vector $x^{(i)}\in \mathbb{R}$ for each item $i$, the most related items $j$ to item $i$ can be found by taking the $j$ items with smallest $\lVert x^{(i)} - x^{(j)}\rVert$ as a small $\lVert x^{(i)} - x^{(j)}\rVert$ indicates "similarity".
\\
\\
In practice, some users might have not rated any items. In that case, all predictions for this user are $0$. {\bf Mean normalization} can be applied to matrix $Y$:
\begin{align*}
\mu_i = \frac{1}{m^{(i)}} \sum_{j:r(i,j)=1}y^{(i,j)}
\end{align*}
\begin{align*}
Y = Y - \mu
\end{align*}
\begin{align*}
y^{(i,j)}=(\theta^{(j)})^T(x^{(i)})+\mu_i
\end{align*}
Mean normalization allows to predict a mean value per item which is more meaningful than $0$. It only takes rated items into account with $m^{(i)}$ number users who rated item $i$. Feature scaling is not necessary as all ratings are already on the same scale.


\chapter{Unsupervised learning}
Unsupervised learning tries to find hidden structure in {\bf unlabeled data} $\{x^{(1)}, x^{(2)},$ $..., x^{(m)}\}$. In machine learning research, unsupervised learning has received less attention than supervised learning but is likely to receive more in the future because of its potential to work on unlabeled data.

\section{k-means}
{\bf k-means} partitions $m$ observations $x^{(i)}$ into $K$ clusters $\mu_k$ as defined in Algorithm~\ref{ref:kmeans}. The optimization objective is to minimize the cost function. The initial cluster coordinates are random which makes this algorithm non-deterministic. A recommended strategy is to randomly pick $K$ training examples as the initial coordinates to make sure that the initial coordinates are not too far away from the actual data. k-means can be run multiple times on the same data to pick the result with the lowest cost. Choosing the number of clusters $K$ is usually done manually depending on the use case.

\begin{algorithm}
\caption{k-means}
\label{ref:kmeans}
\begin{algorithmic}
\State $\mu_1, \mu_2, ..., \mu_K \in \mathbb{R}^n \gets$ random initialization
\Repeat
\For{$i=1$ to $m$}
\State $c^{(i)} \gets$ index of cluster centroid closest to $x^{(i)}$
\EndFor
\For{$k=1$ to $K$}
\State $\mu_k \gets$ mean of points assigned to cluster $k$
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\begin{definition}[k-means cost function]~\\
$J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K)=\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-\mu_{c^{(i)}}\rVert^2 $
\end{definition}

\section{Expectation maximization}
{\bf Expectation maximization} (EM) is a generalization of k-means using Gaussian models to smoothen the class memberships. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters.


\section{Principal component analysis}
\label{ref:pcasection}
"{\bf Principal component analysis} (PCA) computes the most meaningful basis to re-express a noisy, garbled data set"\footnote{\url{http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf}} and can be used for {\bf dimensionality reduction}. In this process, higher dimensional data is mapped to a lower dimension, the {\bf number of principal components}, as defined in Algorithm~\ref{ref:pca}. The goal of PCA is to minimize the {\bf average squared projection error} of all original data and their projections. PCA can be used to reduce memory and time consumption of complex computations. Also, it allows to plot higher dimensional data in 2D or 3D. PCA requires some underlying advanced linear algebra such as the {\bf singular value decomposition} (SVD) which is not further explained in this document. {\bf Matlab} or {\bf Octave} offer functions to compute this. Data can  approximately be mapped back to the origin dimension ($z^{(i)}\in \mathbb{R}^K$ to $x_{approx}^{(i)} \in \mathbb{R}^n$):
\begin{align*}
x_{approx}^{(i)} = U_{reduce}*z^{(i)}
\end{align*}

\begin{definition}[Average squared projection error]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2$
\end{definition}

\begin{definition}[Total variation in data]
$\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2$
\end{definition}

\begin{algorithm}
\caption{Principal component analysis}
\label{ref:pca}
\begin{algorithmic}
\State $x_j^{(i)} \gets \frac{x_j^{(i)} - \mu_j}{\sigma_j} $ \Comment{Mean normalization and feature scaling}
\State $\Sigma \gets \frac{1}{m}X^TX$ \Comment{Covariance matrix}
\State $[U,S,V] \gets svd(\Sigma)$ \Comment{Singular value decomposition}
\State $U_{reduce} \gets$ first $K$ columns of $U$ \Comment{Number $K$ of {\bf principal components}}
\State $z^{(i)} \gets U_{reduce}^Tx^{(i)}$ \Comment{Maps data $x^{(i)} \in \mathbb{R}^n$ to $z^{(i)}\in \mathbb{R}^K$}
\end{algorithmic}
\end{algorithm}

Choosing the number of principal components $K$ comes with a loss of information. Usually, a fixed percentage such as 99\% or 95\% of the {\bf variance} must be {\bf retained}:
\begin{align*}
\frac{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)}-x_{approx}^{(i)} \Vert^2}{\frac{1}{m}\sum_{i=1}^m \lVert x^{(i)} \Vert^2} \le 0.01 \mbox(1\%)
\end{align*}
This measure is more useful than the actual amount of dimensions that were reduced. Then, the smallest $K$ to satisfy this equation is chosen. Computing $K$ this way is inefficient as PCA may need to be performed a lot times to choose the smallest $K$. Alternatively, the retained variance can be computed for a given $K$:
\begin{align*}
\frac{\sum_{i=1}^{K}S_{ii}}{\sum_{i=1}^{n}S_{ii}} \ge 0.99 \mbox(99\%)
\end{align*}

Usually, the large variances occur in the first $K<n$ principal components and then drops off. Therefore, the most relevant dynamics occur only in the first $K$ dimensions.


\section{Anomaly detection}
{\bf Anomaly detection} allows to find data that does not conform to an expected pattern. The cluster-based Algorithm~\ref{ref:anomalydetection} assumes {\bf independence} between selected features which are representative for anomaly detection. These features take unusually large or small values in the event of an anomaly and have a Gaussian distribution:
\begin{align*}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
\end{align*}

Non-Gaussian features can be approximated as Gaussian through operations such as $log(x), \sqrt{x}, x^{1/3}$ etc.

\begin{algorithm}
\caption{Anomaly detection}
\label{ref:anomalydetection}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu_j \gets \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}$ \Comment{Fits parameters $\mu_1,...,\mu_n,\sigma_1,...,\sigma_n$}
\State $\sigma_j^2 \gets \frac{1}{m}\sum_{i=1}^{m}(x_j^{(i)}-\mu_j)^2$
\State For new example $x$: $p(x) \gets \prod_{j=1}^np(x_j,\mu_j,\sigma_j^2)$ \Comment{Assumes independence between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

A good way to evaluate this algorithm is to use it on some labeled data of anomalous and non-anomalous ($y=0$ if normal, $y=1$ if anomalous). The training set only contains non-anomalous examples whereas the cross validation and test sets contain also a few anomalous examples. On a cross validation or test example $x$, predict
\begin{align*}
y = \left\{
\begin{array}{lll}
1  & \mbox{if } p(x) < \epsilon \mbox{ anomaly} \\
0  & \mbox{if } p(x) \ge \epsilon \mbox{ normal}
\end{array}
\right.
\end{align*}

Then, evaluation metrics such as precision/recall and $F_1$ score can be applied to the result. The cross validation set can also be used to choose $\epsilon$.

\subsection{Anomaly detection vs. supervised learning}
{\bf Anomaly detection} is used for a very small number of positive examples and large number of negative exempts. It is also used for many different kinds of anomalies as it is hard for any algorithm to learn from just a few positive examples what the anomalies might look like. There may be also future anomalies which may look completely different to any of the anomalous examples learned so far. {\bf Supervised learning} is used for large numbers of both positive and negative examples. It is also used when there are enough positive examples so that the algorithm can get a sense of what positive examples look like and future positive examples are likely to be similar to the ones in the training set.

\subsection{Correlated features}
Algorithm~\ref{ref:anomalydetection} assumes independence between the features for which the contours are along the axes. If features are {\bf correlated}, the algorithm performs poorly as the contours should rather be along the correlations. In such a case, some anomalies are incorrectly classified as normal. One could add new features to represent this correlation (e.g. $x_{extra}=\frac{x_1}{x_2}$) which requires manual effort but is computationally cheap. \\

Alternatively, {\bf multivariate Gaussian distribution} helps:

\begin{align*}
p(x)=\frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
\end{align*}

where $\vert \Sigma \vert$ is the determinant of the {\bf covariance matrix} $\Sigma$. Algorithm~\ref{ref:anomalydetectionmulti} is a {\bf anomaly detection with multivariate Gaussian} which is a generalization of Algorithm~\ref{ref:anomalydetection}.

\begin{algorithm}
\caption{Anomaly detection with multivariate Gaussian}
\label{ref:anomalydetectionmulti}
\begin{algorithmic}
\State Choose features $x_i$ that might be indicative of anomalous examples
\State $\mu \gets \frac{1}{m}\sum_{i=1}^m x^{(i)}$
\State $\Sigma \gets \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T$
\State For new example $x$: $p(x) \gets \frac{1}{(2\pi)^{n/2}\vert \Sigma \vert ^{1/2}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))$ \Comment{Assumes correlation between features}
\State Anomaly if $p(x) < \epsilon$
\end{algorithmic}
\end{algorithm}

This algorithm automatically captures correlations between features. It is computationally more expensive and requires $m>n$ to invert $\Sigma$. There may be further problems that prevent inversion such as linear dependency.



\chapter{Reinforcement learning}
Supervised learning algorithms approximate a mapping of their inputs to the labels $y$ of the training set. These labels give an unambiguous "right answer" fear of the inputs $x$. In many problems, it is difficult to provide such an explicit supervision to a learning problem. In {\bf reinforcement learning}, the algorithm is only provided a reward (feedback) function which provides a reward or penalty depending on in which state the learning agent goes. Reinforcement learning is in between of supervised and unsupervised learning as there is some supervision, but significantly less than in supervised learning.
The {\bf credit assignment problem} is to figure out after getting a reward what an algorithm did right or wrong to get this reward. For example, a chess algorithm may lose after 30 steps and only gets then a penalty. This makes it hard to solve the credit assignment problem as the cause of the loss might have happened many steps before.

\section{Markov decision processes}
{\bf Markov decision processes} (MDP) provide a formalism in which reinforcement learning problems are usually posed. A MDP is a tuple $(S,A,\{P_{sa}\},\gamma, R)$, where:
\begin{itemize}
\item $S$ is a set of states
\item $A$ is a set of actions
\item $P_{sa}$ are the state transition probabilities which is a distribution over the state space for each state $s\in S$ and action $a\in A$. $P_{sa}(s')$ is the probability of $s'$ being the subsequent state of $s$ given action $a$. $\sum_{s'}P_{sa}(s')=1, P_{sa}(s')\ge 0$ .
\item $\gamma \in [0,1)$ is called the discount factor
\item $R:S \mapsto \mathbb{R}$ is the reward function
\end{itemize}

The dynamics of an MDP proceeds as follows: Start in some state $s_0$ and choose an action $a_0\in A$. The MDP then randomly transitions to some successor state $s_1\sim P_{s_0a_0}$. Then choose another action $a_1$ and randomly transition to $s_2\sim P_{s_1a_1}$: $s_0\xrightarrow{a_0}s_1\xrightarrow{a_1}s_2\xrightarrow{a_2}s_3\xrightarrow{a_3}...$. The total {\bf payoff} is: $R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+...$. This means that later rewards are given less weight than earlier rewards, called {\bf discounting}. The {\bf goal} of reinforcement learning is to maximize:
\begin{align*}
E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...]
\end{align*}
The goal of MDP is to compute a {\bf policy} $\pi:S\mapsto A$ which is a mapping from state to actions. The {\bf value function} of a policy $\pi$ is:
\begin{align*}
V^{\pi}(s) = E[R(s_o)+\gamma R(s_1)+\gamma^2 R(s_2)+...\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma (R(s_1)+\gamma (R(s_2)+...))\vert s_0=s,\pi]
\end{align*}
\begin{align*}
= E[R(s_o)+\gamma (V^{\pi}(s_1))\vert s_0=s,\pi]
\end{align*}
The value function is the expected sum of discounted rewards upon starting in state $s$ and taking actions according to $\pi$. $R(s_o)$ is called the {\bf immediate reward} and $\gamma (V^{\pi}(s_1))$ the {\bf future rewards}. More precisely, the {\bf value function} can be defined recursively which satisfies the {\bf Bellman equations}:
\begin{align*}
V^{\pi}(s) = R(s)+\gamma \sum_{s'\in S} P_{s\pi(s)}(s')V^{\pi}(s')
\end{align*}

Bellman's equations can be used to efficiently solve $V^{\pi}$. Specifically, in a finite-state MDP ($\vert S\vert < \infty$), such one such equation for $V^{\pi}(s)$ can be written down for every state $s$. This gives a set of $\vert S\vert$ linear equations in $\vert S\vert$ variables (the unknown $V^{\pi}(s)$'s, one for each state) which can be efficiently solved for the $V^{\pi}(s)$'s.

The {\bf optimal value function} is:
\begin{align*}
V^{*}(s) = \max\limits_{\pi} V^{\pi}(s)
\end{align*}

There is also a version of Bellman's equations for the optimal value function:
\begin{align*}
V^{*}(s) = R(s)+\max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

The {\bf optimal policy} is:
\begin{align*}
\pi^{*}(s) = \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V^{*}(s')
\end{align*}

One way to get $\pi^{*}$ is to compute $V^{*}$ by iterating all possible policies $\pi$. Then, it can be plugged into the definition of $\pi^{*}$. As there is an exponential amount of policies $(\vert A\vert^{\vert S \vert})$, this is impractical.

\section{Value iteration and policy iteration}
{\bf Value iteration} is an efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:valueiterationalgorithm}. Starting at 0, it repeatedly improves the value function. Eventually, $V$ converges to $V^{*}$. Then $\pi^{*}$ can be computed using the previous equation.

\begin{algorithm}[h!]
\caption{Value iteration}
\label{ref:valueiterationalgorithm}
\begin{algorithmic}
\State $\forall s\in S: V(s) \gets 0$
\Repeat
\State $\forall s\in S:$ update $V(s) \gets R(s) + \max\limits_{a\in A} \gamma \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{convergence}
\end{algorithmic}
\end{algorithm}

{\bf Policy iteration} is another efficient algorithm for solving finite-state MDPs as defined in Algorithm~\ref{ref:policyiterationalgorithm}. It starts with a random policy. First, in each iteration, a fixed policy is given and the algorithm solves for the value function of this policy. This solves a set of $\vert S\vert$ linear equations in $\vert S\vert$ variables. Second, the algorithm maximizes the policy for this value function. This is simple as the value function $S$ was computed in the step before. Eventually, $V$ and $\pi$ converge to $V^{*}$ and $\pi^{*}$ respectively.

\begin{algorithm}[h!]
\caption{Policy iteration}
\label{ref:policyiterationalgorithm}
\begin{algorithmic}
\State Initialize $\pi$ randomly
\Repeat
\State $V\gets V^{\pi}$
\State $\forall s\in S:\:\pi(s)\gets  \max\limits_{a\in A} \sum_{s'\in S} P_{sa}(s')V(s')$
\Until{convergence}
\end{algorithmic}
\end{algorithm}

In comparison, policy iteration is computationally expensive for large MDPs because of the $V\gets V^{\pi}$ step. Nonetheless, it is very fast and converges quickly for small MDPs. In practice, value iteration is used more often because of large state spaces.

\section{Model learning}
In many real-world problems, some parts of an MDP are unknown, e.g. state transition probabilities and rewards. These need to be estimated from data. Trials of transitions:
\begin{align*}
s_0^{(1)}\xrightarrow{a_0^{(1)}}s_1^{(1)}\xrightarrow{a_1^{(1)}}s_2^{(1)}\xrightarrow{a_2^{(1)}}s_3^{(1)}\xrightarrow{a_3^{(1)}}...
\end{align*}
\begin{align*}
s_0^{(2)}\xrightarrow{a_0^{(2)}}s_1^{(2)}\xrightarrow{a_1^{(2)}}s_2^{(2)}\xrightarrow{a_2^{(2)}}s_3^{(2)}\xrightarrow{a_3^{(2)}}...
\end{align*}
can be used to estimate the state transition probabilities:
\begin{align*}
P_{as}(s')=\frac{\#\mbox{times action } a\mbox{ was taken in state } s \mbox{ and got to }s'} {\#\mbox{times action } a\mbox{ was taken in state } s}
\end{align*}
This ratio might be "$0/0$" in case of never having taken action $a$ in state $s$ before, $P_{as}(s')$ can be estimated: $\frac{1}{\vert S\vert}$. A similar procedure can be used if $R$ is unknown. \\
Having learned a model for the MDP, either value iteration or policy iteration can be used to solve the MDP using the estimated transition probabilities and rewards. For example, model learning and value iteration can be put together and repeated until convergence.

\section{Continuous MDPs}
The algorithms discussed so far work on MDPs with a finite number of states. Many real-world problem need an infinite amount of states e.g. to represent positions and orientations. {\bf Discretization} maps the state space to a finite number of buckets on which then value iteration or policy iteration work.
This approach comes with two downsides: First, there is a loss of information which can be partially treated by using a very fine discretization which creates a huge amount of state.
The second downside is called the {\bf curse of dimensionality}. Suppose $S=\mathbb{R}^n$ and each of the $n$ dimensions is discretized into $k$ values. In total this would result in $\vert S_{discretized}\vert=k^n$ states.
This grows exponentially in the dimension of the state space $n$ which does not scale well to large problems. Discretization works generally only well for 1d or 2d problems. Many problems are higher-dimensional such as representing a driving car on a 2d plane as $(x, y, \theta, x', y', \theta')$ which is 6d. There are more advanced algorithms such as {\bf fitted value iteration} and {\bf Q-learning} that work on continuous spaces.

\section{Examples}
\subsection{Routing in a sample world}
This example is to apply the definitions on a small problem. The sample world in Figure~\ref{ref:sampleworld} contains a positive reward in cell $(4, 3)$ and a penalty in $(4, 2)$. It has 11 states and $A = \{N, S, E, W\}$.
\begin{figure}[h!]
\centering
\begin{tabular}{c|c|c|c|c|}
\hline
3 & & & & +1 \\
\hline
2 & & \cellcolor{black} & & -1 \\
\hline
1 & & & & \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\caption{Sample world}
\label{ref:sampleworld}
\end{figure}

Usually, a small penalty is assigned to all other states:
\begin{align*}
R(S)=
\left\{
\begin{array}{ll}
+1 & \mbox{if } S = (4,3) \\
-1  & \mbox{if } S = (4,2) \\
-0.02 & \mbox{otherwise}
\end{array}
\right.
\end{align*}

The state transition probability is defined as follows for all valid movements (invalid movements are 0): the intended outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles to the intended direction (0.1 to the left and 0.1 to to right if intended north).

The optimal policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy}. The $V*$ values were computed through value iteration. The action of $(3, 1)$ is not obvious. If it was north, there would be a 0.1 chance to go to -1 from $(3, 2)$. Therefore, it is left to route away from this potential action.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\uparrow$ & \cellcolor{black} & $\uparrow$ & -1 \\
\hline
1 & $\uparrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .86 & .90 & .93 & +1 \\
\hline
2 & .82 &\cellcolor{black} & .69 & -1 \\
\hline
1 & .78 & .75 & .71 & .49 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Optimal policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy}
\end{figure}

A bad policy is defined in Figure~\ref{ref:sampleworldoptimalpolicy1}. A sample value function definition part is:
\begin{align*}
V^{\pi}((3,1))=R((3,1)) + \gamma[0.8V^{\pi}((3,2))+0.1V^{\pi}((4,1))+0.8V^{\pi}((2,1))]
\end{align*}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & +1 \\
\hline
2 & $\downarrow$ & \cellcolor{black} & $\rightarrow$ & -1 \\
\hline
1 & $\rightarrow$ & $\rightarrow$ & $\uparrow$ & $\uparrow$ \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{c|c|c|c|c|}
\hline
3 & .52 & .73 & .77 & +1 \\
\hline
2 & -.90 &\cellcolor{black} & -.82 & -1 \\
\hline
1 & -.88 & -.87 & -.85 & -1.00 \\
\hline
& 1 & 2 & 3 & 4 \\
\end{tabular}
\end{subfigure}
\caption{Bad policy and corresponding $V^{\pi}$}
\label{ref:sampleworldoptimalpolicy1}
\end{figure}

\subsection{Further examples}
Reinforcement learning can be applied to a variety of real-world problems such as self-driving cars, self-flying helicopters or stock market prediction (rewards are profits/losses).

\chapter{Further machine learning topics}
This chapter contains further machine learning-related topics which do not fit into the previous chapters.

\section{Large scale machine learning}
Large data sets  help to increase the accuracy of {\bf low-bias} learning algorithms. In some cases, the actual algorithm may matter less as others perform similarly well given large data sets:
\begin{align*}
\mbox{"It's not who has the best algorithm that wins. It's who has the most data."}
\end{align*}
One way to find out if the algorithm has low bias is to randomly pick a lower amount (e.g. 1000) of  examples of this set and plot its learning curves.

\subsection{Stochastic gradient descent}
Large data sets come with new problems, e.g. a lack of {\bf scalability} . (Batch) gradient descent as defined in Algorithm~\ref{ref:gradientdescent} often converges within a few steps in the global minimum. But each summation update step of gradient descent can become computationally expensive for large data sets, e.g. 100 million records. The entire set cannot be stored in RAM and needs to be retrieved from hard disk or streamed.

\begin{algorithm}
\caption{Batch gradient descent}
\label{ref:gradientdescent}
\begin{algorithmic}
\Repeat
\For{$i=1$ to $m$}
\State $\theta_j  \gets \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}


{\bf Stochastic gradient descent} computes an approximation of gradient descent as described in Algorithm~\ref{ref:stochasticgradientdescent}. First, the randomization speeds up the algorithm in many cases. Second, the update statement in the inner loop allows the algorithm to improve the parameters quickly without summing up all $m$ training data. The resulting learning curve is {\bf not convex}. In general, the algorithm wanders around the global minimum and moves the parameters towards the region of it.

\begin{algorithm}
\caption{Stochastic gradient descent}
\label{ref:stochasticgradientdescent}
\begin{algorithmic}
\State Randomly shuffle data set
\Repeat
\For{$i=1$ to $m$}
\State $\theta_j  \gets \theta_j - \alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}


\subsection{Mini-batch gradient descent}
{\bf Mini-batch gradient descent} combines batch gradient descent and stochastic gradient descent as described in Algorithm~\ref{ref:minibatchgradientdescent}. It computes the parameters for buckets of $b$ training examples of $m$. It can still be vectorized which is an advantage over stochastic gradient descent and can outperform it.

\begin{algorithm}
\caption{Mini-batch gradient descent}
\label{ref:minibatchgradientdescent}
\begin{algorithmic}
\State $b\gets $ step size
\Repeat
\For{$i$ in $m, step=n$}
\State $\theta_j  \gets \theta_j - \alpha \frac{1}{\vert b\vert}\sum_{k=i}^{i+9} (h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}$ (for all $j$)
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}


\subsection{MapReduce gradient descent}
{\bf MapReduce} can be used to speed up gradient descent by distributing the summation in the gradient descent upgrade step as defined in Algorithm~\ref{ref:mapreducegradientdescent}. The data set is split up in $K$ parts which are sent to different computational units such as cores or servers.

\begin{algorithm}
\caption{MapReduce gradient descent}
\label{ref:mapreducegradientdescent}
\begin{algorithmic}
\State $K\gets $ split count
\Repeat
\For{$k=1..K$}
\State Distribute computation: $temp_j^{(k)}\gets \sum_{i=k\frac{m}{K}+1}^{(k + 1)\frac{m}{K}}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\EndFor
\State $\theta_j  \gets \theta_j - \alpha \frac{1}{m}\sum_{k=1}^{\frac{m}{K}}temp_j^{(k)}$ (for all $j$)
\Until{forever}
\end{algorithmic}
\end{algorithm}


\section{Online learning}
{\bf Online learning} is useful when there is a continuous stream of new data and if the entire data set is too large for efficient re-calculation of the parameters. It is defined in Algorithm~\ref{ref:onlinelearning}. Online learning learns on the fly and also adapts to change of data (e.g. rising housing prices).

\begin{algorithm}
\caption{Online learning}
\label{ref:onlinelearning}
\begin{algorithmic}
\State Get new data
\Repeat
\State Update $\theta$ using new data: $\theta_j\gets \theta_j - \alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ (for all $j$)
\Until{forever}
\end{algorithmic}
\end{algorithm}


\chapter{Localization and tracking}
{\bf Localization} can be done by using the global positioning system (GPS). Unfortunately, GPS is not accurate. Robot localization requires higher accuracy. This chapter provides information on such techniques.
{\bf Tracking} allows to detect and track other vehicles to avoid collisions.
Table \ref{ref:complocalization} summarizes and compares properties of the methods discussed in this chapter.
\begin{table}[h!]
\begin{center}
\begin{tabular}{l||c|c|c|c}
 & State space & Belief & Efficiency & In robotics \\
\hline
\hline
Markov localization & discrete & multi-modal & exponential & approximate \\
\hline
Kalman filter & continuous & uni-modal & quadratic & approximate \\
\hline
Particle filter & continuous & multi-modal & depends on use case & approximate \\

\end{tabular}
\end{center}
\caption{Comparison of localization and tracking methods}
\label{ref:complocalization}
\end{table}

\section{Markov localization}
{\bf Markov localization} is a {\bf discrete} grid-based localization which uses histogram to represent the belief distribution. It is {\bf multi-modal} meaning that there are multiple possible states in which the robot could be. Localization starts in an initial {\bf belief state} and consists of the two steps {\bf sense} and {\bf move} which are executed cyclically. Sense gains information on the current localization. In contrast, move loses information as movements are inaccurate. \\
The computation of the sense step is: \\
\begin{align*}
P(x'_{i,t}\vert z_t) = \frac{P(z_t\vert x'_{i,t})P(x'_{i,t})}{P(z_t)}
\end{align*}
$P(x'_{i,t})$ is the belief state before the update (i.e. $P(x_{i,t}\vert a_t)$). $P(z_t\vert x'_{i,t})$ is the probability of getting measurement $z_t$ from state $x'_{i,t}$. $P(z_t)$ is the probability of a sensor measurement $z_t$. Calculated so that the sum over all states $x_{i,j}$ equals 1.

The computation of the move step is:
\begin{align*}
P(x'_{i,t}) = P(x_{i,t}\vert a_t) = \sum_{j=1}^{n} P(x_{i,t}\vert x_{j,t-1},a_t)P(x_{j,t-1})
\end{align*}
It maps from a belief state $P(x_{j,t-1})$ and action $a_t$ to a new predicted belief state $P(x'_{i,t})$ by summing over all possible ways (i.e. from all states x$_{j,t-1}$) in which the robot may have reached $x'_{i,t}$. \\
Markov localization complexity grows exponentially. This is because any grid that is defined over $k$-dimensions will end up having exponentially many grid cells in the number of dimensions, which does not allow to represent high dimensional grids very well. \\
Algorithm~\ref{ref:markovlocalizationpython} implements Markov localization for a sample cyclic 5-cell world. The final belief state is:
\begin{align*}
[0.0789, 0.0753, 0.2247, {\bf 0.4329}, 0.1882]
\end{align*}

\begin{algorithm}
\caption{Sample Markov localization}
\label{ref:markovlocalizationpython}
\begin{python}
p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'red']
motions = [1,1]
pHit = 0.6
pMiss = 0.2
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    q = []
    for i in range(len(p)):
        s = pExact * p[(i-U) % len(p)]
        s = s + pOvershoot * p[(i-U-1) % len(p)]
        s = s + pUndershoot * p[(i-U+1) % len(p)]
        q.append(s)
    return q

for k in range(len(measurements)):
    p = sense(p, measurements[k])
    p = move(p, motions[k])

print p
\end{python}
\end{algorithm}


\section{Kalman filter}
{\bf Kalman filters} allow to estimate the state of a dynamic system and are often used to track other vehicles. In comparison to Markov localization, they are {\bf continuous} and {\bf uni-modal}. Kalman filters use a {\bf Gaussian distribution} to represent the belief state. Kalman filters have two steps: {\bf measurement update} and {\bf motion update} (prediction). \\

The measurement update of a 1d Kalman filter is a Gaussian multiplication and gains information:
\begin{align*}
\mu\prime = \frac{\sigma_2^2\mu_1 + \sigma_1^2\mu_2}{\sigma_1^2 + \sigma_2^2} \\
\sigma^{2}\prime = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{align*}

The motion update of a 1d Kalman filter is defined as follows and loses information:
\begin{align*}
\mu\prime = \mu_1 + \mu_2 \\
\sigma^{2}\prime = \sigma_1^2 + \sigma_2^2
\end{align*}
The motion itself is uncertain and can be described by a Gaussian $\mu_2, \sigma_2^2$. Algorithm~\ref{ref:1dkalmanfilterpython} implements a sample 1d Kalman filter. The velocity is given in that example. The final estimations are:
\begin{align*}
\mu = 10.99 \\
\sigma^2 = 4.006
\end{align*}

\begin{algorithm}
\caption{Sample 1d Kalman filter}
\label{ref:1dkalmanfilterpython}
\begin{python}
def update(mean1, var1, mean2, var2):
    new_mean = (var2 * mean1 + var1 * mean2) / (var1 + var2)
    new_var = (var1 * var2) / (var1 + var2)
    return [new_mean, new_var]

def predict(mean1, var1, mean2, var2):
    new_mean = mean1 + mean2
    new_var = var1 + var2
    return [new_mean, new_var]

measurements = [5., 6., 7., 9., 10.]
# Inferred motions
motion = [1., 1., 2., 1., 1.]
measurement_sig = 4.
motion_sig = 2.

# Very uncertain initial position belief
mu = 0
sig = 10000

for i in range(len(motion)):
    mu, sig = update(mu, sig, measurements[i], measurement_sig)
    mu, sig = predict(mu, sig, motion[i], motion_sig)

print [mu, sig]

\end{python}
\end{algorithm}


Multi-dimensional Kalman filters allow to infer properties to predict the future, e.g. the velocity in 2d measurements. In that case, this helps a robot to keep track of other vehicles to avoid accidents. They can efficiently be computed through matrix operations. The following elements are needed: {\bf estimate} $x$, {\bf uncertainty covariance} $P$, {\bf state transition matrix} $F$, {\bf motion vector} $u$, {\bf measurement} $z$, {\bf measurement function} $H$ and {\bf measurement noise} $R$. \\

The measurement update is defined as follows:
\begin{align*}
y = z - Hx \\
S = H(PH^T) + R \\
K = P(H^T S^{-1}) \\
x\prime = x + Ky \\
P\prime = (I - KH)P
\end{align*}


The motion update is defined as follows:
\begin{align*}
x\prime = Fx + u \\
P\prime = F(PF^T)
\end{align*}

The Kalman filter is quadratic. It is fully represented by a vector - the mean - and the covariance matrix, which is quadratic. This makes the Kalman filter a lot more efficient because it can operate on a state space with more dimensions. \\

Algorithm~\ref{ref:2dkalmanfilterpython} implements a sample 2d Kalman filter which infers the velocity of the object. The final estimations are:
\begin{align*}
x = 3.99 \\
\dot{x} = 0.99
\end{align*}

\begin{algorithm}
\caption{Sample 2d Kalman filter}
\label{ref:2dkalmanfilterpython}
\begin{python}
# initial state (location and velocity)
x = matrix([[0.], [0.]])
# initial uncertainty
P = matrix([[1000., 0.], [0., 1000.]])
# external motion
u = matrix([[0.], [0.]])
# next state function
F = matrix([[1., 1.], [0, 1.]])
# measurement function
H = matrix([[1., 0.]])
# measurement uncertainty
R = matrix([[1.]]) ]
# identity matrix
I = matrix([[1., 0.], [0., 1.]])

measurements = [1, 2, 3]

def kalman_filter(x, P):
    for n in range(len(measurements)):

        # measurement update
        y = matrix([[measurements[n]]]) - H * x
        S = H * (P * H.transpose()) + R
        K = P * (H.transpose() * S.inverse())
        x = x + K * y
        P = (I - K * H) * P

        # motion update
        x = F * x + u
        P = F * (P * F.transpose())

kalman_filter(x, P)
\end{python}
\end{algorithm}

\section{Particle filter}
{\bf Particle filters} are {\bf continuous} and {\bf multi-modal}.


\end{document}